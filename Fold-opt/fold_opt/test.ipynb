{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple knapsack\n",
    "\n",
    "$$\n",
    "\\min~ cx \\quad {s.t.}\\quad wx \\leq W \\quad 0\\leq x\\leq 1\n",
    "\n",
    "$$\n",
    "\n",
    "The projected gradient descent updates:\n",
    "\n",
    "- $x_{k+1} = proj_{C}(x_k - \\eta \\nabla f(x_k))$\n",
    "- $f(x) = cx$\n",
    "- $\\nabla f(x) = c$\n",
    "- $C$ is the feasible set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\Code\\\\Fold-opt\\\\fold_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GMRES import *\n",
    "from fold_opt import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_knapsack_closed(v, c, Q, max_iter: int = 25):\n",
    "    \"\"\"\n",
    "    Water-filling projection (Duchi et al., 2008) – vectorised & differentiable\n",
    "    v : (B,n)   raw iterate\n",
    "    c : (n,)    positive costs   (Torch tensor)\n",
    "    Q : float   budget\n",
    "    \"\"\"\n",
    "    B, n = v.shape\n",
    "    c = c.to(v)\n",
    "\n",
    "    # 1) clip to orthant\n",
    "    y     = v.clamp_min_(0.)\n",
    "    cost  = (y * c).sum(1)\n",
    "    mask  = cost > Q\n",
    "    if not mask.any():\n",
    "        return y\n",
    "\n",
    "    # 2) batched bisection for λ s.t. Σ_i max(0, y_i − λ c_i)c_i = Q\n",
    "    lam_lo = torch.zeros_like(cost)\n",
    "    lam_hi = (y / c).max(1).values          # tight upper bound\n",
    "    for _ in range(max_iter):\n",
    "        lam   = 0.5 * (lam_lo + lam_hi)\n",
    "        d_tmp = (y - lam.unsqueeze(1)*c).clamp_min_(0.)\n",
    "        excess = (d_tmp*c).sum(1) - Q\n",
    "        lam_lo = torch.where(excess > 0, lam, lam_lo)\n",
    "        lam_hi = torch.where(excess <= 0, lam, lam_hi)\n",
    "\n",
    "    lam  = lam_hi.unsqueeze(1)\n",
    "    d_pf = (y - lam*c).clamp_min_(0.)\n",
    "    return torch.where(mask.unsqueeze(1), d_pf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_knapsack_solver(v, c, Q):\n",
    "    out = []\n",
    "    c_np = c.cpu().numpy()\n",
    "    for row in v.cpu().numpy():\n",
    "        n   = row.size\n",
    "        d   = cp.Variable(n)\n",
    "        prob = cp.Problem(cp.Minimize(0.5*cp.sum_squares(d - row)),\n",
    "                          [d >= 0, c_np @ d <= Q])\n",
    "        prob.solve(solver=cp.OSQP, eps_abs=1e-8, verbose=False)\n",
    "        out.append(torch.tensor(d.value, dtype=v.dtype))\n",
    "    return torch.stack(out).to(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGDStep(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    update_step(c, d)   where   c == r   (risk/parameter vector).\n",
    "    All constants are stored as buffers so autograd only tracks r.\n",
    "    \"\"\"\n",
    "    def __init__(self, g, c_cost, Q: float, alpha: float,\n",
    "                 lr: float = 5e-2, closed_proj: bool = True):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"g\",      g)        # shape (n,)\n",
    "        self.register_buffer(\"c_cost\", c_cost)   # shape (n,)\n",
    "        self.Q      = float(Q)\n",
    "        self.alpha  = float(alpha)\n",
    "        self.lr     = float(lr)\n",
    "        self.closed = closed_proj\n",
    "\n",
    "    def forward(self, r, d):\n",
    "        \"\"\"Projected-gradient step  d_{t+1} ← Π_Ω(d_t − lr ∇f)\"\"\"\n",
    "        util = r * self.g\n",
    "        if self.alpha == 1.0:\n",
    "            grad = -(util / d.clamp_min(1e-12))\n",
    "        else:\n",
    "            grad = - (util**(1 - self.alpha)) * torch.pow(\n",
    "                     d.clamp_min(1e-12), -self.alpha)\n",
    "\n",
    "        d_new = d - self.lr * grad\n",
    "        proj  = proj_knapsack_closed if self.closed else proj_knapsack_solver\n",
    "        return proj(d_new, self.c_cost, self.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_star_closed(r, g, c, alpha: float, Q: float):\n",
    "    util = r * g\n",
    "    if alpha == 0:\n",
    "        idx = (util / c).argmax()\n",
    "        d = torch.zeros_like(r); d[idx] = Q / c[idx]; return d\n",
    "    if alpha == 1:\n",
    "        return (Q / c.sum()) * torch.ones_like(r) / c\n",
    "    if math.isinf(alpha):\n",
    "        S = (c*c / util).sum()\n",
    "        return Q * c / (util * S)\n",
    "    num = torch.pow(c, -1/alpha) * torch.pow(util, 1/alpha - 1)\n",
    "    return Q * num / num.sum()\n",
    "\n",
    "def grad_d_star_closed(r, g, c, alpha: float, Q: float):\n",
    "    d = d_star_closed(r, g, c, alpha, Q)\n",
    "    term = (1/alpha - 1) * g / r\n",
    "    G    = - torch.outer(d, d*term) / Q\n",
    "    diag = d * term * (1 - d/Q)\n",
    "    G[range(len(r)), range(len(r))] = diag\n",
    "    return G            # shape (n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closed_form_solver(g, c_cost, Q: float, alpha: float):\n",
    "    def solver(r_batch: torch.Tensor):\n",
    "        return torch.stack([d_star_closed(r_i, g, c_cost, alpha, Q)\n",
    "                            for r_i in r_batch], 0)\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x* from FoldOptLayer: [0.0, 2.0]\n",
      "Grad wrt c: [0.0, 0.44468268752098083]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "def my_solver(c):\n",
    "    return torch.clamp(c, min=0.0)\n",
    "\n",
    "def my_update_step(c, x):\n",
    "    alpha = 0.1\n",
    "    grad  = x - c\n",
    "    x_new = torch.clamp(x - alpha*grad, min=0.0)\n",
    "    return x_new\n",
    "\n",
    "fold_layer = FoldOptLayer(\n",
    "                solver      = my_solver,\n",
    "                update_step = my_update_step,\n",
    "                n_iter      = 20,\n",
    "                backprop_rule='FPI')\n",
    "\n",
    "c      = torch.tensor([[-1.0], [ 2.0]], requires_grad=True)   # (B=2, n=1)\n",
    "target = torch.tensor([[ 1.0], [ 1.5]])\n",
    "\n",
    "x_star = fold_layer(c)\n",
    "print(\"x* from FoldOptLayer:\", x_star.squeeze().tolist())     # → [0.0, 2.0]\n",
    "\n",
    "loss = 0.5 * torch.sum((x_star - target) ** 2)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Grad wrt c:\", c.grad.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_knapsack_closed(v, c, Q, nit=20):\n",
    "    B, n = v.shape\n",
    "    c = c.to(v)\n",
    "    y = v.clamp_min_(0.)\n",
    "    cost = (y*c).sum(1)\n",
    "    infeas = cost > Q\n",
    "    if not infeas.any():            # already feasible\n",
    "        return y\n",
    "    lam_lo = torch.zeros_like(cost)\n",
    "    lam_hi = (y/c).max(1).values\n",
    "    for _ in range(nit):\n",
    "        lam = 0.5*(lam_lo+lam_hi)\n",
    "        d   = (y - lam.unsqueeze(1)*c).clamp_min_(0.)\n",
    "        excess = (d*c).sum(1) - Q\n",
    "        lam_lo = torch.where(excess>0, lam, lam_lo)\n",
    "        lam_hi = torch.where(excess<=0, lam, lam_hi)\n",
    "    lam_final = lam_hi.unsqueeze(1)\n",
    "    d_proj = (y - lam_final*c).clamp_min_(0.)\n",
    "    return torch.where(infeas.unsqueeze(1), d_proj, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGDStep(torch.nn.Module):\n",
    "    def __init__(self, g, c_cost, Q: float, alpha: float,\n",
    "                 lr: float = 5e-2):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"g\", g)\n",
    "        self.register_buffer(\"c_cost\", c_cost)\n",
    "        self.Q     = float(Q)\n",
    "        self.alpha = float(alpha)\n",
    "        self.lr    = float(lr)\n",
    "\n",
    "    def forward(self, r, d):\n",
    "        util = r * self.g\n",
    "        if self.alpha == 1.0:\n",
    "            grad = -(util / d.clamp_min(1e-12))\n",
    "        else:\n",
    "            grad = -(util**(1-self.alpha)) * torch.pow(d.clamp_min(1e-12),\n",
    "                                                      -self.alpha)\n",
    "        d_new = d - self.lr*grad\n",
    "        return proj_knapsack_closed(d_new, self.c_cost, self.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‖d_out − d_star‖₂      = 0.00e+00\n",
      "max|∂d/∂r − closed|   = 1.12e+00\n",
      "forward+backward time =   19.0 ms\n"
     ]
    }
   ],
   "source": [
    "# --- problem setup ----------------------------------------------------------\n",
    "torch.manual_seed(0)\n",
    "n      = 12\n",
    "alpha  = 2.0\n",
    "Q      = 7.0\n",
    "g      = torch.rand(n) + 0.5\n",
    "c_cost = torch.rand(n) + 0.5\n",
    "r      = torch.rand(n, requires_grad=True) + 0.5\n",
    "\n",
    "# --- components -------------------------------------------------------------\n",
    "solver_opt = make_closed_form_solver(g, c_cost, Q, alpha)\n",
    "pgd_step   = PGDStep(g, c_cost, Q, alpha, lr=4e-2)\n",
    "\n",
    "layer = FoldOptLayer(\n",
    "    solver      = solver_opt,\n",
    "    update_step = pgd_step,\n",
    "    n_iter      = 250,\n",
    "    backprop_rule='GMRES')\n",
    "\n",
    "# --- forward + backward -----------------------------------------------------\n",
    "r_batched = r.unsqueeze(0)             # shape (1,n)\n",
    "t0 = time.time()\n",
    "d_out = layer(r_batched)               # (1,n), should equal d_star\n",
    "loss  = d_out.sum()\n",
    "\n",
    "# use autograd.grad to get ∂loss/∂r_batched\n",
    "grad_r_batched = torch.autograd.grad(loss, r_batched)[0]\n",
    "elapsed = (time.time() - t0)*1e3\n",
    "\n",
    "# --- closed-form checks ----------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    d_star  = d_star_closed(r, g, c_cost, alpha, Q)\n",
    "    G_star  = grad_d_star_closed(r, g, c_cost, alpha, Q)\n",
    "\n",
    "    # compare decisions\n",
    "    dec_err  = torch.norm(d_out.squeeze(0) - d_star).item()\n",
    "    # compare gradients\n",
    "    grad_r   = grad_r_batched.squeeze(0)\n",
    "    grad_err = (grad_r.unsqueeze(1) - G_star).abs().max().item()\n",
    "\n",
    "print(f\"‖d_out − d_star‖₂      = {dec_err:8.2e}\")\n",
    "print(f\"max|∂d/∂r − closed|   = {grad_err:8.2e}\")\n",
    "print(f\"forward+backward time = {elapsed:6.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7764],\n",
       "        [-0.2442],\n",
       "        [-0.2030],\n",
       "        [-0.7714],\n",
       "        [-0.4356],\n",
       "        [-0.2452],\n",
       "        [-0.4158],\n",
       "        [-0.1357],\n",
       "        [-0.4115],\n",
       "        [-0.2842],\n",
       "        [-0.5305],\n",
       "        [-0.5630]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_r.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
