{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "from pyepo.model.opt import optModel\n",
    "sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\MyRepo\\\\FDFL\\\\helper')\n",
    "sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\MyRepo\\\\fold-opt-package\\\\fold_opt')\n",
    "\n",
    "from myutil import *\n",
    "from features import get_all_features\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from GMRES import *\n",
    "from fold_opt import *\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running with simpler model\n",
    "Alpha = 0.5, 1.5, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, Q = 2, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "# fix random seed for reproducibility\n",
    "\n",
    "# report statistics on this dataset\n",
    "df = df.sample(n=5000, random_state=1)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'risk_score_t', 'program_enrolled_t', 'cost_t', 'cost_avoidable_t', 'race', 'dem_female', 'gagne_sum_tm1', 'gagne_sum_t', \n",
    "    'risk_score_percentile', 'screening_eligible', 'avoidable_cost_mapped', 'propensity_score', 'g_binary', \n",
    "    'g_continuous', 'utility_binary', 'utility_continuous'\n",
    "]\n",
    "# for race 0 is white, 1 is black\n",
    "df_stat = df[columns_to_keep]\n",
    "df_feature = df[[col for col in df.columns if col not in columns_to_keep]]\n",
    "\n",
    "# Replace all values less than 0.1 with 0.1\n",
    "#df['risk_score_t'] = df['risk_score_t'].apply(lambda x: 0.1 if x < 0.1 else x)\n",
    "df['g_continuous'] = df['g_continuous'].apply(lambda x: 0.1 if x < 0.1 else x)\n",
    "\n",
    "\n",
    "risk = df['risk_score_t'].values\n",
    "risk = risk + 0.001 if 0 in risk else risk\n",
    "\n",
    "\n",
    "feats = df[get_all_features(df)].values\n",
    "gainF = df['g_continuous'].values\n",
    "decision = df['propensity_score'].values\n",
    "cost = np.random.normal(1, 0.5, len(risk)).clip(0.1, 2)\n",
    "race = df['race'].values\n",
    "\n",
    "# transform the features\n",
    "scaler = StandardScaler()\n",
    "feats = scaler.fit_transform(feats)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    def __init__(self, optmodel, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        self.feats = torch.from_numpy(feats).float()\n",
    "        self.risk = torch.from_numpy(risk).float()\n",
    "        self.gainF = torch.from_numpy(gainF).float()\n",
    "        self.cost = torch.from_numpy(cost).float()\n",
    "        self.race = torch.from_numpy(race).float()\n",
    "        self.optmodel = optmodel\n",
    "\n",
    "        # Solve for w*, z* using separate risk and gainF # Ensure a separate instance\n",
    "        self.w_star, self.z_star = self.optmodel(self.risk, self.gainF, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        self.w_star = torch.tensor(self.w_star, dtype=torch.float)\n",
    "        self.z_star = torch.tensor(self.z_star, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.feats, self.risk, self.gainF, self.cost, self.race, self.w_star, self.z_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FairRiskPredictor(nn.Module):\n",
    "#     def __init__(self, input_dim, dropout_rate=0.1):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 1),\n",
    "#             nn.Softplus()\n",
    "#         )\n",
    "            \n",
    "                    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x).squeeze(-1)\n",
    "    \n",
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer with batch normalization\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(predmodel, optmodel, dataloader, alpha=alpha, Q=Q):\n",
    "    predmodel.eval()\n",
    "    feats, risk, gainF, cost, race, opt_sol, opt_val = next(iter(dataloader))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        feats, risk, gainF, cost, race, opt_sol, opt_val = feats.cuda(), risk.cuda(), gainF.cuda(), cost.cuda(), race.cuda(), opt_sol.cuda(), opt_val.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_risk = predmodel(feats)\n",
    "\n",
    "    risk = risk.detach().to('cpu').numpy()\n",
    "    pred_risk = pred_risk.detach().to('cpu').numpy().flatten()\n",
    "    pred_risk = pred_risk.clip(min=0.001)\n",
    "    gainF = gainF.detach().to('cpu').numpy().flatten()\n",
    "    cost = cost.detach().to('cpu').numpy().flatten()\n",
    "    pred_sol, _ = optmodel(gainF, pred_risk, cost, alpha, Q)\n",
    "        \n",
    "    pred_obj = AlphaFairness(gainF * risk * pred_sol, alpha)\n",
    "\n",
    "    normalized_regret = (opt_val - pred_obj) / (abs(opt_val) + 1e-7)\n",
    "    predmodel.train()\n",
    "    return normalized_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FairRiskPredictor(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=149, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training parameters\n",
    "\n",
    "optmodel = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, risk_train, risk_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    feats, gainF, risk, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(optmodel, feats_train, risk_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(optmodel, feats_test, risk_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "# Create dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "predmodel.to(device)\n",
    "# save the initial model\n",
    "# torch.save(predmodel.state_dict(), 'initial_model.pth')\n",
    "# load the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# assume make_foldopt_layer and alpha_fair are already in scope from your fold-opt code\n",
    "# from fold_opt import make_foldopt_layer, alpha_fair\n",
    "\n",
    "def trainFairModelFoldOpt(\n",
    "    predmodel,\n",
    "    loader_train,\n",
    "    loader_test,\n",
    "    alpha,\n",
    "    Q,\n",
    "    lambda_fairness=0.1,\n",
    "    num_epochs=10,\n",
    "    lr_pred=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    pgd_lr=1e-2,\n",
    "    n_fixedpt=200,\n",
    "    backprop_rule=\"GMRES\"\n",
    "):\n",
    "    device = next(predmodel.parameters()).device\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr_pred, weight_decay=weight_decay)\n",
    "\n",
    "    logs = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_mse\": [],\n",
    "        \"train_fair\": [],\n",
    "        \"train_regret\": []\n",
    "    }\n",
    "\n",
    "    predmodel.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # --- pull the one-and-only batch (the entire dataset) ---\n",
    "        feats, risk, gainF, cost, race, opt_d, opt_obj = next(iter(loader_train))\n",
    "\n",
    "        # squeeze away the dummy batch‐dim and move to device\n",
    "        feats   = feats.squeeze(0).to(device)   # (n,m)\n",
    "        risk    = risk.squeeze(0).to(device)    # (n)\n",
    "        gainF   = gainF.squeeze(0).to(device)   # (n)\n",
    "        cost    = cost.squeeze(0).to(device)    # (n)\n",
    "        race    = race.squeeze(0).to(device)    # (n)\n",
    "        opt_d   = opt_d.squeeze(0).to(device)   # (n)\n",
    "        opt_obj = opt_obj.squeeze(0).to(device) # scalar or (1)\n",
    "\n",
    "        # --- forward pass: predict risk ---\n",
    "        pred_risk = predmodel(feats).clamp(min=1e-3)  # (n,)\n",
    "\n",
    "        # --- build a Fold-Opt layer for this batch ---\n",
    "        #    it will map r_batch (1,n) → d_pred (1,n)\n",
    "        fold_layer = make_foldopt_layer(\n",
    "            gainF, cost, alpha, Q,\n",
    "            lr=pgd_lr,\n",
    "            n_fixedpt=n_fixedpt,\n",
    "            rule=backprop_rule\n",
    "        )\n",
    "\n",
    "        # run it (we need a 2-d input for the layer)\n",
    "        d_pred = fold_layer(pred_risk.unsqueeze(0)).squeeze(0)  # (n,)\n",
    "\n",
    "        # --- compute regret loss via alpha‐fairness ---\n",
    "        u_pred    = d_pred * risk * gainF               # (n,)\n",
    "        pred_obj  = alpha_fair(u_pred.unsqueeze(0), alpha)   # (1,)\n",
    "        regret_l1 = (opt_obj - pred_obj) / (opt_obj.abs() + 1e-7)  # (1,)\n",
    "\n",
    "        # --- fairness penalty: difference in MSE across race groups ---\n",
    "        m0 = (pred_risk[race == 0] - risk[race == 0]).pow(2).mean() if (race==0).any() else torch.tensor(0., device=device)\n",
    "        m1 = (pred_risk[race == 1] - risk[race == 1]).pow(2).mean() if (race==1).any() else torch.tensor(0., device=device)\n",
    "        fair_reg = torch.abs(m0 - m1)\n",
    "\n",
    "        # --- total loss & backward ---\n",
    "        loss = regret_l1 + lambda_fairness * fair_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- compute simple MSE for logging ---\n",
    "        mse_train = (pred_risk - risk).pow(2).mean()\n",
    "\n",
    "        # --- log everything ---\n",
    "        logs[\"train_loss\"].append(loss.item())\n",
    "        logs[\"train_mse\"].append(mse_train.item())\n",
    "        logs[\"train_fair\"].append(fair_reg.item())\n",
    "        logs[\"train_regret\"].append(regret_l1.item())\n",
    "\n",
    "        # (optional) print progress\n",
    "        print(f\"Epoch {epoch:2d} | Loss={loss.item():.4f} | MSE={mse_train.item():.4f} | Fair={fair_reg.item():.4f} | Regret={regret_l1.item():.4f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "    predmodel.eval()\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss=-0.0475 | MSE=48.4381 | Fair=65.9715 | Regret=-0.0475 | 1.0s\n",
      "Epoch  2 | Loss=-0.0502 | MSE=48.5404 | Fair=65.7493 | Regret=-0.0502 | 1.0s\n",
      "Epoch  3 | Loss=-0.0538 | MSE=48.5815 | Fair=65.7072 | Regret=-0.0538 | 1.0s\n",
      "Epoch  4 | Loss=-0.0576 | MSE=48.6448 | Fair=65.7161 | Regret=-0.0576 | 1.0s\n",
      "Epoch  5 | Loss=-0.0591 | MSE=48.7762 | Fair=65.8527 | Regret=-0.0591 | 1.0s\n",
      "Epoch  6 | Loss=-0.0633 | MSE=48.8298 | Fair=65.6968 | Regret=-0.0633 | 1.0s\n",
      "Epoch  7 | Loss=-0.0664 | MSE=48.9592 | Fair=65.9307 | Regret=-0.0664 | 1.0s\n",
      "Epoch  8 | Loss=-0.0693 | MSE=49.0397 | Fair=66.1033 | Regret=-0.0693 | 1.0s\n",
      "Epoch  9 | Loss=-0.0736 | MSE=49.1447 | Fair=66.0042 | Regret=-0.0736 | 1.0s\n",
      "Epoch 10 | Loss=-0.0775 | MSE=49.2398 | Fair=66.0903 | Regret=-0.0775 | 1.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logs \u001b[38;5;241m=\u001b[39m trainFairModelFoldOpt(\n\u001b[0;32m      2\u001b[0m     predmodel,\n\u001b[0;32m      3\u001b[0m     dataloader_train,\n\u001b[0;32m      4\u001b[0m     dataloader_test,\n\u001b[0;32m      5\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.50\u001b[39m,\n\u001b[0;32m      6\u001b[0m     Q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m      7\u001b[0m     lambda_fairness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      8\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      9\u001b[0m     lr_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[129], line 76\u001b[0m, in \u001b[0;36mtrainFairModelFoldOpt\u001b[1;34m(predmodel, loader_train, loader_test, alpha, Q, lambda_fairness, num_epochs, lr_pred, weight_decay, pgd_lr, n_fixedpt, backprop_rule)\u001b[0m\n\u001b[0;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m regret_l1 \u001b[38;5;241m+\u001b[39m lambda_fairness \u001b[38;5;241m*\u001b[39m fair_reg\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 76\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# --- compute simple MSE for logging ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mE:\\User\\Stevens\\MyRepo\\fold-opt-package\\fold_opt\\fold_opt.py:59\u001b[0m, in \u001b[0;36mFixedPtDiffGMRES.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[0;32m     58\u001b[0m     c, x_star_step, x_star, max_iter \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[1;32m---> 59\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m JgP_GMRES(c, x_star_step, x_star, grad_output, n_steps\u001b[38;5;241m=\u001b[39mmax_iter\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_input\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\User\\Stevens\\MyRepo\\fold-opt-package\\fold_opt\\fold_opt.py:182\u001b[0m, in \u001b[0;36mJgP_GMRES\u001b[1;34m(c, x_star_step, x_star, g, n_steps, tol)\u001b[0m\n\u001b[0;32m    177\u001b[0m     else:\n\u001b[0;32m    178\u001b[0m         # Create the Givens matrix and use it to update O\n\u001b[0;32m    179\u001b[0m         # Recover R from O and H\n\u001b[0;32m    180\u001b[0m         O, R = GMRES.v_update_O_R(H, O, I2, I2skew, k)\n\u001b[0;32m    181\u001b[0m         # Create the RHS of the least squares problem\n\u001b[1;32m--> 182\u001b[0m         # Least squares is equivalent to this back-substitution\n\u001b[0;32m    183\u001b[0m         z = GMRES.v_solve_x(Q,O,R,x0,e,beta,k)\n\u001b[0;32m    185\u001b[0m gradient = torch.autograd.grad(x_star_step, c, z, retain_graph=True)[0].detach()\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\vmap.py:266\u001b[0m, in \u001b[0;36mvmap_impl\u001b[1;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[0;32m    263\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    267\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    268\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\vmap.py:379\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[1;32m--> 379\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\User\\Stevens\\MyRepo\\fold-opt-package\\fold_opt\\GMRES.py:152\u001b[0m, in \u001b[0;36mupdate_O_R\u001b[1;34m(H, O, I2, I2skew, k)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_O_R\u001b[39m(H, O, I2, I2skew, k): \u001b[38;5;66;03m# extra input k\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Create the Givens matrix and use it to update O\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     rho_sigma \u001b[38;5;241m=\u001b[39m O[k:k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m,:k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;129m@H\u001b[39m[:k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m,k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 152\u001b[0m     Gblock \u001b[38;5;241m=\u001b[39m (rho_sigma[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mI2 \u001b[38;5;241m+\u001b[39m rho_sigma[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mI2skew) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(rho_sigma)\n\u001b[0;32m    153\u001b[0m     O[k:k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m,:] \u001b[38;5;241m=\u001b[39m Gblock\u001b[38;5;129m@O\u001b[39m[k:k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m,:]\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Recover R from O and H\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\functional.py:1480\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;129m@overload\u001b[39m  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(\u001b[38;5;28minput\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m         \u001b[38;5;66;03m# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(\u001b[38;5;28minput\u001b[39m, p: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the matrix norm or vector norm of a given tensor.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    .. warning::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;124;03m        (tensor(3.7417), tensor(11.2250))\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs = trainFairModelFoldOpt(\n",
    "    predmodel,\n",
    "    dataloader_train,\n",
    "    dataloader_test,\n",
    "    alpha=1.50,\n",
    "    Q=1000,\n",
    "    lambda_fairness=0,\n",
    "    num_epochs=50,\n",
    "    lr_pred=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss=-0.1018 | MSE=48.7463 | Fair=66.3695 | Regret=-0.1018 | 1.0s\n",
      "Epoch  2 | Loss=-0.1058 | MSE=48.7710 | Fair=66.2251 | Regret=-0.1058 | 1.0s\n",
      "Epoch  3 | Loss=-0.1114 | MSE=48.8249 | Fair=66.2088 | Regret=-0.1114 | 1.0s\n",
      "Epoch  4 | Loss=-0.1164 | MSE=48.8049 | Fair=66.0563 | Regret=-0.1164 | 1.0s\n",
      "Epoch  5 | Loss=-0.1212 | MSE=48.8947 | Fair=65.9276 | Regret=-0.1212 | 1.0s\n",
      "Epoch  6 | Loss=-0.1261 | MSE=48.9604 | Fair=66.0105 | Regret=-0.1261 | 1.0s\n",
      "Epoch  7 | Loss=-0.1285 | MSE=49.0425 | Fair=65.8967 | Regret=-0.1285 | 1.0s\n",
      "Epoch  8 | Loss=-0.1342 | MSE=49.0996 | Fair=66.0029 | Regret=-0.1342 | 1.0s\n",
      "Epoch  9 | Loss=-0.1361 | MSE=49.2121 | Fair=65.9900 | Regret=-0.1361 | 1.0s\n",
      "Epoch 10 | Loss=-0.1361 | MSE=49.3038 | Fair=66.0044 | Regret=-0.1361 | 1.0s\n",
      "Epoch 11 | Loss=-0.1423 | MSE=49.4695 | Fair=66.1590 | Regret=-0.1423 | 1.0s\n",
      "Epoch 12 | Loss=-0.1435 | MSE=49.5863 | Fair=66.1687 | Regret=-0.1435 | 1.0s\n",
      "Epoch 13 | Loss=-0.1460 | MSE=49.7538 | Fair=66.5696 | Regret=-0.1460 | 1.0s\n",
      "Epoch 14 | Loss=-0.1485 | MSE=49.8019 | Fair=66.2730 | Regret=-0.1485 | 1.0s\n",
      "Epoch 15 | Loss=-0.1520 | MSE=49.9434 | Fair=66.5233 | Regret=-0.1520 | 1.0s\n",
      "Epoch 16 | Loss=-0.1539 | MSE=50.0815 | Fair=66.5404 | Regret=-0.1539 | 1.0s\n",
      "Epoch 17 | Loss=-0.1619 | MSE=50.1823 | Fair=66.6567 | Regret=-0.1619 | 1.0s\n",
      "Epoch 18 | Loss=-0.1644 | MSE=50.3095 | Fair=66.7582 | Regret=-0.1644 | 1.0s\n",
      "Epoch 19 | Loss=-0.1646 | MSE=50.4106 | Fair=66.6727 | Regret=-0.1646 | 1.0s\n",
      "Epoch 20 | Loss=-0.1658 | MSE=50.5342 | Fair=66.8480 | Regret=-0.1658 | 1.0s\n"
     ]
    }
   ],
   "source": [
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "predmodel.to(device)\n",
    "\n",
    "logs = trainFairModelFoldOpt(\n",
    "    predmodel,\n",
    "    dataloader_train,\n",
    "    dataloader_test,\n",
    "    alpha=1.9,\n",
    "    Q=1000,\n",
    "    lambda_fairness=0,\n",
    "    num_epochs=20,\n",
    "    lr_pred=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss=-2.6334 | MSE=48.2352 | Fair=66.4849 | Regret=-2.6334 | 1.7s\n",
      "Epoch  2 | Loss=-2.6274 | MSE=48.2610 | Fair=66.6189 | Regret=-2.6274 | 1.7s\n",
      "Epoch  3 | Loss=-2.6347 | MSE=48.2671 | Fair=66.8725 | Regret=-2.6347 | 1.7s\n",
      "Epoch  4 | Loss=-2.6298 | MSE=48.2601 | Fair=66.6652 | Regret=-2.6298 | 1.7s\n",
      "Epoch  5 | Loss=-2.6319 | MSE=48.2638 | Fair=66.6183 | Regret=-2.6319 | 1.7s\n",
      "Epoch  6 | Loss=-2.6304 | MSE=48.1750 | Fair=66.3980 | Regret=-2.6304 | 1.7s\n",
      "Epoch  7 | Loss=-2.6325 | MSE=48.1621 | Fair=66.3467 | Regret=-2.6325 | 1.7s\n",
      "Epoch  8 | Loss=-2.6333 | MSE=48.1063 | Fair=66.1515 | Regret=-2.6333 | 1.7s\n",
      "Epoch  9 | Loss=-2.6428 | MSE=48.0501 | Fair=66.1426 | Regret=-2.6428 | 1.0s\n",
      "Epoch 10 | Loss=-2.6408 | MSE=48.0377 | Fair=65.7064 | Regret=-2.6408 | 1.0s\n",
      "Epoch 11 | Loss=-2.6386 | MSE=48.0283 | Fair=66.0297 | Regret=-2.6386 | 1.0s\n",
      "Epoch 12 | Loss=-2.6440 | MSE=47.9888 | Fair=65.8849 | Regret=-2.6440 | 1.0s\n",
      "Epoch 13 | Loss=-2.6450 | MSE=47.9276 | Fair=65.2172 | Regret=-2.6450 | 1.0s\n",
      "Epoch 14 | Loss=-2.6421 | MSE=47.9271 | Fair=65.4077 | Regret=-2.6421 | 1.0s\n",
      "Epoch 15 | Loss=-2.6460 | MSE=47.8407 | Fair=65.2131 | Regret=-2.6460 | 1.0s\n",
      "Epoch 16 | Loss=-2.6454 | MSE=47.8532 | Fair=65.1110 | Regret=-2.6454 | 0.9s\n",
      "Epoch 17 | Loss=-2.6492 | MSE=47.7535 | Fair=65.1119 | Regret=-2.6492 | 1.0s\n",
      "Epoch 18 | Loss=-2.6447 | MSE=47.7996 | Fair=65.0862 | Regret=-2.6447 | 1.0s\n",
      "Epoch 19 | Loss=-2.6545 | MSE=47.6485 | Fair=64.5826 | Regret=-2.6545 | 1.0s\n",
      "Epoch 20 | Loss=-2.6521 | MSE=47.6514 | Fair=64.6297 | Regret=-2.6521 | 0.9s\n"
     ]
    }
   ],
   "source": [
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "predmodel.to(device)\n",
    "\n",
    "logs = trainFairModelFoldOpt(\n",
    "    predmodel,\n",
    "    dataloader_train,\n",
    "    dataloader_test,\n",
    "    alpha=0.5,\n",
    "    Q=1000,\n",
    "    lambda_fairness=0,\n",
    "    num_epochs=20,\n",
    "    lr_pred=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_0p5.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_0p5_fair.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_1p5.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_1p5_fair.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_2.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_2_fair.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_inf.pkl\n",
    "# E:\\User\\Stevens\\MyRepo\\FDFL\\res\\cvxpylayer\\LR\\results_cvx_inf_fair.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
