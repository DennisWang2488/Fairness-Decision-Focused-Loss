{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d88cd91",
   "metadata": {},
   "source": [
    "\n",
    "## Group-Aware alpha-Fair Allocation: closed form, gradients, tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "721a20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1, resource 1: [2.25 3.5  4.5  5.25] Mean: 3.88\n",
      "Group 1, resource 2: [3.66666667 5.33333333 6.66666667 7.66666667] Mean: 5.83\n",
      "Group 2, resource 1: [4.  5.  6.5 8. ] Mean: 5.88\n",
      "Group 2, resource 2: [3.75   4.125  4.6875 5.25  ] Mean: 4.45\n",
      "\n",
      "True values\n",
      "Group 1, true utility: 19.75\n",
      "Group 2, true utility: 19.88\n",
      "\n",
      "Standard MSE - Resource 1: 2.302980398457583\n",
      "MSE by group - G1, G2: 2.7250826038412868 3.3162882851016224\n",
      "Standard MSE - Resource 2: 0.08547529946800914\n",
      "MSE by group - G1, G2: 0.36346671877677333 0.74276129414888\n",
      "\n",
      "MSE predictions\n",
      "Group 1, predicted utility: 16.92\n",
      "Group 2, predicted utility: 18.06\n",
      "Decision error: 5.46\n",
      "\n",
      "MSE when fair - Resource 1: 2.3931863312455346\n",
      "MSE by group - G1, G2: 3.093985304909456 3.093985388292682\n",
      "MSE when fair - Resource 2: 0.955360648421686\n",
      "MSE by group - G1, G2: 1.9548510409249535 1.954851041682745\n",
      "\n",
      "Fair predictions\n",
      "Group 1, predicted utility: 18.75\n",
      "Group 2, predicted utility: 18.94\n",
      "Decision error: 5.66\n",
      "\n",
      "E2E - Resource 1: 48.17213559130881\n",
      "MSE by group - G1, G2: 18.523679118302724 6.500030511668854\n",
      "E2E - Resource 2: 38.98664416134241\n",
      "MSE by group - G1, G2: 14.621190339919938 9.905248423667688\n",
      "\n",
      "E2E predictions\n",
      "Group 1, predicted utility: 15.50\n",
      "Group 2, predicted utility: 17.81\n",
      "Decision error: 5.66\n",
      "\n",
      "Fair E2E - Resource 1: 18.528287924399383\n",
      "MSE by group - G1, G2: 6.348592095637199 10.388536075810102\n",
      "Fair E2E - Resource 2: 19.67149355731015\n",
      "MSE by group - G1, G2: 10.290939935465856 7.174155260594462\n",
      "\n",
      "Fair E2E predictions\n",
      "Group 1, predicted utility: 18.75\n",
      "Group 2, predicted utility: 19.88\n",
      "Decision error: 2.83\n"
     ]
    }
   ],
   "source": [
    "# New toy example for Fair Decision Feedback Loop (FDFL)\n",
    "# Focus: Prediction fairness with accuracy disparity\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Data Generation\n",
    "def generate_data():\n",
    "    \"\"\"Generate feature and benefit data for two groups and two resources.\"\"\"\n",
    "    f1 = np.array([5, 10, 14, 17])  # Group 1 features\n",
    "    f2 = np.array([4, 6, 9, 12])    # Group 2 features\n",
    "    n = len(f1)                     # Number of individuals per group\n",
    "\n",
    "    # Benefits for each group and resource\n",
    "    a = 1/4 * f1 + 1  # Group 1, resource 1\n",
    "    c = 1/3 * f1 + 2  # Group 1, resource 2\n",
    "    b = 1/2 * f2 + 2  # Group 2, resource 1\n",
    "    d = 3/16 * f2 + 3 # Group 2, resource 2\n",
    "\n",
    "    # Print benefits and their means\n",
    "    print(\"Group 1, resource 1:\", a, f\"Mean: {np.mean(a):.2f}\")\n",
    "    print(\"Group 1, resource 2:\", c, f\"Mean: {np.mean(c):.2f}\")\n",
    "    print(\"Group 2, resource 1:\", b, f\"Mean: {np.mean(b):.2f}\")\n",
    "    print(\"Group 2, resource 2:\", d, f\"Mean: {np.mean(d):.2f}\")\n",
    "\n",
    "    X = np.concatenate([f1, f2]).reshape(-1, 1)\n",
    "    y1 = np.concatenate([a, b])  # Outcome for resource 1\n",
    "    y2 = np.concatenate([c, d])  # Outcome for resource 2\n",
    "    return f1, f2, a, b, c, d, X, y1, y2, n\n",
    "\n",
    "# Standard MSE Regression\n",
    "def standard_regression(X, y1, y2, a, b, c, d, n):\n",
    "    \"\"\"Perform standard linear regression for both resources.\"\"\"\n",
    "    # Resource 1\n",
    "    reg1 = LinearRegression().fit(X, y1)\n",
    "    pred1 = reg1.predict(X)\n",
    "    mse1 = mean_squared_error(y1, pred1)\n",
    "    ahat_mse = pred1[:n]\n",
    "    bhat_mse = pred1[n:]\n",
    "\n",
    "    # Resource 2\n",
    "    reg2 = LinearRegression().fit(X, y2)\n",
    "    pred2 = reg2.predict(X)\n",
    "    mse2 = mean_squared_error(y2, pred2)\n",
    "    chat_mse = pred2[:n]\n",
    "    dhat_mse = pred2[n:]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nStandard MSE - Resource 1:\", mse1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_mse - a), np.linalg.norm(bhat_mse - b))\n",
    "    print(\"Standard MSE - Resource 2:\", mse2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_mse - c), np.linalg.norm(dhat_mse - d))\n",
    "\n",
    "    return ahat_mse, bhat_mse, chat_mse, dhat_mse\n",
    "\n",
    "# Fair Regression with Accuracy Disparity\n",
    "def fair_regression(X, y1, y2, f1, a, f2, b, c, d, n, lam=1):\n",
    "    \"\"\"Train fair regression model minimizing accuracy disparity.\"\"\"\n",
    "    initial_guess = [1.0, 1.0]\n",
    "    bounds = [(-10, 10), (-10, 10)]\n",
    "\n",
    "    def y_mse_loss_fair(params, x, y, x1, y1, x2, y2, lam):\n",
    "        slope, intercept = params\n",
    "        yhat = slope * x + intercept\n",
    "        yg1 = yhat[:len(x1)]\n",
    "        yg2 = yhat[len(x1):]\n",
    "        disparity = np.abs(np.linalg.norm(yg1 - y1) - np.linalg.norm(yg2 - y2))\n",
    "        mse = np.mean((y - yhat) ** 2)\n",
    "        return mse + lam * disparity\n",
    "\n",
    "    def train_fair_regression_scipy(x, y, x1, y1, x2, y2, lam):\n",
    "        result = minimize(y_mse_loss_fair, initial_guess, args=(x, y, x1, y1, x2, y2, lam),\n",
    "                         bounds=bounds, method='L-BFGS-B')\n",
    "        return result.x, result.fun\n",
    "\n",
    "    # Train for Resource 1\n",
    "    w1, obj_val1 = train_fair_regression_scipy(X.flatten(), y1, f1, a, f2, b, lam)\n",
    "    fpred1 = w1[0] * X.flatten() + w1[1]\n",
    "    fmse1 = mean_squared_error(y1, fpred1)\n",
    "    ahat_fair = fpred1[:n]\n",
    "    bhat_fair = fpred1[n:]\n",
    "\n",
    "    # Train for Resource 2\n",
    "    w2, obj_val2 = train_fair_regression_scipy(X.flatten(), y2, f1, c, f2, d, lam)\n",
    "    fpred2 = w2[0] * X.flatten() + w2[1]\n",
    "    fmse2 = mean_squared_error(y2, fpred2)\n",
    "    chat_fair = fpred2[:n]\n",
    "    dhat_fair = fpred2[n:]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nMSE when fair - Resource 1:\", fmse1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_fair - a), np.linalg.norm(bhat_fair - b))\n",
    "    print(\"MSE when fair - Resource 2:\", fmse2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_fair - c), np.linalg.norm(dhat_fair - d))\n",
    "\n",
    "    return ahat_fair, bhat_fair, chat_fair, dhat_fair\n",
    "\n",
    "# Optimal Decision Making\n",
    "def opt_dec(a, b, c, d, n):\n",
    "    \"\"\"Compute optimal decisions to minimize utility difference.\"\"\"\n",
    "    # Ensure inputs are 1D arrays\n",
    "    a, b, c, d = np.asarray(a).flatten(), np.asarray(b).flatten(), np.asarray(c).flatten(), np.asarray(d).flatten()\n",
    "    \n",
    "    x = cp.Variable(n, boolean=True)  # Group 1, resource 1\n",
    "    y = cp.Variable(n, boolean=True)  # Group 1, resource 2\n",
    "    s = cp.Variable(n, boolean=True)  # Group 2, resource 1\n",
    "    t = cp.Variable(n, boolean=True)  # Group 2, resource 2\n",
    "\n",
    "    # Total benefit for each group using CVXPY's element-wise multiplication\n",
    "    u_G1 = cp.sum(cp.multiply(a, x) + cp.multiply(c, y))\n",
    "    u_G2 = cp.sum(cp.multiply(b, s) + cp.multiply(d, t))\n",
    "\n",
    "    # Minimize utility difference\n",
    "    objective = cp.Minimize((u_G1 - u_G2) ** 2)\n",
    "    constraints = [\n",
    "        *[x[i] + y[i] == 1 for i in range(n)],\n",
    "        *[s[i] + t[i] == 1 for i in range(n)],\n",
    "        cp.sum(x) + cp.sum(s) == n,\n",
    "        cp.sum(y) + cp.sum(t) == n\n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.ECOS_BB)\n",
    "    \n",
    "    if prob.status != 'optimal':\n",
    "        raise ValueError(f\"Optimization failed with status: {prob.status}\")\n",
    "    \n",
    "    return x.value.round(), y.value.round(), s.value.round(), t.value.round()\n",
    "\n",
    "# End-to-End (E2E) Training\n",
    "def e2e_training(X, a, b, c, d, n):\n",
    "    \"\"\"Train end-to-end model minimizing decision error.\"\"\"\n",
    "    initial_guess = [1, 1, 1, 1]\n",
    "    bounds = [(0, 10), (0, 10), (0, 10), (0, 10)]\n",
    "\n",
    "    def decision_loss(params, X, a, b, c, d):\n",
    "        slope_r1, intercept_r1, slope_r2, intercept_r2 = params\n",
    "        yhat_r1 = slope_r1 * X.flatten() + intercept_r1\n",
    "        yhat_r2 = slope_r2 * X.flatten() + intercept_r2\n",
    "        ap, bp, cp, dp = yhat_r1[:n], yhat_r1[n:], yhat_r2[:n], yhat_r2[n:]\n",
    "        x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "        xp, yp, sp, tp = opt_dec(ap, bp, cp, dp, n)\n",
    "        return np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp)\n",
    "\n",
    "    result = minimize(decision_loss, initial_guess, args=(X, a, b, c, d), bounds=bounds, method='L-BFGS-B')\n",
    "    k1_e2e, b1_e2e, k2_e2e, b2_e2e = result.x\n",
    "\n",
    "    y1_e2e = k1_e2e * X.flatten() + b1_e2e\n",
    "    e2e_r1 = mean_squared_error(y1, y1_e2e)\n",
    "    ahat_e2e = y1_e2e[:n]\n",
    "    bhat_e2e = y1_e2e[n:]\n",
    "\n",
    "    y2_e2e = k2_e2e * X.flatten() + b2_e2e\n",
    "    e2e_r2 = mean_squared_error(y2, y2_e2e)\n",
    "    chat_e2e = y2_e2e[:n]\n",
    "    dhat_e2e = y2_e2e[n:]\n",
    "\n",
    "    print(\"\\nE2E - Resource 1:\", e2e_r1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_e2e - a), np.linalg.norm(bhat_e2e - b))\n",
    "    print(\"E2E - Resource 2:\", e2e_r2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_e2e - c), np.linalg.norm(dhat_e2e - d))\n",
    "\n",
    "    return ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e\n",
    "\n",
    "# Fair End-to-End Training\n",
    "def fair_e2e_training(X, a, b, c, d, n, lam=1):\n",
    "    \"\"\"Train fair end-to-end model with accuracy disparity penalty.\"\"\"\n",
    "    initial_guess = [1, 1, 1, 1]\n",
    "    bounds = [(0, 10), (0, 10), (0, 10), (0, 10)]\n",
    "\n",
    "    def fair_decision_loss(params, X, a, b, c, d, lam):\n",
    "        slope_r1, intercept_r1, slope_r2, intercept_r2 = params\n",
    "        yhat_r1 = slope_r1 * X.flatten() + intercept_r1\n",
    "        yhat_r2 = slope_r2 * X.flatten() + intercept_r2\n",
    "        ap, bp, cp, dp = yhat_r1[:n], yhat_r1[n:], yhat_r2[:n], yhat_r2[n:]\n",
    "        x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "        xp, yp, sp, tp = opt_dec(ap, bp, cp, dp, n)\n",
    "        d_mse = np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp)\n",
    "        acc_disparity = np.abs(np.linalg.norm(a - ap) - np.linalg.norm(b - bp)) + \\\n",
    "                        np.abs(np.linalg.norm(c - cp) - np.linalg.norm(d - dp))\n",
    "        return d_mse + lam * acc_disparity\n",
    "\n",
    "    result = minimize(fair_decision_loss, initial_guess, args=(X, a, b, c, d, lam), bounds=bounds, method='L-BFGS-B')\n",
    "    k1_e2ef, b1_e2ef, k2_e2ef, b2_e2ef = result.x\n",
    "\n",
    "    y1_e2ef = k1_e2ef * X.flatten() + b1_e2ef\n",
    "    e2ef_r1 = mean_squared_error(y1, y1_e2ef)\n",
    "    ahat_e2ef = y1_e2ef[:n]\n",
    "    bhat_e2ef = y1_e2ef[n:]\n",
    "\n",
    "    y2_e2ef = k2_e2ef * X.flatten() + b2_e2ef\n",
    "    e2ef_r2 = mean_squared_error(y2, y2_e2ef)\n",
    "    chat_e2ef = y2_e2ef[:n]\n",
    "    dhat_e2ef = y2_e2ef[n:]\n",
    "\n",
    "    print(\"\\nFair E2E - Resource 1:\", e2ef_r1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_e2ef - a), np.linalg.norm(bhat_e2ef - b))\n",
    "    print(\"Fair E2E - Resource 2:\", e2ef_r2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_e2ef - c), np.linalg.norm(dhat_e2ef - d))\n",
    "\n",
    "    return ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef\n",
    "\n",
    "# Utility and Decision Error Calculation\n",
    "def print_decision_results(a, b, c, d, n, ahat, bhat, chat, dhat, label):\n",
    "    \"\"\"Calculate and print utility and decision error for given predictions.\"\"\"\n",
    "    x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "    xp, yp, sp, tp = opt_dec(ahat, bhat, chat, dhat, n)\n",
    "    print(f\"\\n{label} predictions\")\n",
    "    print(f\"Group 1, predicted utility: {np.dot(a, xp) + np.dot(c, yp):.2f}\")\n",
    "    print(f\"Group 2, predicted utility: {np.dot(b, sp) + np.dot(d, tp):.2f}\")\n",
    "    print(f\"Decision error: {np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp):.2f}\")\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Generate data\n",
    "    f1, f2, a, b, c, d, X, y1, y2, n = generate_data()\n",
    "\n",
    "    # True utilities\n",
    "    print(\"\\nTrue values\")\n",
    "    x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "    print(f\"Group 1, true utility: {np.dot(a, x) + np.dot(c, y):.2f}\")\n",
    "    print(f\"Group 2, true utility: {np.dot(b, s) + np.dot(d, t):.2f}\")\n",
    "\n",
    "    # Standard MSE regression\n",
    "    ahat_mse, bhat_mse, chat_mse, dhat_mse = standard_regression(X, y1, y2, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_mse, bhat_mse, chat_mse, dhat_mse, \"MSE\")\n",
    "\n",
    "    # Fair regression\n",
    "    ahat_fair, bhat_fair, chat_fair, dhat_fair = fair_regression(X, y1, y2, f1, a, f2, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_fair, bhat_fair, chat_fair, dhat_fair, \"Fair\")\n",
    "\n",
    "    # End-to-end training\n",
    "    ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e = e2e_training(X, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e, \"E2E\")\n",
    "\n",
    "    # Fair end-to-end training\n",
    "    ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef = fair_e2e_training(X, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef, \"Fair E2E\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98f637f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1, resource 1: [3.92010928 6.36953645 8.08437176 9.29323216] Mean: 6.92\n",
      "Group 1, resource 2: [2.65780815 4.97572387 6.82244801 8.21559742] Mean: 5.67\n",
      "Group 2, resource 1: [3.1022907  4.14024893 5.52803555 6.80413246] Mean: 4.89\n",
      "Group 2, resource 2: [1.98402531 2.67460536 3.62083128 4.51296925] Mean: 3.20\n",
      "\n",
      "True values\n",
      "Group 1, true utility: 22.67\n",
      "Group 2, true utility: 19.57\n",
      "\n",
      "Standard MSE - Resource 1: 0.02653498466702308\n",
      "MSE by group - G1, G2: 0.34091546499942527 0.30992986797082056\n",
      "Standard MSE - Resource 2: 0.17225324675677112\n",
      "MSE by group - G1, G2: 0.8185897791759223 0.8413897714394225\n",
      "\n",
      "MSE predictions\n",
      "Group 1, predicted utility: 22.67\n",
      "Group 2, predicted utility: 19.57\n",
      "Decision error: 0.00\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 253\u001b[0m\n\u001b[1;32m    250\u001b[0m     print_decision_results(a, b, c, d, n, ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFair E2E\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 241\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m print_decision_results(a, b, c, d, n, ahat_mse, bhat_mse, chat_mse, dhat_mse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Fair polynomial regression\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m ahat_fair, bhat_fair, chat_fair, dhat_fair \u001b[38;5;241m=\u001b[39m \u001b[43mfair_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m print_decision_results(a, b, c, d, n, ahat_fair, bhat_fair, chat_fair, dhat_fair, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFair\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# End-to-end training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 86\u001b[0m, in \u001b[0;36mfair_regression\u001b[0;34m(X, y1, y2, f1, a, f2, b, c, d, n, lam, degree)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mx, result\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Train for Resource 1\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m w1, obj_val1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fair_regression_scipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m fpred1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum([w1[i] \u001b[38;5;241m*\u001b[39m (X\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w1))], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     88\u001b[0m fmse1 \u001b[38;5;241m=\u001b[39m mean_squared_error(y1, fpred1)\n",
      "Cell \u001b[0;32mIn[52], line 81\u001b[0m, in \u001b[0;36mfair_regression.<locals>.train_fair_regression_scipy\u001b[0;34m(x, y, x1, y1, x2, y2, lam)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_fair_regression_scipy\u001b[39m(x, y, x1, y1, x2, y2, lam):\n\u001b[0;32m---> 81\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_mse_loss_fair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mx, result\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[52], line 73\u001b[0m, in \u001b[0;36mfair_regression.<locals>.y_mse_loss_fair\u001b[0;34m(params, x, y, x1, y1, x2, y2, lam)\u001b[0m\n\u001b[1;32m     71\u001b[0m yhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(x)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(params)):\n\u001b[0;32m---> 73\u001b[0m     yhat \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m params[i] \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i)\n\u001b[1;32m     74\u001b[0m yg1 \u001b[38;5;241m=\u001b[39m yhat[:\u001b[38;5;28mlen\u001b[39m(x1)]\n\u001b[1;32m     75\u001b[0m yg2 \u001b[38;5;241m=\u001b[39m yhat[\u001b[38;5;28mlen\u001b[39m(x1):]\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "# New toy example for Fair Decision Feedback Loop (FDFL)\n",
    "# Focus: Prediction fairness with accuracy disparity and nonlinear benefits\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Data Generation with Nonlinear Benefits\n",
    "def generate_data():\n",
    "    \"\"\"Generate feature and benefit data for two groups and two resources with nonlinear benefits.\"\"\"\n",
    "    f1 = np.array([5, 10, 14, 17])  # Group 1 features\n",
    "    f2 = np.array([4, 6, 9, 12])    # Group 2 features\n",
    "    n = len(f1)                     # Number of individuals per group\n",
    "\n",
    "    # Nonlinear benefits for each group and resource\n",
    "    a = 0.5 * f1**0.9 + np.log(f1 + 1)  # Group 1, resource 1\n",
    "    c = 0.3 * f1**1.1 + 0.5 * np.log(f1 + 1)  # Group 1, resource 2\n",
    "    b = 0.4 * f2**0.95 + np.log(f2 + 1)  # Group 2, resource 1\n",
    "    d = 0.2 * f2**1.05 + 0.7 * np.log(f2 + 1)  # Group 2, resource 2\n",
    "\n",
    "    # Print benefits and their means\n",
    "    print(\"Group 1, resource 1:\", a, f\"Mean: {np.mean(a):.2f}\")\n",
    "    print(\"Group 1, resource 2:\", c, f\"Mean: {np.mean(c):.2f}\")\n",
    "    print(\"Group 2, resource 1:\", b, f\"Mean: {np.mean(b):.2f}\")\n",
    "    print(\"Group 2, resource 2:\", d, f\"Mean: {np.mean(d):.2f}\")\n",
    "\n",
    "    X = np.concatenate([f1, f2]).reshape(-1, 1)\n",
    "    y1 = np.concatenate([a, b])  # Outcome for resource 1\n",
    "    y2 = np.concatenate([c, d])  # Outcome for resource 2\n",
    "    return f1, f2, a, b, c, d, X, y1, y2, n\n",
    "\n",
    "# Polynomial Regression (Standard MSE)\n",
    "def standard_regression(X, y1, y2, a, b, c, d, n, degree=2):\n",
    "    \"\"\"Perform polynomial regression for both resources.\"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Resource 1\n",
    "    reg1 = LinearRegression().fit(X_poly, y1)\n",
    "    pred1 = reg1.predict(X_poly)\n",
    "    mse1 = mean_squared_error(y1, pred1)\n",
    "    ahat_mse = pred1[:n]\n",
    "    bhat_mse = pred1[n:]\n",
    "\n",
    "    # Resource 2\n",
    "    reg2 = LinearRegression().fit(X_poly, y2)\n",
    "    pred2 = reg2.predict(X_poly)\n",
    "    mse2 = mean_squared_error(y2, pred2)\n",
    "    chat_mse = pred2[:n]\n",
    "    dhat_mse = pred2[n:]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nStandard MSE - Resource 1:\", mse1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_mse - a), np.linalg.norm(bhat_mse - b))\n",
    "    print(\"Standard MSE - Resource 2:\", mse2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_mse - c), np.linalg.norm(dhat_mse - d))\n",
    "\n",
    "    return ahat_mse, bhat_mse, chat_mse, dhat_mse\n",
    "\n",
    "# Fair Polynomial Regression with Accuracy Disparity\n",
    "def fair_regression(X, y1, y2, f1, a, f2, b, c, d, n, lam=1, degree=2):\n",
    "    \"\"\"Train fair polynomial regression model minimizing accuracy disparity.\"\"\"\n",
    "    initial_guess = [1.0] * (degree + 1)  # Coefficients for polynomial of given degree\n",
    "    bounds = [(-10, 10)] * (degree + 1)\n",
    "\n",
    "    def y_mse_loss_fair(params, x, y, x1, y1, x2, y2, lam):\n",
    "        # Construct polynomial features manually\n",
    "        yhat = np.zeros_like(x)\n",
    "        for i in range(len(params)):\n",
    "            yhat += params[i] * (x ** i)\n",
    "        yg1 = yhat[:len(x1)]\n",
    "        yg2 = yhat[len(x1):]\n",
    "        disparity = np.abs(np.linalg.norm(yg1 - y1) - np.linalg.norm(yg2 - y2))\n",
    "        mse = np.mean((y - yhat) ** 2)\n",
    "        return mse + lam * disparity\n",
    "\n",
    "    def train_fair_regression_scipy(x, y, x1, y1, x2, y2, lam):\n",
    "        result = minimize(y_mse_loss_fair, initial_guess, args=(x, y, x1, y1, x2, y2, lam),\n",
    "                         bounds=bounds, method='L-BFGS-B')\n",
    "        return result.x, result.fun\n",
    "\n",
    "    # Train for Resource 1\n",
    "    w1, obj_val1 = train_fair_regression_scipy(X.flatten(), y1, f1, a, f2, b, lam)\n",
    "    fpred1 = np.sum([w1[i] * (X.flatten() ** i) for i in range(len(w1))], axis=0)\n",
    "    fmse1 = mean_squared_error(y1, fpred1)\n",
    "    ahat_fair = fpred1[:n]\n",
    "    bhat_fair = fpred1[n:]\n",
    "\n",
    "    # Train for Resource 2\n",
    "    w2, obj_val2 = train_fair_regression_scipy(X.flatten(), y2, f1, c, f2, d, lam)\n",
    "    fpred2 = np.sum([w2[i] * (X.flatten() ** i) for i in range(len(w2))], axis=0)\n",
    "    fmse2 = mean_squared_error(y2, fpred2)\n",
    "    chat_fair = fpred2[:n]\n",
    "    dhat_fair = fpred2[n:]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nMSE when fair - Resource 1:\", fmse1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_fair - a), np.linalg.norm(bhat_fair - b))\n",
    "    print(\"MSE when fair - Resource 2:\", fmse2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_fair - c), np.linalg.norm(dhat_fair - d))\n",
    "\n",
    "    return ahat_fair, bhat_fair, chat_fair, dhat_fair\n",
    "\n",
    "# Optimal Decision Making\n",
    "def opt_dec(a, b, c, d, n):\n",
    "    \"\"\"Compute optimal decisions to minimize utility difference.\"\"\"\n",
    "    a, b, c, d = np.asarray(a).flatten(), np.asarray(b).flatten(), np.asarray(c).flatten(), np.asarray(d).flatten()\n",
    "    \n",
    "    x = cp.Variable(n, boolean=True)  # Group 1, resource 1\n",
    "    y = cp.Variable(n, boolean=True)  # Group 1, resource 2\n",
    "    s = cp.Variable(n, boolean=True)  # Group 2, resource 1\n",
    "    t = cp.Variable(n, boolean=True)  # Group 2, resource 2\n",
    "\n",
    "    # Total benefit for each group\n",
    "    u_G1 = cp.sum(cp.multiply(a, x) + cp.multiply(c, y))\n",
    "    u_G2 = cp.sum(cp.multiply(b, s) + cp.multiply(d, t))\n",
    "\n",
    "    # Minimize utility difference\n",
    "    objective = cp.Minimize((u_G1 - u_G2) ** 2)\n",
    "    constraints = [\n",
    "        *[x[i] + y[i] == 1 for i in range(n)],\n",
    "        *[s[i] + t[i] == 1 for i in range(n)],\n",
    "        cp.sum(x) + cp.sum(s) == n,\n",
    "        cp.sum(y) + cp.sum(t) == n\n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.ECOS_BB)\n",
    "    \n",
    "    if prob.status != 'optimal':\n",
    "        raise ValueError(f\"Optimization failed with status: {prob.status}\")\n",
    "    \n",
    "    return x.value.round(), y.value.round(), s.value.round(), t.value.round()\n",
    "\n",
    "# End-to-End (E2E) Training\n",
    "def e2e_training(X, a, b, c, d, n, degree=2):\n",
    "    \"\"\"Train end-to-end polynomial model minimizing decision error.\"\"\"\n",
    "    initial_guess = [1] * (degree + 1) * 2  # Coefficients for two polynomials\n",
    "    bounds = [(0, 10)] * (degree + 1) * 2\n",
    "\n",
    "    def decision_loss(params, X, a, b, c, d):\n",
    "        half = len(params) // 2\n",
    "        w1, w2 = params[:half], params[half:]\n",
    "        yhat_r1 = np.sum([w1[i] * (X.flatten() ** i) for i in range(len(w1))], axis=0)\n",
    "        yhat_r2 = np.sum([w2[i] * (X.flatten() ** i) for i in range(len(w2))], axis=0)\n",
    "        ap, bp, cp, dp = yhat_r1[:n], yhat_r1[n:], yhat_r2[:n], yhat_r2[n:]\n",
    "        x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "        xp, yp, sp, tp = opt_dec(ap, bp, cp, dp, n)\n",
    "        return np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp)\n",
    "\n",
    "    result = minimize(decision_loss, initial_guess, args=(X, a, b, c, d), bounds=bounds, method='L-BFGS-B')\n",
    "    half = len(result.x) // 2\n",
    "    w1, w2 = result.x[:half], result.x[half:]\n",
    "\n",
    "    y1_e2e = np.sum([w1[i] * (X.flatten() ** i) for i in range(len(w1))], axis=0)\n",
    "    e2e_r1 = mean_squared_error(y1, y1_e2e)\n",
    "    ahat_e2e = y1_e2e[:n]\n",
    "    bhat_e2e = y1_e2e[n:]\n",
    "\n",
    "    y2_e2e = np.sum([w2[i] * (X.flatten() ** i) for i in range(len(w2))], axis=0)\n",
    "    e2e_r2 = mean_squared_error(y2, y2_e2e)\n",
    "    chat_e2e = y2_e2e[:n]\n",
    "    dhat_e2e = y2_e2e[n:]\n",
    "\n",
    "    print(\"\\nE2E - Resource 1:\", e2e_r1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_e2e - a), np.linalg.norm(bhat_e2e - b))\n",
    "    print(\"E2E - Resource 2:\", e2e_r2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_e2e - c), np.linalg.norm(dhat_e2e - d))\n",
    "\n",
    "    return ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e\n",
    "\n",
    "# Fair End-to-End Training\n",
    "def fair_e2e_training(X, a, b, c, d, n, lam=1, degree=2):\n",
    "    \"\"\"Train fair end-to-end polynomial model with accuracy disparity penalty.\"\"\"\n",
    "    initial_guess = [1] * (degree + 1) * 2\n",
    "    bounds = [(0, 10)] * (degree + 1) * 2\n",
    "\n",
    "    def fair_decision_loss(params, X, a, b, c, d, lam):\n",
    "        half = len(params) // 2\n",
    "        w1, w2 = params[:half], params[half:]\n",
    "        yhat_r1 = np.sum([w1[i] * (X.flatten() ** i) for i in range(len(w1))], axis=0)\n",
    "        yhat_r2 = np.sum([w2[i] * (X.flatten() ** i) for i in range(len(w2))], axis=0)\n",
    "        ap, bp, cp, dp = yhat_r1[:n], yhat_r1[n:], yhat_r2[:n], yhat_r2[n:]\n",
    "        x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "        xp, yp, sp, tp = opt_dec(ap, bp, cp, dp, n)\n",
    "        d_mse = np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp)\n",
    "        acc_disparity = np.abs(np.linalg.norm(a - ap) - np.linalg.norm(b - bp)) + \\\n",
    "                        np.abs(np.linalg.norm(c - cp) - np.linalg.norm(d - dp))\n",
    "        return d_mse + lam * acc_disparity\n",
    "\n",
    "    result = minimize(fair_decision_loss, initial_guess, args=(X, a, b, c, d, lam), bounds=bounds, method='L-BFGS-B')\n",
    "    half = len(result.x) // 2\n",
    "    w1, w2 = result.x[:half], result.x[half:]\n",
    "\n",
    "    y1_e2ef = np.sum([w1[i] * (X.flatten() ** i) for i in range(len(w1))], axis=0)\n",
    "    e2ef_r1 = mean_squared_error(y1, y1_e2ef)\n",
    "    ahat_e2ef = y1_e2ef[:n]\n",
    "    bhat_e2ef = y1_e2ef[n:]\n",
    "\n",
    "    y2_e2ef = np.sum([w2[i] * (X.flatten() ** i) for i in range(len(w2))], axis=0)\n",
    "    e2ef_r2 = mean_squared_error(y2, y2_e2ef)\n",
    "    chat_e2ef = y2_e2ef[:n]\n",
    "    dhat_e2ef = y2_e2ef[n:]\n",
    "\n",
    "    print(\"\\nFair E2E - Resource 1:\", e2ef_r1)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(ahat_e2ef - a), np.linalg.norm(bhat_e2ef - b))\n",
    "    print(\"Fair E2E - Resource 2:\", e2ef_r2)\n",
    "    print(\"MSE by group - G1, G2:\", np.linalg.norm(chat_e2ef - c), np.linalg.norm(dhat_e2ef - d))\n",
    "\n",
    "    return ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef\n",
    "\n",
    "# Utility and Decision Error Calculation\n",
    "def print_decision_results(a, b, c, d, n, ahat, bhat, chat, dhat, label):\n",
    "    \"\"\"Calculate and print utility and decision error for given predictions.\"\"\"\n",
    "    x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "    xp, yp, sp, tp = opt_dec(ahat, bhat, chat, dhat, n)\n",
    "    print(f\"\\n{label} predictions\")\n",
    "    print(f\"Group 1, predicted utility: {np.dot(a, xp) + np.dot(c, yp):.2f}\")\n",
    "    print(f\"Group 2, predicted utility: {np.dot(b, sp) + np.dot(d, tp):.2f}\")\n",
    "    print(f\"Decision error: {np.linalg.norm(x - xp) + np.linalg.norm(y - yp) + np.linalg.norm(s - sp) + np.linalg.norm(t - tp):.2f}\")\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Generate data\n",
    "    f1, f2, a, b, c, d, X, y1, y2, n = generate_data()\n",
    "\n",
    "    # True utilities\n",
    "    print(\"\\nTrue values\")\n",
    "    x, y, s, t = opt_dec(a, b, c, d, n)\n",
    "    print(f\"Group 1, true utility: {np.dot(a, x) + np.dot(c, y):.2f}\")\n",
    "    print(f\"Group 2, true utility: {np.dot(b, s) + np.dot(d, t):.2f}\")\n",
    "\n",
    "    # Standard MSE regression (polynomial)\n",
    "    ahat_mse, bhat_mse, chat_mse, dhat_mse = standard_regression(X, y1, y2, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_mse, bhat_mse, chat_mse, dhat_mse, \"MSE\")\n",
    "\n",
    "    # Fair polynomial regression\n",
    "    ahat_fair, bhat_fair, chat_fair, dhat_fair = fair_regression(X, y1, y2, f1, a, f2, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_fair, bhat_fair, chat_fair, dhat_fair, \"Fair\")\n",
    "\n",
    "    # End-to-end training\n",
    "    ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e = e2e_training(X, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_e2e, bhat_e2e, chat_e2e, dhat_e2e, \"E2E\")\n",
    "\n",
    "    # Fair end-to-end training\n",
    "    ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef = fair_e2e_training(X, a, b, c, d, n)\n",
    "    print_decision_results(a, b, c, d, n, ahat_e2ef, bhat_e2ef, chat_e2ef, dhat_e2ef, \"Fair E2E\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f503b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [898.24768394   0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [898.24768394   0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "α=    0,  max|closed-form − solver| = 0.000e+00\n",
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [299.41589465   0.           0.           0.        ]\n",
      " [  0.         131.50747217   0.           0.        ]\n",
      " [ 89.07492632   0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[1.25020129e-05 1.34577150e-05 1.60728288e-05 1.15721217e-05]\n",
      " [2.99373760e+02 8.41978198e-05 6.81399539e-06 7.38655655e-06]\n",
      " [1.06880544e-05 1.31522426e+02 7.09596435e-06 5.08799434e-06]\n",
      " [8.90771415e+01 8.19658983e-06 4.33705917e-05 1.71885029e-05]]\n",
      "α=    1,  max|closed-form − solver| = 4.213e-02\n",
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [546.2301177    0.           0.           0.        ]\n",
      " [  0.         117.44324258   0.           0.        ]\n",
      " [ 25.17499689   0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[3.01028070e-06 3.12150509e-06 3.58911801e-06 2.48457990e-06]\n",
      " [5.46221231e+02 1.63973199e-05 1.53426303e-06 1.66947945e-06]\n",
      " [2.43999022e-06 1.17445043e+02 1.57982157e-06 9.94377068e-07]\n",
      " [2.51763791e+01 1.67908149e-06 1.06968592e-05 3.96895656e-06]]\n",
      "α=  0.5,  max|closed-form − solver| = 8.887e-03\n",
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [180.73747351   0.           0.           0.        ]\n",
      " [  0.         113.45806677   0.           0.        ]\n",
      " [136.60677732   0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[6.31757729e-09 6.61164504e-09 7.68889154e-09 5.31941947e-09]\n",
      " [1.80737288e+02 4.02718788e-08 3.31254599e-09 3.62075368e-09]\n",
      " [5.31553706e-09 1.13458258e+02 3.51033291e-09 2.18607445e-09]\n",
      " [1.36606703e+02 3.63392358e-09 2.23106898e-08 8.41562993e-09]]\n",
      "α=  2.0,  max|closed-form − solver| = 1.908e-04\n",
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [124.47615781   0.           0.           0.        ]\n",
      " [  0.          96.81447682   0.           0.        ]\n",
      " [164.61759959   0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[13.60272006 17.39985879 36.07285593 22.63957003]\n",
      " [ 7.68972994  7.42974012  9.62842999  8.76103617]\n",
      " [10.19372456  7.74146011  8.86824965 23.56894964]\n",
      " [10.4614675  24.06400726 11.90812788 18.43476065]]\n",
      "α=  5.0,  max|closed-form − solver| = 1.542e+02\n",
      "d_cf:\n",
      " [[  0.           0.           0.           0.        ]\n",
      " [ 94.57580649   0.           0.           0.        ]\n",
      " [  0.          84.85526991   0.           0.        ]\n",
      " [181.6132419    0.           0.           0.        ]]\n",
      "d_cvx:\n",
      " [[1.86828932e-08 2.51173341e-08 3.34472639e-08 2.31599661e-08]\n",
      " [9.45758065e+01 2.11886823e-08 9.23188298e-09 1.27797486e-08]\n",
      " [2.35909544e-08 8.48552699e+01 1.36537909e-08 5.75599215e-09]\n",
      " [1.81613242e+02 1.40673927e-08 1.60821683e-08 3.22376345e-08]]\n",
      "α=  inf,  max|closed-form − solver| = 8.159e-08\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "VERIFY group-wise α-fair closed form against CVXPY\n",
    "(one global budget, any #groups, continuous doses)\n",
    "============================================================\n",
    "pip install torch cvxpy numpy\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# CLOSED-FORM ALLOCATION   (covers α = 0,1,∞ + finite α)\n",
    "# --------------------------------------------------------------------\n",
    "def closed_form_group_alpha(\n",
    "    b_hat: np.ndarray,\n",
    "    cost: np.ndarray,\n",
    "    group: np.ndarray,\n",
    "    Q: float,\n",
    "    alpha,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_hat : (N,T)  positive utilities\n",
    "    cost  : (N,T)  positive costs\n",
    "    group : (N,)   integers 0..K-1\n",
    "    Q     : float  budget\n",
    "    alpha : float >0 or 0 or 1 or np.inf\n",
    "    Returns\n",
    "    -------\n",
    "    d_star : (N,T) optimal doses\n",
    "    \"\"\"\n",
    "    N, T = b_hat.shape\n",
    "    K = group.max() + 1\n",
    "\n",
    "    # per-group best benefit-cost ratio ρ_k and its argmax (i*,t*)\n",
    "    rho_k = np.zeros(K)\n",
    "    idx_k = np.zeros((K, 2), dtype=int)\n",
    "    for k in range(K):\n",
    "        mask = group == k\n",
    "        ratio = (b_hat[mask] / cost[mask]).reshape(-1)\n",
    "        i_star = ratio.argmax()\n",
    "        i_glob = np.where(mask)[0][i_star // T]\n",
    "        t_glob = i_star % T\n",
    "        rho_k[k] = ratio.max()\n",
    "        idx_k[k] = [i_glob, t_glob]\n",
    "\n",
    "    p_k = rho_k / np.bincount(group)  # p_k = ρ_k / G_k\n",
    "\n",
    "    # ------------ Stage I: allocate budget B_k = x_k ------------\n",
    "    if alpha == 0:  # utilitarian\n",
    "        winners = np.flatnonzero(p_k == p_k.max())\n",
    "        x = np.zeros(K)\n",
    "        x[winners] = Q / len(winners)\n",
    "    elif alpha == 1:  # logarithmic\n",
    "        x = np.full(K, Q / K)\n",
    "    elif alpha == np.inf:  # max-min\n",
    "        inv = 1 / p_k\n",
    "        x = Q * inv / inv.sum()\n",
    "    else:  # generic  (0<α<∞, α≠1)\n",
    "        weights = p_k ** (1 / alpha - 1)\n",
    "        x = Q * weights / weights.sum()\n",
    "\n",
    "    # ------------ Stage II: spend each x_k on its best item -----\n",
    "    d_star = np.zeros_like(b_hat)\n",
    "    for k in range(K):\n",
    "        i, t = idx_k[k]\n",
    "        d_star[i, t] = x[k] / cost[i, t]\n",
    "\n",
    "    return d_star\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# CVXPY benchmark (same problem)\n",
    "# --------------------------------------------------------------------\n",
    "def solve_cvxpy(b_hat, cost, group, Q, alpha):\n",
    "    N, T = b_hat.shape\n",
    "    d = cp.Variable((N, T), nonneg=True)\n",
    "\n",
    "    util = []\n",
    "    for k in range(group.max() + 1):\n",
    "        idx = group == k\n",
    "        util_k = cp.sum(cp.multiply(b_hat[idx], d[idx])) / idx.sum()\n",
    "        util.append(util_k)\n",
    "    util = cp.hstack(util)\n",
    "\n",
    "    if alpha == 0:\n",
    "        obj = cp.Maximize(cp.sum(util))\n",
    "    elif alpha == 1:\n",
    "        obj = cp.Maximize(cp.sum(cp.log(util)))\n",
    "    elif alpha == np.inf:\n",
    "        obj = cp.Maximize(cp.min(util))\n",
    "    else:\n",
    "        obj = cp.Maximize(cp.sum(util ** (1 - alpha) / (1 - alpha)))\n",
    "\n",
    "    prob = cp.Problem(obj, [cp.sum(cp.multiply(cost, d)) <= Q])\n",
    "    prob.solve()\n",
    "    return d.value\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# quick numerical experiment\n",
    "# --------------------------------------------------------------------\n",
    "def one_run(N=4, T=4, K=3, Q=200.0, alpha=2.0, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    b = 1 + 4 * rng.random((N, T))\n",
    "    c = 0.2 + 0.8 * rng.random((N, T))\n",
    "    g = rng.integers(0, K, size=N)\n",
    "\n",
    "    d_cf = closed_form_group_alpha(b, c, g, Q, alpha)\n",
    "    d_cvx = solve_cvxpy(b, c, g, Q, alpha)\n",
    "    gap = np.abs(d_cf - d_cvx).max()\n",
    "    print('d_cf:\\n', d_cf)\n",
    "    print('d_cvx:\\n', d_cvx)\n",
    "\n",
    "    print(f\"α={alpha:5},  max|closed-form − solver| = {gap:.3e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for a in [0, 1, 0.5, 2.0, 5.0, np.inf]:\n",
    "        one_run(alpha=a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08f0c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max|∇W_fd − ∇W_an| = 0.0029929353122838793\n",
      "max|Jac_fd − Jac_an| = 1.4945091919571496e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "group_alpha_derivs.py\n",
    "=====================\n",
    "\n",
    "Closed form, objective gradient, and Jacobian for\n",
    "  • single global budget Q\n",
    "  • continuous doses\n",
    "  • any α in {0, 1, ∞} ∪ (0, ∞)\n",
    "  • arbitrary number of groups K (ties assumed measure-zero)\n",
    "\n",
    "Run this file to see a finite-difference sanity check.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# closed-form allocation  (two-stage: pick best item per group, then budget)\n",
    "# ---------------------------------------------------------------------\n",
    "def closed_form_group_alpha(b, c, g, Q, alpha):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b : (N,T) positive predicted benefit   (numpy)\n",
    "    c : (N,T) positive cost\n",
    "    g : (N,)  integer group labels 0..K-1\n",
    "    Q : float budget\n",
    "    alpha : float >0   or 0   or 1   or np.inf\n",
    "    Returns\n",
    "    -------\n",
    "    d_star : (N,T) allocation (numpy)\n",
    "    idx_k  : (K,2) index (i,t) of best item for each group (needed later)\n",
    "    \"\"\"\n",
    "    N, T = b.shape\n",
    "    K = g.max() + 1\n",
    "\n",
    "    # ----- best item (i_k,t_k) & ratio ρ_k in each group --------------\n",
    "    rho = np.zeros(K)\n",
    "    idx_k = np.zeros((K, 2), dtype=int)\n",
    "\n",
    "    for k in range(K):\n",
    "        mask = g == k\n",
    "        ratio = (b[mask] / c[mask]).reshape(-1)\n",
    "        flat_idx = ratio.argmax()\n",
    "        i_global = np.where(mask)[0][flat_idx // T]\n",
    "        t_global = flat_idx % T\n",
    "        idx_k[k] = i_global, t_global\n",
    "        rho[k] = ratio.max()\n",
    "\n",
    "    G = np.bincount(g, minlength=K)\n",
    "    p = rho / G                       # p_k = ρ_k / G_k\n",
    "\n",
    "    # ---------- Stage I : budgets x_k = B_k ---------------------------\n",
    "    if alpha == 0:                    # utilitarian\n",
    "        winners = np.flatnonzero(p == p.max())\n",
    "        x = np.zeros(K)\n",
    "        x[winners] = Q / len(winners)\n",
    "    elif alpha == 1:                  # log utility\n",
    "        x = np.full(K, Q / K)\n",
    "    elif alpha == np.inf:             # max–min\n",
    "        inv = 1 / p\n",
    "        x = Q * inv / inv.sum()\n",
    "    else:                             # generic α\n",
    "        beta = 1.0 / alpha\n",
    "        weights = p ** (beta - 1)     # p^{1/α - 1}\n",
    "        x = Q * weights / weights.sum()\n",
    "\n",
    "    # ---------- Stage II : spend within group on best (i,t) -----------\n",
    "    d_star = np.zeros_like(b)\n",
    "    for k, (i, t) in enumerate(idx_k):\n",
    "        d_star[i, t] = x[k] / c[i, t]\n",
    "\n",
    "    return d_star, idx_k, p, x\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# gradient of objective  W(b)  w.r.t. each b_{it}\n",
    "# ---------------------------------------------------------------------\n",
    "def grad_W_wrt_b(b, c, g, Q, alpha):\n",
    "    \"\"\"\n",
    "    Returns  ∇_b W  with the same shape as b\n",
    "    \"\"\"\n",
    "    d_star, idx_k, p, _ = closed_form_group_alpha(b, c, g, Q, alpha)\n",
    "    K = len(p)\n",
    "    G = np.bincount(g, minlength=K)\n",
    "    grad = np.zeros_like(b)\n",
    "\n",
    "    if alpha in (0, np.inf):\n",
    "        # objective is non-smooth here; return NaNs\n",
    "        grad[:] = np.nan\n",
    "        return grad\n",
    "\n",
    "    if alpha == 1:                                  # W = Σ log u_k\n",
    "        for k, (i, t) in enumerate(idx_k):\n",
    "            grad[i, t] = 1 / (p[k] * G[k] * c[i, t])\n",
    "        return grad\n",
    "\n",
    "    # ---------- generic  0<α<∞, α≠1  -------------------------------\n",
    "    beta = 1.0 / alpha\n",
    "    D = np.sum(p ** (beta - 1))                     # denominator\n",
    "    u = Q * p ** beta / D                          # group utilities\n",
    "\n",
    "    # ∂W/∂u_k  and helper coeffs\n",
    "    dW_du = u ** (-alpha)                          # u_k^{−α}\n",
    "    coeff = Q ** (-alpha) * D ** (alpha - 2)       # common front factor\n",
    "\n",
    "    for k, (i, t) in enumerate(idx_k):\n",
    "        pk = p[k]\n",
    "        term = (beta * D - (beta - 1) * pk ** (beta - 1))\n",
    "        dW_dpk = coeff * pk ** (beta - 2) * term\n",
    "        grad[i, t] = dW_dpk / (G[k] * c[i, t])     # dp/db = 1/(G_k c)\n",
    "    return grad\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# full Jacobian  ∂d*/∂b   (sparse dictionary representation)\n",
    "# ---------------------------------------------------------------------\n",
    "def jacobian_d_wrt_b(b, c, g, Q, alpha):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    J : dict mapping (i*,t*)  ->  gradient row (N,T) as numpy array\n",
    "        Only K rows are non-zero: one per group winner (i*,t*).\n",
    "    \"\"\"\n",
    "    d_star, idx_k, p, x = closed_form_group_alpha(b, c, g, Q, alpha)\n",
    "    K = len(p)\n",
    "    G = np.bincount(g, minlength=K)\n",
    "    N, T = b.shape\n",
    "    J = {}\n",
    "\n",
    "    # special / non-smooth cases --------------------------------------\n",
    "    if alpha in (0, 1, np.inf):\n",
    "        # Jacobian exists but is piecewise-constant & sparse:\n",
    "        #  ∂d*(winner k)/∂b(winner k) via budget split; others zero.\n",
    "        # Users typically rely on sub-gradients → return NaNs.\n",
    "        for k, (i, t) in enumerate(idx_k):\n",
    "            J[(i, t)] = np.full_like(b, np.nan)\n",
    "        return J\n",
    "\n",
    "    # ---------- generic  0<α<∞, α≠1 ----------------------------------\n",
    "    beta = 1.0 / alpha\n",
    "    D = np.sum(p ** (beta - 1))\n",
    "    dD_dpk = (beta - 1) * p ** (beta - 2)          # derivative of D\n",
    "    # pre-compute    ∂x_l / ∂p_k   for every pair (l,k)\n",
    "    x_grad = np.zeros((K, K))\n",
    "    for l in range(K):\n",
    "        for k in range(K):\n",
    "            if k == l:\n",
    "                num = (beta - 1) * p[k] ** (beta - 2) * D\n",
    "                num -= p[k] ** (beta - 1) * dD_dpk[k]\n",
    "                x_grad[l, k] = Q * num / D ** 2\n",
    "            else:\n",
    "                x_grad[l, k] = -Q * p[l] ** (beta - 1) * dD_dpk[k] / D ** 2\n",
    "\n",
    "    # build Jacobian rows (only one non-zero col per group)\n",
    "    for k, (i_win, t_win) in enumerate(idx_k):\n",
    "        row = np.zeros_like(b)\n",
    "        # effect of p_k on every x_l  (thus on every winner l)\n",
    "        for l, (i_l, t_l) in enumerate(idx_k):\n",
    "            row[i_l, t_l] += (\n",
    "                x_grad[l, k] / c[i_l, t_l] / (G[k] * c[i_win, t_win])\n",
    "            )\n",
    "        J[(i_win, t_win)] = row\n",
    "    return J\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# quick finite-difference check  (generic α)\n",
    "# ---------------------------------------------------------------------\n",
    "def finite_difference_check():\n",
    "    N, T, K = 10, 3, 3\n",
    "    Q, alpha = 100.0, 2.0\n",
    "    rng = np.random.default_rng(0)\n",
    "    b = 1 + 4 * rng.random((N, T))\n",
    "    c = 0.3 + 0.7 * rng.random((N, T))\n",
    "    g = rng.integers(0, K, size=N)\n",
    "\n",
    "    # gradient of W ---------------------------------------------------\n",
    "    grad_analytic = grad_W_wrt_b(b, c, g, Q, alpha)\n",
    "    eps = 1e-6\n",
    "    grad_fd = np.zeros_like(b)\n",
    "    for i in range(N):\n",
    "        for t in range(T):\n",
    "            b_plus = b.copy()\n",
    "            b_minus = b.copy()\n",
    "            b_plus[i, t] += eps\n",
    "            b_minus[i, t] -= eps\n",
    "            W_plus = objective_value(b_plus, c, g, Q, alpha)\n",
    "            W_minus = objective_value(b_minus, c, g, Q, alpha)\n",
    "            grad_fd[i, t] = (W_plus - W_minus) / (2 * eps)\n",
    "    print(\"max|∇W_fd − ∇W_an| =\", np.abs(grad_fd - grad_analytic).max())\n",
    "\n",
    "    # Jacobian of d* --------------------------------------------------\n",
    "    J = jacobian_d_wrt_b(b, c, g, Q, alpha)\n",
    "    worst = 0.0\n",
    "    for (i0, t0), row in J.items():\n",
    "        b_eps = b.copy()\n",
    "        b_eps[i0, t0] += eps\n",
    "        d_plus, *_ = closed_form_group_alpha(b_eps, c, g, Q, alpha)\n",
    "        d_orig, *_ = closed_form_group_alpha(b, c, g, Q, alpha)\n",
    "        fd_col = (d_plus - d_orig) / eps\n",
    "        worst = max(worst, np.abs(fd_col - row).max())\n",
    "    print(\"max|Jac_fd − Jac_an| =\", worst)\n",
    "\n",
    "\n",
    "def objective_value(b, c, g, Q, alpha):\n",
    "    d, *_ = closed_form_group_alpha(b, c, g, Q, alpha)\n",
    "    K = g.max() + 1\n",
    "    G = np.bincount(g, minlength=K)\n",
    "    util = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        i, t = np.where((g == k).reshape(-1, 1))\n",
    "    # faster: read utilities from d:\n",
    "    for k in range(K):\n",
    "        mask = g == k\n",
    "        util[k] = (b[mask] * d[mask]).sum() / G[k]\n",
    "\n",
    "    if alpha == 0:\n",
    "        return util.sum()\n",
    "    if alpha == 1:\n",
    "        return np.log(util).sum()\n",
    "    if alpha == np.inf:\n",
    "        return util.min()\n",
    "    return np.sum(util ** (1 - alpha) / (1 - alpha))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    finite_difference_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57e7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
