{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Decision Focused Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Data\n",
    "2. Processing\n",
    "3. Define `optDataset`, `optModel`, and `regretLoss`.\n",
    "4. Train\n",
    "5. Eval and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\Code\\\\The Paper\\\\algorithm')\n",
    "\n",
    "from myutil import *\n",
    "from features import get_all_features\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 168)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "\n",
    "columns_to_keep = [\n",
    "    'risk_score_t', 'program_enrolled_t', 'cost_t', 'cost_avoidable_t', 'race', 'dem_female', 'gagne_sum_tm1', 'gagne_sum_t', \n",
    "    'risk_score_percentile', 'screening_eligible', 'avoidable_cost_mapped', 'propensity_score', 'g_binary', \n",
    "    'g_continuous', 'utility_binary', 'utility_continuous'\n",
    "]\n",
    "# for race 0 is white, 1 is black\n",
    "df_stat = df[columns_to_keep]\n",
    "df_feature = df[[col for col in df.columns if col not in columns_to_keep]]\n",
    "\n",
    "# Replace all values less than 0.1 with 0.1\n",
    "#df['risk_score_t'] = df['risk_score_t'].apply(lambda x: 0.1 if x < 0.1 else x)\n",
    "df['g_continuous'] = df['g_continuous'].apply(lambda x: 0.1 if x < 0.1 else x)\n",
    "\n",
    "# subset a sample of 5000 rows of df\n",
    "df = df.sample(n=20000, random_state=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12000\n",
      "Test size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Define input variables for DFL\n",
    "feats = df[get_all_features(df)].values\n",
    "risk = df['risk_score_t'].values\n",
    "gainF = df['g_continuous'].values\n",
    "decision = df['propensity_score'].values\n",
    "cost = np.ones(risk.shape)\n",
    "race = df['race'].values\n",
    "alpha = 0.5\n",
    "Q = 1000\n",
    "\n",
    "# transform the features\n",
    "scaler = StandardScaler()\n",
    "feats = scaler.fit_transform(feats)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, risk_train, risk_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    feats, gainF, risk, cost, df['race'].values, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimization and prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlphaFairness(util,alpha):\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(util))\n",
    "    elif alpha == 0:\n",
    "        return np.sum(util)\n",
    "    elif alpha == 'inf':\n",
    "        return np.min(util)\n",
    "    else:\n",
    "        return np.sum(util**(1-alpha)/(1-alpha))\n",
    "\n",
    "\n",
    "def solve_optimization(gainF, risk, cost, alpha=alpha, Q=Q):\n",
    "    # Flatten input arrays\n",
    "\n",
    "    # if any of the inputs are tensor, convert to numpy array\n",
    "    gainF = gainF.detach().cpu().numpy() if isinstance(gainF, torch.Tensor) else gainF\n",
    "    risk = risk.detach().cpu().numpy() if isinstance(risk, torch.Tensor) else risk\n",
    "    cost = cost.detach().cpu().numpy() if isinstance(cost, torch.Tensor) else cost\n",
    "\n",
    "\n",
    "    risk = risk.clip(min=0.001)\n",
    "    gainF, risk, cost = gainF.flatten(), risk.flatten(), cost.flatten()\n",
    "    d = cp.Variable(risk.shape, nonneg=True)\n",
    "\n",
    "    # raise error if dimensions do not match\n",
    "    if gainF.shape != risk.shape or risk.shape != cost.shape:\n",
    "        raise ValueError(\"Dimensions of gainF, risk, and cost do not match\")\n",
    "    \n",
    "    utils = cp.multiply(cp.multiply(gainF, risk), d)\n",
    "    \n",
    "    if alpha == 'inf':\n",
    "        # Maximin formulation\n",
    "        t = cp.Variable()  # auxiliary variable for minimum utility\n",
    "        objective = cp.Maximize(t)\n",
    "        constraints = [\n",
    "            d >= 0,\n",
    "            # d <= 1,\n",
    "            cp.sum(cost * d) <= Q,\n",
    "            utils >= t  # t is the minimum utility\n",
    "        ]\n",
    "    elif alpha == 1:\n",
    "        # Nash welfare (alpha = 1)\n",
    "        objective = cp.Maximize(cp.sum(cp.log(utils)))\n",
    "        constraints = [\n",
    "            d >= 0,\n",
    "            # d <= 1,\n",
    "            cp.sum(cost * d) <= Q\n",
    "        ]\n",
    "    elif alpha == 0:\n",
    "        # Utilitarian welfare (alpha = 0)\n",
    "        objective = cp.Maximize(cp.sum(utils))\n",
    "        constraints = [\n",
    "            d >= 0,\n",
    "            # d <= 1,\n",
    "            cp.sum(cost * d) <= Q\n",
    "        ]\n",
    "    else:\n",
    "        # General alpha-fairness\n",
    "        objective = cp.Maximize(cp.sum(utils**(1-alpha))/(1-alpha) if alpha != 0 \n",
    "                              else cp.sum(utils))\n",
    "        constraints = [\n",
    "            d >= 0,\n",
    "            # d <= 1,\n",
    "            cp.sum(cost * d) <= Q\n",
    "        ]\n",
    "    \n",
    "    # Solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.MOSEK, verbose=False, warm_start=True, mosek_params={'MSK_IPAR_LOG': 1})\n",
    "    \n",
    "    if problem.status != 'optimal':\n",
    "        print(f\"Warning: Problem status is {problem.status}\")\n",
    "    \n",
    "    optimal_decision = d.value\n",
    "    optimal_value = AlphaFairness(optimal_decision * gainF * risk, alpha)\n",
    "    \n",
    "    return optimal_decision, optimal_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FairRiskPredictor(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=149, out_features=1, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the prediction model\n",
    "class FairRiskDataset(Dataset):\n",
    "    def __init__(self, features, races, risks):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.races = torch.LongTensor(races)\n",
    "        self.risks = torch.FloatTensor(risks).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.races[idx], self.risks[idx]\n",
    "\n",
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "def train_fair_model(features, races, risks, epochs=20, batch_size=32, lambda_fairness=0):\n",
    "    \"\"\"\n",
    "    Train a fair regression model with a fairness regularizer.\n",
    "    \n",
    "    Args:\n",
    "        features (np.ndarray): Feature array.\n",
    "        races (np.ndarray): Array indicating race (0: white, 1: black).\n",
    "        risks (np.ndarray): True risk values.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        lambda_fairness (float): Weight for the fairness regularizer.\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: Trained fair regression model.\n",
    "    \"\"\"\n",
    "    dataset = FairRiskDataset(features, races, risks)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = FairRiskPredictor(features.shape[1])\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_features, batch_races, batch_risks in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_features)\n",
    "            mse_loss = criterion(predictions, batch_risks)\n",
    "            \n",
    "            # Compute fairness loss\n",
    "            group0 = predictions[batch_races == 0]\n",
    "            group1 = predictions[batch_races == 1]\n",
    "            if len(group0) > 0 and len(group1) > 0:\n",
    "                fairness_loss = torch.abs(group0.mean() - group1.mean())\n",
    "            else:\n",
    "                fairness_loss = torch.tensor(0.0)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = mse_loss + lambda_fairness * fairness_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the fair model\n",
    "# model = train_fair_model(feats_train, race, risk_train, epochs=20, lambda_fairness=0).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # save\n",
    "# torch.save(model.state_dict(), 'risk_predictor_model.pth')\n",
    "\n",
    "\n",
    "# load model from disk\n",
    "model = FairRiskPredictor(feats.shape[1])\n",
    "model.load_state_dict(torch.load('risk_predictor_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current alpha and Q values are: 0.5 1000\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "class FairDFLDataset(Dataset):\n",
    "    def __init__(self, features, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        self.features = features\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "        self.alpha = alpha\n",
    "        self.Q = Q\n",
    "\n",
    "        self.sols, self.vals = self._get_solutions()\n",
    "        self._to_tensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def _get_solutions(self):\n",
    "        sols, vals = solve_optimization(self.gainF, self.risk, self.cost, self.alpha, self.Q)\n",
    "        return sols, vals\n",
    "\n",
    "    def _to_tensor(self):\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.risk = torch.FloatTensor(self.risk)\n",
    "        self.gainF = torch.FloatTensor(self.gainF)\n",
    "        self.cost = torch.FloatTensor(self.cost)\n",
    "        self.race = torch.LongTensor(self.race)\n",
    "        self.sols = torch.FloatTensor(self.sols)\n",
    "        self.vals = torch.FloatTensor([self.vals])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.risk[idx], self.gainF[idx], self.cost[idx], self.race[idx], self.sols[idx], self.vals\n",
    "\n",
    "\n",
    "\n",
    "# test the dataset and dataloader\n",
    "dataset_train = FairDFLDataset(feats_train, risk_train, gainF_train, cost_train, race_train)\n",
    "dataset_test = FairDFLDataset(feats_test, risk_test, gainF_test, cost_test, race_test)\n",
    "print('The current alpha and Q values are:', alpha, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataLoader\n",
    "loader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(predModel, optModel, dataloader, alphas=[alpha], Q=Q):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret.\n",
    "\n",
    "    Args:\n",
    "        predModel (nn.Module): Trained prediction model.\n",
    "        optModel (nn.Module): Trained optimization model.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        alphas (list): List of alpha values for fairness.\n",
    "        Q (int): Budget constraint.\n",
    "\n",
    "    Returns:\n",
    "        float: Average normalized regret across alphas.\n",
    "    \"\"\"\n",
    "    predModel.eval()\n",
    "\n",
    "    features, risk, gainF, cost, race, true_sols, true_vals = next(iter(dataloader))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    features, risk, gainF, cost, race, true_sols, true_vals = (\n",
    "        features.to(device),\n",
    "        risk.to(device),\n",
    "        gainF.to(device),\n",
    "        cost.to(device),\n",
    "        race.to(device),\n",
    "        true_sols.to(device),\n",
    "        true_vals.to(device),\n",
    "    )\n",
    "\n",
    "    # Predict risk\n",
    "    with torch.no_grad():\n",
    "        pred_risk = predModel(features).clamp(min=0.001)  # Ensure no zero values\n",
    "\n",
    "    risk = risk.clamp(min=0.001)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    pred_risk = pred_risk.cpu().numpy()\n",
    "    risk = risk.cpu().numpy()\n",
    "    gainF = gainF.cpu().numpy()\n",
    "    cost = cost.cpu().numpy()\n",
    "\n",
    "    regrets = []\n",
    "    for alpha in alphas:\n",
    "        # Calculate true solution and objective\n",
    "        true_sol, true_obj = optModel(gainF, risk, cost, alpha, Q)\n",
    "        \n",
    "        # Calculate predicted solution\n",
    "        pred_sol, _ = optModel(gainF, pred_risk, cost, alpha, Q)\n",
    "        \n",
    "        # Calculate predicted objective using true risk\n",
    "        pred_obj = AlphaFairness(gainF * risk * pred_sol, alpha)\n",
    "        \n",
    "        # Calculate normalized regret\n",
    "        normalized_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "        regrets.append(normalized_regret)\n",
    "\n",
    "    predModel.train()\n",
    "    return np.mean(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the regret calculation\n",
    "# regret(model, solve_optimization, loader_test, alphas=[2], Q=Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "\n",
    "class MultiDirCentralDiffFunction(Function):\n",
    "    \"\"\"\n",
    "    A custom PyTorch autograd.Function that:\n",
    "      1) In the forward pass, solves once with pred_risk to get regret.\n",
    "      2) In the backward pass, uses k random directions + central difference \n",
    "         to approximate the gradient w.r.t. pred_risk.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "                pred_risk: torch.Tensor,\n",
    "                true_risk: torch.Tensor,\n",
    "                gainF:     torch.Tensor,\n",
    "                cost:      torch.Tensor,\n",
    "                alpha:     float,\n",
    "                Q:         float,\n",
    "                solver_func,\n",
    "                k_directions: int = 4,   # number of random directions\n",
    "                eps: float = 1e-4,\n",
    "                normalize_dir: bool = True):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "         1) Solve once with pred_risk -> store regret\n",
    "        Arguments:\n",
    "          - k_directions: how many random directions to use in backward\n",
    "          - eps: step size for central difference\n",
    "          - normalize_dir: whether to L2-normalize each direction\n",
    "        \"\"\"\n",
    "        device = pred_risk.device\n",
    "\n",
    "        # Convert to CPU for solver\n",
    "        pred_risk_cpu = pred_risk.detach().cpu().numpy()\n",
    "        true_risk_cpu = true_risk.detach().cpu().numpy()\n",
    "        gainF_cpu     = gainF.detach().cpu().numpy()\n",
    "        cost_cpu      = cost.detach().cpu().numpy()\n",
    "\n",
    "        # Solve with pred_risk\n",
    "        d_pred, _ = solver_func(gainF_cpu, pred_risk_cpu, cost_cpu, alpha, Q)\n",
    "\n",
    "        # Solve for best utility with true_risk\n",
    "        d_opt_true, _ = solver_func(gainF_cpu, true_risk_cpu, cost_cpu, alpha, Q)\n",
    "\n",
    "        # Compute predicted util under true_risk\n",
    "        pred_util = (gainF_cpu * true_risk_cpu * d_pred).sum()\n",
    "        best_util = (gainF_cpu * true_risk_cpu * d_opt_true).sum()\n",
    "\n",
    "        regret = best_util - pred_util\n",
    "        loss_tensor = torch.tensor(regret, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Save for backward\n",
    "        ctx.solver_func   = solver_func\n",
    "        ctx.alpha         = alpha\n",
    "        ctx.Q             = Q\n",
    "        ctx.k_directions  = k_directions\n",
    "        ctx.eps           = eps\n",
    "        ctx.normalize_dir = normalize_dir\n",
    "\n",
    "        # We'll need these in backward\n",
    "        ctx.save_for_backward(pred_risk, true_risk, gainF, cost, loss_tensor)\n",
    "\n",
    "        # Also store these for reference\n",
    "        ctx.best_util      = best_util\n",
    "        ctx.orig_regret    = regret\n",
    "\n",
    "        return loss_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Uses k random directions + central difference approximation:\n",
    "          grad ~ (1/k) sum_j [ (f(r + eps u_j) - f(r - eps u_j)) / (2 eps) ] * u_j\n",
    "        \"\"\"\n",
    "        solver_func   = ctx.solver_func\n",
    "        alpha         = ctx.alpha\n",
    "        Q             = ctx.Q\n",
    "        k_directions  = ctx.k_directions\n",
    "        eps           = ctx.eps\n",
    "        normalize_dir = ctx.normalize_dir\n",
    "\n",
    "        pred_risk, true_risk, gainF, cost, loss_val = ctx.saved_tensors\n",
    "        best_util      = ctx.best_util\n",
    "        original_regret= ctx.orig_regret\n",
    "\n",
    "        device = pred_risk.device\n",
    "        n = pred_risk.numel()\n",
    "\n",
    "        # Move to CPU for solver\n",
    "        pred_risk_cpu = pred_risk.detach().cpu().numpy().copy()\n",
    "        true_risk_cpu = true_risk.detach().cpu().numpy()\n",
    "        gainF_cpu     = gainF.detach().cpu().numpy()\n",
    "        cost_cpu      = cost.detach().cpu().numpy()\n",
    "\n",
    "        # Original loss = original_regret\n",
    "        orig_loss = original_regret\n",
    "\n",
    "        # We'll accumulate gradient across k directions\n",
    "        grad_approx_np = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "        # For each random direction\n",
    "        for _ in range(k_directions):\n",
    "            # Sample a random direction u\n",
    "            u = np.random.normal(size=(n,))\n",
    "            if normalize_dir:\n",
    "                norm_u = np.sqrt((u**2).sum())\n",
    "                if norm_u > 1e-12:\n",
    "                    u /= norm_u  # L2-normalize\n",
    "\n",
    "            # Evaluate f(r + eps * u)\n",
    "            pred_risk_plus = pred_risk_cpu + eps * u\n",
    "            d_pred_plus, _ = solver_func(gainF_cpu, pred_risk_plus, cost_cpu, alpha, Q)\n",
    "            pred_util_plus = (gainF_cpu * true_risk_cpu * d_pred_plus).sum()\n",
    "            loss_plus = best_util - pred_util_plus  # regret\n",
    "\n",
    "            # Evaluate f(r - eps * u)\n",
    "            pred_risk_minus = pred_risk_cpu - eps * u\n",
    "            d_pred_minus, _ = solver_func(gainF_cpu, pred_risk_minus, cost_cpu, alpha, Q)\n",
    "            pred_util_minus = (gainF_cpu * true_risk_cpu * d_pred_minus).sum()\n",
    "            loss_minus = best_util - pred_util_minus\n",
    "\n",
    "            # Central diff\n",
    "            fd_val = (loss_plus - loss_minus) / (2 * eps)\n",
    "            grad_approx_np += fd_val * u  # accumulate\n",
    "\n",
    "        # Average over k directions\n",
    "        grad_approx_np /= float(k_directions)\n",
    "\n",
    "        # Convert to torch\n",
    "        grad_approx = torch.from_numpy(grad_approx_np).to(device)\n",
    "\n",
    "        # Chain rule\n",
    "        grad_approx *= grad_output.item()\n",
    "\n",
    "        # Return gradient wrt pred_risk, None for the other inputs\n",
    "        return grad_approx, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Wrap the function in an nn.Module\n",
    "###############################################################################\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiDirCentralDiffLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A 'best-practice' random FD approach:\n",
    "      - multiple random directions (k)\n",
    "      - central difference\n",
    "      - optional normalization\n",
    "    This yields a higher-fidelity gradient estimate than one-sided or single-direction FD.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=alpha, Q=Q, solver_func=None,\n",
    "                 k_directions=4, eps=1e-4, normalize_dir=True):\n",
    "        super().__init__()\n",
    "        self.alpha         = alpha\n",
    "        self.Q             = Q\n",
    "        self.solver_func   = solver_func\n",
    "        self.k_directions  = k_directions\n",
    "        self.eps           = eps\n",
    "        self.normalize_dir = normalize_dir\n",
    "\n",
    "    def forward(self, pred_risk, true_risk, gainF, cost):\n",
    "        return MultiDirCentralDiffFunction.apply(\n",
    "            pred_risk, \n",
    "            true_risk, \n",
    "            gainF, \n",
    "            cost, \n",
    "            self.alpha, \n",
    "            self.Q,\n",
    "            self.solver_func,\n",
    "            self.k_directions,\n",
    "            self.eps,\n",
    "            self.normalize_dir\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_focused_mdcd(\n",
    "    predModel,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    alpha=alpha,\n",
    "    Q=Q,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    "    k_directions=4,\n",
    "    eps=1e-4,\n",
    "    normalize_dir=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Example training function using MultiDirCentralDiffLoss\n",
    "    with multiple random directions + central difference\n",
    "    for more accurate (but still approximate) gradients.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predModel.to(device)\n",
    "    predModel.train()\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    import time\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=len(test_dataset),  shuffle=False)\n",
    "\n",
    "    # Our advanced FD-based DFL criterion\n",
    "    dfl_loss = MultiDirCentralDiffLoss(\n",
    "        alpha=alpha, Q=Q, solver_func=solve_optimization,\n",
    "        k_directions=k_directions, eps=eps, normalize_dir=normalize_dir\n",
    "    )\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(predModel.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_log = []\n",
    "    train_mse_log  = []\n",
    "    test_mse_log   = []\n",
    "    test_regret_log= []\n",
    "    epoch_times = []\n",
    "\n",
    "    # Evaluate initial test regret\n",
    "    initial_regret = regret(predModel, solve_optimization, test_loader, alphas=[alpha], Q=Q)\n",
    "    print(f\"Initial Test Regret: {initial_regret:.4f}\")\n",
    "    test_regret_log.append(initial_regret)\n",
    "\n",
    "    # Evaluate initial test MSE\n",
    "    with torch.no_grad():\n",
    "        feats_test, risk_test, gainF_test, cost_test, race_test, _, _ = next(iter(test_loader))\n",
    "        feats_test, risk_test = feats_test.to(device), risk_test.to(device)\n",
    "        pred_test  = predModel(feats_test).view(-1)\n",
    "        init_mse   = mse_criterion(pred_test, risk_test.view(-1))\n",
    "    test_mse_log.append(init_mse.item())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for feats, risk_true, gainF, cost, race, _, _ in train_loader:\n",
    "            feats     = feats.to(device)\n",
    "            risk_true = risk_true.to(device)\n",
    "            gainF     = gainF.to(device)\n",
    "            cost      = cost.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict risk\n",
    "            pred_risk = predModel(feats).view(-1)\n",
    "            # Multi-direction central diff loss\n",
    "            loss = dfl_loss(pred_risk, risk_true, gainF, cost)\n",
    "\n",
    "            # Also measure MSE for logging\n",
    "            mse_batch = mse_criterion(pred_risk, risk_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_log.append(loss.item())\n",
    "            train_mse_log.append(mse_batch.item())\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        # Evaluate test regret\n",
    "        test_reg = regret(predModel, solve_optimization, test_loader, alphas=[alpha], Q=Q)\n",
    "\n",
    "        # Evaluate test MSE\n",
    "        with torch.no_grad():\n",
    "            feats_test, risk_test, gainF_test, cost_test, race_test, _, _ = next(iter(test_loader))\n",
    "            feats_test, risk_test = feats_test.to(device), risk_test.to(device)\n",
    "            pred_test = predModel(feats_test).view(-1)\n",
    "            test_mse  = mse_criterion(pred_test, risk_test.view(-1))\n",
    "\n",
    "        test_regret_log.append(test_reg)\n",
    "        test_mse_log.append(test_mse.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
    "              f\" | TrainRegretLoss={loss.item():.4f}\"\n",
    "              f\" | TrainMSE={mse_batch.item():.4f}\"\n",
    "              f\" | TestRegret={test_reg:.4f}\"\n",
    "              f\" | TestMSE={test_mse:.4f}\"\n",
    "              f\" | Time={epoch_time:.2f}s\")\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss_log,\n",
    "        \"train_mse\": train_mse_log,\n",
    "        \"test_regret\": test_regret_log,\n",
    "        \"test_mse\": test_mse_log,\n",
    "        \"epoch_times\": epoch_times,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# results = train_decision_focused_mdcd(model, dataset_train, dataset_test, alpha=alpha, Q=Q, epochs=20, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cvxpy as cp\n",
    "\n",
    "class FairDFLDataset(Dataset):\n",
    "    \"\"\"Dataset class for Fair DFL\"\"\"\n",
    "    def __init__(self, features, risk, gainF, cost, race, bb_problem):\n",
    "        self.features = features\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "        self.bb_problem = bb_problem\n",
    "        \n",
    "        self.sols, self.vals = self._get_solutions()\n",
    "        self._to_tensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def _get_solutions(self):\n",
    "        sols, vals = self.bb_problem.solve_optimization(self.gainF, self.risk, self.cost)\n",
    "        return sols, vals\n",
    "\n",
    "    def _to_tensor(self):\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.risk = torch.FloatTensor(self.risk)\n",
    "        self.gainF = torch.FloatTensor(self.gainF)\n",
    "        self.cost = torch.FloatTensor(self.cost)\n",
    "        self.race = torch.LongTensor(self.race)\n",
    "        self.sols = torch.FloatTensor(self.sols)\n",
    "        self.vals = torch.FloatTensor([self.vals])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.features[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sols[idx],\n",
    "            self.vals\n",
    "        )\n",
    "\n",
    "class BaseProblem:\n",
    "    \"\"\"Base problem class defining the optimization problem\"\"\"\n",
    "    def __init__(self, alpha=alpha, Q=Q):\n",
    "        self.alpha = float(alpha)  # Ensure alpha is float\n",
    "        self.Q = float(Q)  # Ensure Q is float\n",
    "        self.num_feats = None\n",
    "        self.lancer_out_activation = \"relu\"\n",
    "    \n",
    "    def solve_optimization(self, gainF, risk, cost):\n",
    "        \"\"\"Solves the optimization problem\"\"\"\n",
    "        # Convert tensors to numpy if needed\n",
    "        gainF = gainF.detach().cpu().numpy() if isinstance(gainF, torch.Tensor) else gainF\n",
    "        risk = risk.detach().cpu().numpy() if isinstance(risk, torch.Tensor) else risk\n",
    "        cost = cost.detach().cpu().numpy() if isinstance(cost, torch.Tensor) else cost\n",
    "        \n",
    "        risk = risk.clip(min=0.001)\n",
    "        gainF, risk, cost = gainF.flatten(), risk.flatten(), cost.flatten()\n",
    "        d = cp.Variable(risk.shape, nonneg=True)\n",
    "        \n",
    "        utils = cp.multiply(cp.multiply(gainF, risk), d)\n",
    "        \n",
    "        if self.alpha == float('inf'):\n",
    "            t = cp.Variable()\n",
    "            objective = cp.Maximize(t)\n",
    "            constraints = [\n",
    "                d >= 0,\n",
    "                cp.sum(cost * d) <= self.Q,\n",
    "                utils >= t\n",
    "            ]\n",
    "        elif self.alpha == 1:\n",
    "            objective = cp.Maximize(cp.sum(cp.log(utils)))\n",
    "            constraints = [\n",
    "                d >= 0,\n",
    "                cp.sum(cost * d) <= self.Q\n",
    "            ]\n",
    "        elif self.alpha == 0:\n",
    "            objective = cp.Maximize(cp.sum(utils))\n",
    "            constraints = [\n",
    "                d >= 0,\n",
    "                cp.sum(cost * d) <= self.Q\n",
    "            ]\n",
    "        else:\n",
    "            objective = cp.Maximize(cp.sum(cp.power(utils, 1 - self.alpha)) / (1 - self.alpha))\n",
    "            constraints = [\n",
    "                d >= 0,\n",
    "                cp.sum(cost * d) <= self.Q\n",
    "            ]\n",
    "        \n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        try:\n",
    "            problem.solve(solver=cp.MOSEK, verbose=False)\n",
    "            \n",
    "            if problem.status != 'optimal':\n",
    "                print(f\"Warning: Problem status is {problem.status}\")\n",
    "                \n",
    "            optimal_decision = d.value\n",
    "            optimal_value = self.eval_utility(optimal_decision, gainF, risk)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Optimization error: {e}\")\n",
    "            optimal_decision = np.zeros(risk.shape)\n",
    "            optimal_value = 0.0\n",
    "            \n",
    "        return optimal_decision, optimal_value\n",
    "\n",
    "    def eval_utility(self, decision, gainF, risk):\n",
    "        \"\"\"Evaluates the utility function\"\"\"\n",
    "        utils = decision * gainF * risk\n",
    "        if self.alpha == 1:\n",
    "            return np.sum(np.log(utils + 1e-10))\n",
    "        elif self.alpha == 0:\n",
    "            return np.sum(utils)\n",
    "        elif self.alpha == float('inf'):\n",
    "            return np.min(utils)\n",
    "        else:\n",
    "            return np.sum(np.power(utils, 1 - self.alpha)) / (1 - self.alpha)\n",
    "    \n",
    "    def get_c_shapes(self):\n",
    "        \"\"\"Returns shapes for the C model\"\"\"\n",
    "        return self.num_feats, 1\n",
    "    \n",
    "    def get_activations(self):\n",
    "        \"\"\"Returns activation functions for the models\"\"\"\n",
    "        return \"tanh\", \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCModel(nn.Module):\n",
    "    \"\"\"MLP-based C Model for predicting risk scores\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + [hidden_dim]*n_layers + [output_dim]\n",
    "        \n",
    "        for i in range(len(dims)-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.ReLU() if i < len(dims)-2 else nn.Softplus()\n",
    "            ])\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MLPLancer(nn.Module):\n",
    "    \"\"\"MLP-based LANCER Model for estimating decision loss\"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + [hidden_dim]*n_layers + [1]\n",
    "        \n",
    "        for i in range(len(dims)-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.ReLU() if i < len(dims)-2 else nn.Identity()\n",
    "            ])\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, z_pred, z_true):\n",
    "        # Ensure correct dimensions\n",
    "        z_pred = z_pred.view(-1, 1)\n",
    "        z_true = z_true.view(-1, 1)\n",
    "        diff = torch.square(z_pred - z_true)\n",
    "        return self.model(diff)\n",
    "\n",
    "class LancerLearner:\n",
    "    \"\"\"Main LANCER learning framework\"\"\"\n",
    "    def __init__(self, bb_problem, c_model, lancer_model, device='cuda', \n",
    "                 c_lr=1e-4, lancer_lr=1e-4, weight_decay=1e-5):\n",
    "        self.bb_problem = bb_problem\n",
    "        self.c_model = c_model.to(device)\n",
    "        self.lancer_model = lancer_model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Use lower learning rates and add weight decay\n",
    "        self.c_optimizer = optim.AdamW(\n",
    "            self.c_model.parameters(),\n",
    "            lr=c_lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.lancer_optimizer = optim.AdamW(\n",
    "            self.lancer_model.parameters(),\n",
    "            lr=lancer_lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Add learning rate schedulers\n",
    "        self.c_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.c_optimizer, mode='min', factor=0.5, patience=2\n",
    "        )\n",
    "        self.lancer_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.lancer_optimizer, mode='min', factor=0.5, patience=2\n",
    "        )\n",
    "        \n",
    "    def train_step(self, feats, risk, gainF, cost, race, sols, vals):\n",
    "        # Convert to tensors if they aren't already\n",
    "        if not isinstance(feats, torch.Tensor):\n",
    "            feats = torch.FloatTensor(feats)\n",
    "        if not isinstance(risk, torch.Tensor):\n",
    "            risk = torch.FloatTensor(risk)\n",
    "        if not isinstance(gainF, torch.Tensor):\n",
    "            gainF = torch.FloatTensor(gainF)\n",
    "        if not isinstance(cost, torch.Tensor):\n",
    "            cost = torch.FloatTensor(cost)\n",
    "        if not isinstance(sols, torch.Tensor):\n",
    "            sols = torch.FloatTensor(sols)\n",
    "            \n",
    "        feats = feats.to(self.device)\n",
    "        risk = risk.to(self.device)\n",
    "        gainF = gainF.to(self.device)\n",
    "        cost = cost.to(self.device)\n",
    "        sols = sols.to(self.device)\n",
    "        \n",
    "        # Train LANCER model\n",
    "        self.lancer_optimizer.zero_grad()\n",
    "        z_pred = self.c_model(feats)\n",
    "        lancer_pred = self.lancer_model(z_pred, risk)\n",
    "        lancer_loss = self.lancer_model.loss_fn(lancer_pred, sols.view(-1, 1))\n",
    "        \n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.lancer_model.parameters(), max_norm=1.0)\n",
    "        lancer_loss.backward()\n",
    "        self.lancer_optimizer.step()\n",
    "        \n",
    "        # Train C model using LANCER loss\n",
    "        self.c_optimizer.zero_grad()\n",
    "        z_pred = self.c_model(feats)\n",
    "        c_loss = torch.mean(self.lancer_model(z_pred, risk))\n",
    "        \n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.c_model.parameters(), max_norm=1.0)\n",
    "        c_loss.backward()\n",
    "        self.c_optimizer.step()\n",
    "        \n",
    "        return lancer_loss.item(), c_loss.item()\n",
    "    \n",
    "    def train(self, train_loader, test_loader, n_epochs=10, early_stop_patience=5):\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            self.c_model.train()\n",
    "            self.lancer_model.train()\n",
    "            \n",
    "            epoch_lancer_loss = 0\n",
    "            epoch_c_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                feats, risk, gainF, cost, race, sols, vals = batch\n",
    "                lancer_loss, c_loss = self.train_step(\n",
    "                    feats, risk, gainF, cost, race, sols, vals\n",
    "                )\n",
    "                epoch_lancer_loss += lancer_loss\n",
    "                epoch_c_loss += c_loss\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Calculate average losses\n",
    "            avg_lancer_loss = epoch_lancer_loss / num_batches\n",
    "            avg_c_loss = epoch_c_loss / num_batches\n",
    "            \n",
    "            # Calculate regret on test set\n",
    "            self.c_model.eval()\n",
    "            train_regret = regret(self.c_model, solve_optimization, train_loader, alphas=[alpha], Q=Q)\n",
    "            \n",
    "            # Update learning rate schedulers\n",
    "            self.lancer_scheduler.step(avg_lancer_loss)\n",
    "            self.c_scheduler.step(avg_c_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            current_loss = avg_lancer_loss + avg_c_loss\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(f\"LANCER Loss: {avg_lancer_loss:.4f}\")\n",
    "            print(f\"C Model Loss: {avg_c_loss:.4f}\")\n",
    "            print(f\"Train Regret: {train_regret:.4f}\")\n",
    "            print(f\"Learning rates - LANCER: {self.lancer_optimizer.param_groups[0]['lr']:.6f}, \"\n",
    "                  f\"C: {self.c_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    def predict(self, feats):\n",
    "        self.c_model.eval()\n",
    "        with torch.no_grad():\n",
    "            feats = torch.FloatTensor(feats).to(self.device)\n",
    "            return self.c_model(feats).cpu().numpy()\n",
    "\n",
    "def setup_training(feats_train, risk_train, gainF_train, cost_train, race_train, \n",
    "                  alpha=alpha, Q=Q, batch_size=32):\n",
    "    # Initialize problem\n",
    "    bb_problem = BaseProblem(alpha=alpha, Q=Q)\n",
    "    bb_problem.num_feats = feats_train.shape[1]\n",
    "    \n",
    "    # Initialize models\n",
    "    c_model = MLPCModel(\n",
    "        input_dim=feats_train.shape[1],\n",
    "        output_dim=1\n",
    "    )\n",
    "    \n",
    "    lancer_model = MLPLancer(\n",
    "        input_dim=1  # For squared difference between pred and true\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = FairDFLDataset(\n",
    "        feats_train, risk_train, gainF_train, cost_train, race_train,\n",
    "        bb_problem\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Initialize learner\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    learner = LancerLearner(bb_problem, c_model, lancer_model, device)\n",
    "    \n",
    "    return learner, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "LANCER Loss: 5.3529\n",
      "C Model Loss: 0.1646\n",
      "Train Regret: 0.0509\n",
      "Learning rates - LANCER: 0.000100, C: 0.000100\n",
      "Epoch 2/10\n",
      "LANCER Loss: 0.0251\n",
      "C Model Loss: 0.0806\n",
      "Train Regret: 0.0896\n",
      "Learning rates - LANCER: 0.000100, C: 0.000100\n",
      "Epoch 3/10\n",
      "LANCER Loss: 0.9652\n",
      "C Model Loss: 0.0897\n",
      "Train Regret: 0.1863\n",
      "Learning rates - LANCER: 0.000100, C: 0.000100\n",
      "Epoch 4/10\n",
      "LANCER Loss: 0.3630\n",
      "C Model Loss: 0.0753\n",
      "Train Regret: 0.1358\n",
      "Learning rates - LANCER: 0.000100, C: 0.000100\n",
      "Epoch 5/10\n",
      "LANCER Loss: 0.1500\n",
      "C Model Loss: 0.0806\n",
      "Train Regret: 0.0870\n",
      "Learning rates - LANCER: 0.000050, C: 0.000100\n",
      "Epoch 6/10\n",
      "LANCER Loss: 0.0058\n",
      "C Model Loss: 0.0859\n",
      "Train Regret: 0.0646\n",
      "Learning rates - LANCER: 0.000050, C: 0.000100\n",
      "Epoch 7/10\n",
      "LANCER Loss: 0.0110\n",
      "C Model Loss: 0.0837\n",
      "Train Regret: 0.0345\n",
      "Learning rates - LANCER: 0.000050, C: 0.000050\n",
      "Epoch 8/10\n",
      "LANCER Loss: 0.0503\n",
      "C Model Loss: 0.0802\n",
      "Train Regret: 0.1056\n",
      "Learning rates - LANCER: 0.000050, C: 0.000050\n",
      "Epoch 9/10\n",
      "LANCER Loss: 0.3788\n",
      "C Model Loss: 0.0796\n",
      "Train Regret: 0.0904\n",
      "Learning rates - LANCER: 0.000025, C: 0.000050\n",
      "Epoch 10/10\n",
      "LANCER Loss: 0.0058\n",
      "C Model Loss: 0.0854\n",
      "Train Regret: 0.1431\n",
      "Learning rates - LANCER: 0.000025, C: 0.000025\n"
     ]
    }
   ],
   "source": [
    "# Create train and test dataloaders\n",
    "learner, train_dataloader = setup_training(\n",
    "    feats_train, risk_train, gainF_train, cost_train, race_train,\n",
    "    alpha=alpha, Q=Q, batch_size=32)\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = FairDFLDataset(\n",
    "    feats_test, risk_test, gainF_test, cost_test, race_test,\n",
    "    learner.bb_problem\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Train with regret tracking\n",
    "learner.train(train_dataloader, test_dataloader, n_epochs=10, early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancer_pred_risk = learner.predict(feats_test)\n",
    "lancer_pred_risk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANCER regret on Test is: 0.14386002851309154\n",
      "2-Stage Regret on Test is: 0.39090804755143727\n"
     ]
    }
   ],
   "source": [
    "print(\"LANCER regret on Test is:\", regret(learner.c_model, solve_optimization, test_dataloader, alphas=[alpha], Q=Q))\n",
    "print(\"2-Stage Regret on Test is:\",regret(model, solve_optimization, test_dataloader, alphas=[alpha], Q=Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANCER regret on Train is: 0.0731937018544552\n",
      "2-Stage Regret on Train is: 0.40421903602272125\n"
     ]
    }
   ],
   "source": [
    "print(\"LANCER regret on Train is:\", regret(learner.c_model, solve_optimization, train_dataloader, alphas=[alpha], Q=Q))\n",
    "print(\"2-Stage Regret on Train is:\",regret(model, solve_optimization, train_dataloader, alphas=[alpha], Q=Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
