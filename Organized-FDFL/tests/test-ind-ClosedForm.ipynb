{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0469d29",
   "metadata": {},
   "source": [
    "- Change min-risk from 0.001 to 1\n",
    "\n",
    "- Write fold-opt subsection.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9450ea9",
   "metadata": {},
   "source": [
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "     - Closed-Form Done\n",
    "     - <b>2-Stage Done</b>\n",
    "\n",
    "6. <b>For Fold-OPT Change PGD closed-form to solver.</b>\n",
    "\n",
    "8. Verify Individual and Group Regret Performance Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66fb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "     AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6818df",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39fa5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "da3d9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = pd.read_csv('E:\\\\myREPO\\\\Fairness-Decision-Focused-Loss\\\\Organized-FDFL\\\\src\\\\data\\\\data.csv')\n",
    "\n",
    "df = df.sample(n=20,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 1) \n",
    "benefit = benefit + 1\n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f20f2",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cef7387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd6cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df314909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d661549",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37802e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ea0565",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a78a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "22d84524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10\n",
      "Test size: 10\n",
      "First five feats: tensor([[ 0.6547,  0.0000,  2.0000, -0.2294, -0.4201]])\n",
      "risk: tensor([2.])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1145])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([1828.4967])\n",
      "sol_group: tensor([1828.4967])\n",
      "obj_group: tensor([-0.0033])\n",
      "obj_ind: tensor([-0.0033])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847c1c",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2886a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ee06d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "def _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group):\n",
    "    \"\"\"Helper to compute regret and the decision variable d_hat.\"\"\"\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        regret = obj_val_at_d_star - obj_val_at_d_hat\n",
    "        # regret = np.log1p(np.exp(regret * 10)) / 10\n",
    "        return regret, d_hat_np\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        # Return a zero regret and a placeholder for d_hat\n",
    "        return 0.0, np.zeros_like(pred_r_np)\n",
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd Function for regret with a closed-form or finite-difference gradient.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method):\n",
    "        # --- Loss Calculation (Regret) ---\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(\n",
    "            lambda t: to_numpy_1d(t.detach()), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "\n",
    "        regret, d_hat_np = _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group)\n",
    "        regret_loss = torch.tensor(regret, dtype=pred_r.dtype, device=pred_r.device)\n",
    "        # regret_loss = F.softplus(torch.tensor(regret, dtype=pred_r.dtype,device=pred_r.device), beta=10)\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 8\n",
    "\n",
    "        try:\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                # (Closed-form gradient calculation remains the same)\n",
    "                if params['group']:\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    v_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    Jac_mat = solve_coupled_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    vT_J_np = v_np @ Jac_mat\n",
    "                    grad_regret = -torch.from_numpy(vT_J_np).to(pred_r.device,dtype=pred_r.dtype)\n",
    "\n",
    "                else:\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - params['alpha']) * d_hat ** (-params['alpha']) # Grad of alpha-fairness obj\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "                    print(\"closed-form grad\",grad_regret)\n",
    "            \n",
    "            elif params['grad_method'] == 'finite-diff':\n",
    "\n",
    "                pred_r_np = to_numpy_1d(pred_r)\n",
    "                grad_regret_np = np.zeros_like(pred_r_np)\n",
    "\n",
    "                eps = 1e-3                                    # relative 0.1 %\n",
    "                eps_vec = eps * np.maximum(1.0, np.abs(pred_r_np))\n",
    "\n",
    "                # Detach and convert tensors needed for perturbations once\n",
    "                true_r_np, gainF_np, cost_np, race_np = map(\n",
    "                    lambda t: to_numpy_1d(t.detach()), [true_r, gainF, cost, race]\n",
    "                )\n",
    "\n",
    "                for i in range(len(pred_r_np)):\n",
    "                    # Perturb pred_r for forward and backward steps\n",
    "                    pred_r_plus = pred_r_np.copy(); pred_r_plus[i]  += eps_vec[i]\n",
    "                    pred_r_minus = pred_r_np.copy(); pred_r_minus[i] -= eps_vec[i]\n",
    "\n",
    "                    regret_plus, _ = _calculate_regret_and_d_hat(pred_r_plus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "                    regret_minus, _ = _calculate_regret_and_d_hat(pred_r_minus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "\n",
    "                    grad_regret_np[i] = (regret_plus - regret_minus) / (2 * eps_vec[i])\n",
    "                    print(\"finite-diff grad\",grad_regret_np)\n",
    "\n",
    "                # The gradient of the loss is the negative of the gradient of the regret\n",
    "                grad_regret = torch.from_numpy(grad_regret_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Gradient calculation failed: {e}. Returning zero grad.\")\n",
    "\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Regret loss from the custom function\n",
    "        regret_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method)\n",
    "\n",
    "        # 2. Fairness penalty using standard PyTorch autograd\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "        \n",
    "        # 3. Total loss\n",
    "        total_loss = regret_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a09dba",
   "metadata": {},
   "source": [
    "# Training Gemini Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0ebf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "\n",
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor via direct regret minimization, logging detailed metrics\n",
    "    at each evaluation point.\n",
    "    \"\"\"\n",
    "    # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # --- Initialize Logs ---\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "    unique_groups = torch.unique(race_test).cpu().numpy()\n",
    "    per_group_mse_log = {g: [] for g in unique_groups}\n",
    "    per_group_obj_log = {g: [] for g in unique_groups}\n",
    "    per_group_true_benefit_log = {g: [] for g in unique_groups}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze().clamp(min=1e-4)\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # --- Periodic Evaluation on Test Set ---\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test).squeeze().clamp(min=1e-4)\n",
    "                # Overall MSE\n",
    "                mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "                mse_log.append(mse_val)\n",
    "\n",
    "                # Overall Regret\n",
    "                _, _, d_pred_np = _calculate_loss_and_decision(pred_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                _, _, d_true_np = _calculate_loss_and_decision(y_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                if d_pred_np is not None and d_true_np is not None:\n",
    "                    y_test_np = to_numpy_1d(y_test)\n",
    "                    race_test_np = to_numpy_1d(race_test)\n",
    "                    if group:\n",
    "                        true_obj = compute_coupled_group_obj(d_true_np, y_test_np, race_test_np, alpha)\n",
    "                        pred_obj = compute_coupled_group_obj(d_pred_np, y_test_np, race_test_np, alpha)\n",
    "                    else:\n",
    "                        true_obj = AlphaFairness(y_test_np * d_true_np, alpha)\n",
    "                        pred_obj = AlphaFairness(y_test_np * d_pred_np, alpha)\n",
    "                    norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "                else:\n",
    "                    norm_regret = np.nan\n",
    "                regret_log.append(norm_regret)\n",
    "\n",
    "                # Overall Fairness\n",
    "                fair_val = 0.0\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=0.5, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "                fairness_log.append(fair_val)\n",
    "\n",
    "                # Group-wise Metrics\n",
    "                for g in unique_groups:\n",
    "                    mask = (race_test == g)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    # Group MSE\n",
    "                    per_group_mse_log[g].append(((pred_test[mask] - y_test[mask]).pow(2)).mean().item())\n",
    "                    # Group True Benefit\n",
    "                    per_group_true_benefit_log[g].append(y_test[mask].mean().item())\n",
    "                    # Group Decision Objective\n",
    "                    if d_pred_np is not None:\n",
    "                        group_mask_np = (race_test_np == g)\n",
    "                        # We use the true benefits (y_test) to evaluate the utility of the decisions (d_pred_np)\n",
    "                        group_utility = y_test_np[group_mask_np] * d_pred_np[group_mask_np]\n",
    "                        # For simplicity, we report the mean utility as the objective\n",
    "                        per_group_obj_log[g].append(group_utility.mean())\n",
    "                    else:\n",
    "                        per_group_obj_log[g].append(np.nan)\n",
    "\n",
    "                print(f\"Epoch {epoch:03d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_log[-1]:.4f} | Regret {regret_log[-1]:.4f} | Fair-Val {fairness_log[-1]:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "    # Return a dictionary of all logs\n",
    "    return model, {\n",
    "        \"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log,\n",
    "        \"training_time\": total_time,\n",
    "        \"per_group_mse\": per_group_mse_log,\n",
    "        \"per_group_decision_objective\": per_group_obj_log,\n",
    "        \"per_group_true_benefit\": per_group_true_benefit_log\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8386a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23a66413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha = 2\n",
    "# hyperparams = {\n",
    "#     \"alpha\":alpha,\n",
    "#     \"Q\": 1000,\n",
    "#     \"lambda_fair\": 0,\n",
    "#     \"fairness_type\": \"atkinson\",   \n",
    "#     \"group\": True,            # Set to True for group fairness, False for individual\n",
    "#     \"grad_method\": \"finite-diff\",\n",
    "#     \"num_epochs\": 50,        \n",
    "#     \"lr\": 0.005,\n",
    "#     \"batch_size\": len(b_train),\n",
    "#     \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# }\n",
    "\n",
    "# final_model, logs = train_model_regret(\n",
    "#     X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "#     X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "#     model_class=FairRiskPredictor,\n",
    "#     input_dim=feats_train.shape[1],\n",
    "#     **hyperparams\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab4122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1.  MULTI-TRIAL REGRET TRAINING WITH FULL LOGGING\n",
    "# ---------------------------------------------------------------------\n",
    "def train_many_trials_regret(n_trials=3, base_seed=2025, **train_args):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` different seeds.\n",
    "    Returns a FLAT dict whose keys are:\n",
    "        regret, regret_std, mse, mse_std, fairness, fairness_std, …,\n",
    "        G0_mse, G0_mse_std, G0_decision_obj, G0_decision_obj_std, …\n",
    "    \"\"\"\n",
    "    # -------------------- run all trials -----------------------------\n",
    "    per_trial_metrics = defaultdict(list)      # collects trial-level scalars\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        _, logs = train_model_regret(**train_args)   # one full run\n",
    "\n",
    "        # ---- overall scalars ---------------------------------------\n",
    "        per_trial_metrics['regret'       ].append(logs['regret_log']  [-1])\n",
    "        per_trial_metrics['mse'          ].append(logs['mse_log']     [-1])\n",
    "        per_trial_metrics['fairness'     ].append(logs['fairness_log'][-1])\n",
    "        per_trial_metrics['training_time'].append(logs['training_time'])\n",
    "\n",
    "        # ---- per-group metrics (final epoch) -----------------------\n",
    "        for g_id, g_log in logs['per_group_mse'].items():\n",
    "            if g_log:                      # just in case\n",
    "                per_trial_metrics[f'G{int(g_id)}_mse'          ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_decision_objective'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_decision_obj' ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_true_benefit'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_true_benefit' ].append(g_log[-1])\n",
    "\n",
    "    # -------------------- aggregate over trials ----------------------\n",
    "    avg_results = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      AVERAGED RESULTS ACROSS ALL TRIALS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for key, values in per_trial_metrics.items():\n",
    "        μ, σ = np.mean(values), np.std(values)\n",
    "        avg_results[key]      = μ\n",
    "        avg_results[f'{key}_std'] = σ\n",
    "        print(f\"[{key.upper():>20s}]  μ = {μ:.4f} | σ = {σ:.4f}\")\n",
    "\n",
    "    return avg_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175a7d",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68991548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "closed-form grad tensor([-0.0909, -0.0594, -0.0469, -0.1998, -0.0136, -0.2501, -0.2955,  0.0764,\n",
      "        -0.1930,  0.2614])\n",
      "Epoch 001/20 | Train-Loss 0.4272 | Test-MSE 8.9158 | Regret 0.0315 | Fair-Val 7.0376\n",
      "closed-form grad tensor([-0.1740, -0.2640, -0.2701, -0.0127, -0.1739, -0.0505,  0.2599,  0.0738,\n",
      "        -0.0415, -0.0732])\n",
      "closed-form grad tensor([-0.0798,  0.2623, -0.1862, -0.0169, -0.0366, -0.0335,  0.0742, -0.2445,\n",
      "        -0.2171, -0.1715])\n",
      "closed-form grad tensor([-0.2168,  0.0709, -0.1694, -0.0406,  0.2510, -0.0318, -0.0710, -0.1462,\n",
      "        -0.2194, -0.0139])\n",
      "closed-form grad tensor([ 0.2504, -0.0389, -0.0107, -0.2194, -0.1513, -0.0677,  0.0683, -0.0364,\n",
      "        -0.2178, -0.1416])\n",
      "closed-form grad tensor([-0.1427, -0.0167, -0.0177,  0.2492, -0.0396, -0.1277, -0.2086,  0.0683,\n",
      "        -0.2078, -0.0657])\n",
      "closed-form grad tensor([ 0.2413, -0.0146, -0.0132, -0.1891,  0.0637, -0.1049, -0.0418, -0.0618,\n",
      "        -0.1219, -0.1921])\n",
      "closed-form grad tensor([-0.0168, -0.0567,  0.0621, -0.0144, -0.1857,  0.2392, -0.1431, -0.1094,\n",
      "        -0.0360, -0.1145])\n",
      "closed-form grad tensor([-0.0524,  0.0615,  0.2381, -0.0117, -0.0311, -0.1745, -0.0175, -0.1309,\n",
      "        -0.1248, -0.1124])\n",
      "closed-form grad tensor([-0.1681, -0.0409, -0.0177, -0.1024, -0.0118,  0.2383,  0.0624, -0.0839,\n",
      "        -0.1155, -0.0570])\n",
      "Epoch 010/20 | Train-Loss 0.3579 | Test-MSE 8.4772 | Regret 0.0319 | Fair-Val 6.6958\n",
      "closed-form grad tensor([-0.1024, -0.0298, -0.1141, -0.0791, -0.1682, -0.0550, -0.0197,  0.2351,\n",
      "        -0.0113,  0.0630])\n",
      "closed-form grad tensor([-0.0101, -0.1481, -0.0232, -0.0439, -0.0944, -0.0245, -0.0907, -0.0773,\n",
      "         0.0571,  0.2289])\n",
      "closed-form grad tensor([ 0.0559, -0.0222, -0.1619, -0.0708, -0.0096, -0.0882, -0.0272,  0.2253,\n",
      "        -0.0918, -0.0479])\n",
      "closed-form grad tensor([-0.0397,  0.2193, -0.0961, -0.0814,  0.0560, -0.0074, -0.0356, -0.1417,\n",
      "        -0.0168, -0.0604])\n",
      "closed-form grad tensor([-0.0830, -0.0356, -0.0920, -0.0529, -0.0360,  0.2189, -0.0149, -0.1559,\n",
      "        -0.0107,  0.0574])\n",
      "closed-form grad tensor([ 0.0575, -0.0515,  0.2000, -0.0112, -0.0714, -0.0360, -0.0836, -0.0147,\n",
      "        -0.0230, -0.1514])\n",
      "closed-form grad tensor([-0.0075, -0.0547,  0.1857, -0.0390,  0.0512, -0.0122, -0.0234, -0.0627,\n",
      "        -0.1243, -0.0599])\n",
      "closed-form grad tensor([ 0.1872, -0.0121,  0.0564, -0.0563, -0.1102, -0.1029, -0.0129, -0.0225,\n",
      "        -0.0342, -0.0333])\n",
      "closed-form grad tensor([ 0.1727, -0.1044, -0.0215, -0.0082, -0.0551, -0.0064, -0.0415, -0.0665,\n",
      "        -0.0466,  0.0424])\n",
      "closed-form grad tensor([-0.0491, -0.0338,  0.1629, -0.0598, -0.0083, -0.0508, -0.0099,  0.0487,\n",
      "        -0.0187, -0.1164])\n",
      "Epoch 020/20 | Train-Loss 0.3161 | Test-MSE 7.8324 | Regret 0.0439 | Fair-Val 6.3021\n",
      "Training finished in 0.03s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0439 | σ = 0.0000\n",
      "[                 MSE]  μ = 7.8324 | σ = 0.0000\n",
      "[            FAIRNESS]  μ = 6.3021 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 0.0343 | σ = 0.0000\n",
      "[              G0_MSE]  μ = 7.8324 | σ = 0.0000\n",
      "[     G0_DECISION_OBJ]  μ = 10.0890 | σ = 0.0000\n",
      "[     G0_TRUE_BENEFIT]  μ = 3.2647 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "finite-diff grad [-0.038445  0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.      ]\n",
      "finite-diff grad [-0.038445   -0.10937452  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.023067\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.023067\n",
      "  0.07534027  0.          0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.023067\n",
      "  0.07534027  0.07474422  0.          0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.023067\n",
      "  0.07534027  0.07474422  0.08046626  0.        ]\n",
      "finite-diff grad [-0.038445   -0.10937452 -0.1066327  -0.03194809 -0.20903347  0.023067\n",
      "  0.07534027  0.07474422  0.08046626  0.25057793]\n",
      "Epoch 001/20 | Train-Loss 0.4272 | Test-MSE 8.9868 | Regret 0.0321 | Fair-Val 7.0819\n",
      "finite-diff grad [-0.03051758  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615 -0.0835657\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615 -0.0835657\n",
      "  0.24712084  0.          0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615 -0.0835657\n",
      "  0.24712084  0.0706315   0.          0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615 -0.0835657\n",
      "  0.24712084  0.0706315  -0.08749961  0.        ]\n",
      "finite-diff grad [-0.03051758  0.07367134  0.02080202 -0.16415118  0.07897615 -0.0835657\n",
      "  0.24712084  0.0706315  -0.08749961 -0.03111362]\n",
      "finite-diff grad [-0.03546476  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411 -0.0537188\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411 -0.0537188\n",
      "  0.07104874  0.          0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411 -0.0537188\n",
      "  0.07104874  0.07361174  0.          0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411 -0.0537188\n",
      "  0.07104874  0.07361174  0.02288818  0.        ]\n",
      "finite-diff grad [-0.03546476  0.24873017 -0.03874302 -0.16152857 -0.07325411 -0.0537188\n",
      "  0.07104874  0.07361174  0.02288818  0.07957219]\n",
      "finite-diff grad [0.02121925 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.02121925 0.06890297 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777 -0.04696918\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777 -0.04696918\n",
      " -0.0333786   0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777 -0.04696918\n",
      " -0.0333786   0.07659197  0.          0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777 -0.04696918\n",
      " -0.0333786   0.07659197  0.07015467  0.        ]\n",
      "finite-diff grad [ 0.02121925  0.06890297 -0.03826618 -0.07474422  0.23853777 -0.04696918\n",
      " -0.0333786   0.07659197  0.07015467 -0.12296437]\n",
      "finite-diff grad [0.23961066 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881 -0.03224611\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881 -0.03224611\n",
      "  0.06604195  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881 -0.03224611\n",
      "  0.06604195 -0.06049871  0.          0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881 -0.03224611\n",
      "  0.06604195 -0.06049871  0.02098083  0.        ]\n",
      "finite-diff grad [ 0.23961066 -0.05082662 -0.10156631  0.06979704  0.07706881 -0.03224611\n",
      "  0.06604195 -0.06049871  0.02098083 -0.03552437]\n",
      "finite-diff grad [-0.04291534  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.07659197\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.07659197\n",
      "  0.02062321  0.          0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.07659197\n",
      "  0.02062321  0.06765127  0.          0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.07659197\n",
      "  0.02062321  0.06765127  0.07039309  0.        ]\n",
      "finite-diff grad [-0.04291534 -0.11116266 -0.02327316  0.23579596 -0.06157159  0.07659197\n",
      "  0.02062321  0.06765127  0.07039309 -0.03141165]\n",
      "finite-diff grad [0.22429226 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.07522106\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.07522106\n",
      " -0.05823373  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.07522106\n",
      " -0.05823373 -0.03182888  0.          0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.07522106\n",
      " -0.05823373 -0.03182888 -0.03683567  0.        ]\n",
      "finite-diff grad [ 0.22429226 -0.02013616 -0.07497677  0.020504    0.0629425   0.07522106\n",
      " -0.05823373 -0.03182888 -0.03683567  0.06812811]\n",
      "finite-diff grad [-0.06918521  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.2247691\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.2247691\n",
      "  0.0666976   0.          0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.2247691\n",
      "  0.0666976   0.07486343  0.          0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.2247691\n",
      "  0.0666976   0.07486343 -0.04575857  0.        ]\n",
      "finite-diff grad [-0.06918521 -0.03010034  0.06234645 -0.01796931  0.02002716  0.2247691\n",
      "  0.0666976   0.07486343 -0.04575857 -0.0385046 ]\n",
      "finite-diff grad [-0.02861023  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.02041459\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.02041459\n",
      " -0.06343493  0.          0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.02041459\n",
      " -0.06343493 -0.04857778  0.          0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.02041459\n",
      " -0.06343493 -0.04857778  0.06985664  0.        ]\n",
      "finite-diff grad [-0.02861023  0.06201863  0.21529196 -0.01291209 -0.03795087  0.02041459\n",
      " -0.06343493 -0.04857778  0.06985664  0.0757277 ]\n",
      "finite-diff grad [0.02205372 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.22527574\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.22527574\n",
      "  0.06809831  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.22527574\n",
      "  0.06809831  0.0797212   0.          0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.22527574\n",
      "  0.06809831  0.0797212   0.07039309  0.        ]\n",
      "finite-diff grad [ 0.02205372 -0.0474751  -0.06388636 -0.04288554 -0.01252614  0.22527574\n",
      "  0.06809831  0.0797212   0.07039309 -0.03060698]\n",
      "Epoch 010/20 | Train-Loss 0.1703 | Test-MSE 9.2103 | Regret 0.0336 | Fair-Val 7.1659\n",
      "finite-diff grad [0.06866455 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826 -0.02926588\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826 -0.02926588\n",
      " -0.04870968  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826 -0.02926588\n",
      " -0.04870968  0.20569561  0.          0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826 -0.02926588\n",
      " -0.04870968  0.20569561 -0.01088518  0.        ]\n",
      "finite-diff grad [ 0.06866455 -0.0316885  -0.04917383  0.07772446  0.02136826 -0.02926588\n",
      " -0.04870968  0.20569561 -0.01088518  0.0666678 ]\n",
      "finite-diff grad [-0.00842438  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336 -0.0265261\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336 -0.0265261\n",
      " -0.04509091  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336 -0.0265261\n",
      " -0.04509091  0.07876753  0.          0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336 -0.0265261\n",
      " -0.04509091  0.07876753  0.06490946  0.        ]\n",
      "finite-diff grad [-0.00842438  0.02253055 -0.06052049 -0.02491474  0.06884336 -0.0265261\n",
      " -0.04509091  0.07876753  0.06490946  0.20053981]\n",
      "finite-diff grad [0.06186962 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705 -0.04899501\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705 -0.04899501\n",
      " -0.02638106  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705 -0.04899501\n",
      " -0.02638106  0.20864604  0.          0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705 -0.04899501\n",
      " -0.02638106  0.20864604  0.06765127  0.        ]\n",
      "finite-diff grad [ 0.06186962 -0.0551017   0.02130866  0.07501245 -0.00806705 -0.04899501\n",
      " -0.02638106  0.20864604  0.06765127 -0.02685189]\n",
      "finite-diff grad [-0.02193451  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302 -0.00569394\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302 -0.00569394\n",
      " -0.02850846  0.          0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302 -0.00569394\n",
      " -0.02850846  0.0218153   0.          0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302 -0.00569394\n",
      " -0.02850846  0.0218153  -0.03427127  0.        ]\n",
      "finite-diff grad [-0.02193451  0.1695752  -0.04684925  0.06684661  0.06437302 -0.00569394\n",
      " -0.02850846  0.0218153  -0.03427127  0.07811189]\n",
      "finite-diff grad [-0.05748868  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.17192958\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.17192958\n",
      " -0.02688449  0.          0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.17192958\n",
      " -0.02688449  0.02107024  0.          0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.17192958\n",
      " -0.02688449  0.02107024 -0.00738873  0.        ]\n",
      "finite-diff grad [-0.05748868 -0.02770698  0.06467104  0.07981061 -0.01978874  0.17192958\n",
      " -0.02688449  0.02107024 -0.00738873  0.06732345]\n",
      "finite-diff grad [0.06702542 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.06702542 0.07414818 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.06702542 0.07414818 0.1398325  0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823   0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928 -0.0205636\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928 -0.0205636\n",
      "  0.06735325  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928 -0.0205636\n",
      "  0.06735325 -0.02601764  0.          0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928 -0.0205636\n",
      "  0.06735325 -0.02601764 -0.01848079  0.        ]\n",
      "finite-diff grad [ 0.06702542  0.07414818  0.1398325  -0.0066823  -0.04985928 -0.0205636\n",
      "  0.06735325 -0.02601764 -0.01848079  0.02104044]\n",
      "finite-diff grad [-0.00406856  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562 -0.02017643\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562 -0.02017643\n",
      " -0.0151558   0.          0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562 -0.02017643\n",
      " -0.0151558   0.06663799  0.          0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562 -0.02017643\n",
      " -0.0151558   0.06663799  0.02157688  0.        ]\n",
      "finite-diff grad [-0.00406856  0.07742643  0.10550021 -0.01949072  0.06699562 -0.02017643\n",
      " -0.0151558   0.06663799  0.02157688 -0.05862116]\n",
      "finite-diff grad [0.07915496 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.06699562\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.06699562\n",
      " -0.01901764  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.06699562\n",
      " -0.01901764 -0.01276935  0.          0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.06699562\n",
      " -0.01901764 -0.01276935  0.07978081  0.        ]\n",
      "finite-diff grad [ 0.07915496 -0.00607491  0.07411838 -0.04938244  0.02408027  0.06699562\n",
      " -0.01901764 -0.01276935  0.07978081 -0.01698732]\n",
      "finite-diff grad [0.11718272 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.11718272 0.0231862  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555 -0.00276384\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555 -0.00276384\n",
      " -0.02300739  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555 -0.00276384\n",
      " -0.02300739  0.05733966  0.          0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555 -0.00276384\n",
      " -0.02300739  0.05733966  0.07641315  0.        ]\n",
      "finite-diff grad [ 0.11718272  0.0231862  -0.01076128 -0.02007793 -0.06097555 -0.00276384\n",
      " -0.02300739  0.05733966  0.07641315  0.06660819]\n",
      "finite-diff grad [-0.02372265  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.06613135\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.06613135\n",
      " -0.01655774  0.          0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.06613135\n",
      " -0.01655774  0.07134676  0.          0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.06613135\n",
      " -0.01655774  0.07134676 -0.00998701  0.        ]\n",
      "finite-diff grad [-0.02372265  0.07992982  0.07557869 -0.05790591 -0.00423544  0.06613135\n",
      " -0.01655774  0.07134676 -0.00998701  0.02470612]\n",
      "Epoch 020/20 | Train-Loss 0.0658 | Test-MSE 9.4147 | Regret 0.0387 | Fair-Val 7.2564\n",
      "Training finished in 0.07s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0387 | σ = 0.0000\n",
      "[                 MSE]  μ = 9.4147 | σ = 0.0000\n",
      "[            FAIRNESS]  μ = 7.2564 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 0.0675 | σ = 0.0000\n",
      "[              G0_MSE]  μ = 9.4147 | σ = 0.0000\n",
      "[     G0_DECISION_OBJ]  μ = 10.2982 | σ = 0.0000\n",
      "[     G0_TRUE_BENEFIT]  μ = 3.2647 | σ = 0.0000\n",
      "\n",
      "==========================================================================================\n",
      "                           GRID SEARCH COMPLETE\n",
      "==========================================================================================\n",
      "   Group  Grad Method  Alpha  Lambda Fairness  G0_decision_obj  G0_decision_obj_std    G0_mse  G0_mse_std  G0_true_benefit  G0_true_benefit_std  fairness  fairness_std       mse  mse_std    regret  regret_std  training_time  training_time_std\n",
      "0  False  closed-form      2       0      mad        10.088964                  0.0  7.832386         0.0         3.264706                  0.0  6.302073           0.0  7.832386      0.0  0.043860         0.0       0.034334                0.0\n",
      "1  False  finite-diff      2       0      mad        10.298169                  0.0  9.414669         0.0         3.264706                  0.0  7.256433           0.0  9.414669      0.0  0.038711         0.0       0.067535                0.0\n",
      "\n",
      "--- LaTeX Table Output ---\n",
      "\\begin{table}\n",
      "\\caption{Averaged Experimental Results Across Different Parameters.}\n",
      "\\label{tab:avg_exp_results_expanded}\n",
      "\\begin{tabular}{rlrrlrrrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "Group & Grad Method & Alpha & Lambda & Fairness & G0_decision_obj & G0_decision_obj_std & G0_mse & G0_mse_std & G0_true_benefit & G0_true_benefit_std & fairness & fairness_std & mse & mse_std & regret & regret_std & training_time & training_time_std \\\\\n",
      "\\midrule\n",
      "False & closed-form & 2 & 0 & mad & 10.0890 & 0.0000 & 7.8324 & 0.0000 & 3.2647 & 0.0000 & 6.3021 & 0.0000 & 7.8324 & 0.0000 & 0.0439 & 0.0000 & 0.0343 & 0.0000 \\\\\n",
      "False & finite-diff & 2 & 0 & mad & 10.2982 & 0.0000 & 9.4147 & 0.0000 & 3.2647 & 0.0000 & 7.2564 & 0.0000 & 9.4147 & 0.0000 & 0.0387 & 0.0000 & 0.0675 & 0.0000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [2]\n",
    "fairness_lambdas = [0]\n",
    "group_settings = [False]\n",
    "grad_methods = ['closed-form','finite-diff'] # New parameter\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  GRID-SEARCH HARNESS  (only the inner loop changed)\n",
    "# ---------------------------------------------------------------------\n",
    "results_list = []\n",
    "\n",
    "for group in group_settings:\n",
    "    fairness_types = ['mad', 'atkinson']           # keep acc_parity only if λ>0\n",
    "    for grad_method in grad_methods:\n",
    "        for lam in fairness_lambdas:\n",
    "            for fairness in fairness_types:\n",
    "                if lam == 0 and fairness != fairness_types[0]:\n",
    "                    continue                       # skip unattainable combos\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method,\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = dict(\n",
    "                        X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                        cost_train=cost_train, gainF_train=gainF_train,\n",
    "                        X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                        cost_test=cost_test, gainF_test=gainF_test,\n",
    "                        model_class=FairRiskPredictor,\n",
    "                        input_dim=feats_train.shape[1],\n",
    "                        alpha=alpha, Q=10,\n",
    "                        lambda_fair=lam, fairness_type=fairness,\n",
    "                        group=group, grad_method=grad_method,\n",
    "                        num_epochs=20, lr=0.001 ,\n",
    "                    )\n",
    "\n",
    "                    avg_results = train_many_trials_regret(\n",
    "                        n_trials=1, **train_args)\n",
    "\n",
    "                    # ---------------- build DataFrame row ------------\n",
    "                    row = run_params.copy()\n",
    "                    row.update(avg_results)          # every metric goes in\n",
    "                    results_list.append(row)\n",
    "\n",
    "# ---------------- DataFrame & LaTeX dump -----------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Put the hyper-parameters first; everything else follows automatically\n",
    "hp_cols = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness']\n",
    "other_cols = sorted([c for c in results_df.columns if c not in hp_cols])\n",
    "results_df = results_df[hp_cols + other_cols]\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Averaged Experimental Results Across Different Parameters.\",\n",
    "    label=\"tab:avg_exp_results_expanded\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None,\n",
    "                       'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n--- LaTeX Table Output ---\")\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"20250709results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "da20a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Grad Method</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Fairness</th>\n",
       "      <th>G0_decision_obj</th>\n",
       "      <th>G0_decision_obj_std</th>\n",
       "      <th>G0_mse</th>\n",
       "      <th>G0_mse_std</th>\n",
       "      <th>G0_true_benefit</th>\n",
       "      <th>G0_true_benefit_std</th>\n",
       "      <th>fairness</th>\n",
       "      <th>fairness_std</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_std</th>\n",
       "      <th>regret</th>\n",
       "      <th>regret_std</th>\n",
       "      <th>training_time</th>\n",
       "      <th>training_time_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>mad</td>\n",
       "      <td>10.088964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.832386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.264706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.832386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>mad</td>\n",
       "      <td>10.298169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.414669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.264706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.256433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.414669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group  Grad Method  Alpha  Lambda Fairness  G0_decision_obj  \\\n",
       "0  False  closed-form      2       0      mad        10.088964   \n",
       "1  False  finite-diff      2       0      mad        10.298169   \n",
       "\n",
       "   G0_decision_obj_std    G0_mse  G0_mse_std  G0_true_benefit  \\\n",
       "0                  0.0  7.832386         0.0         3.264706   \n",
       "1                  0.0  9.414669         0.0         3.264706   \n",
       "\n",
       "   G0_true_benefit_std  fairness  fairness_std       mse  mse_std    regret  \\\n",
       "0                  0.0  6.302073           0.0  7.832386      0.0  0.043860   \n",
       "1                  0.0  7.256433           0.0  9.414669      0.0  0.038711   \n",
       "\n",
       "   regret_std  training_time  training_time_std  \n",
       "0         0.0       0.034334                0.0  \n",
       "1         0.0       0.067535                0.0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "85beff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite-diff grad [0.00799855 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00799855 0.00842412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00799855 0.00842412 0.00463619 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757   0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072 -0.00827798\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072 -0.00827798\n",
      " -0.00688424  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072 -0.00827798\n",
      " -0.00688424 -0.00483088  0.          0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072 -0.00827798\n",
      " -0.00688424 -0.00483088  0.00275205  0.        ]\n",
      "finite-diff grad [ 0.00799855  0.00842412  0.00463619 -0.0042757  -0.00888072 -0.00827798\n",
      " -0.00688424 -0.00483088  0.00275205  0.004841  ]\n",
      "Epoch 001/50 | Train-Loss 0.0289 | Test-MSE 8.9734 | Regret 0.0002 | Fair-Val 0.0000\n",
      "finite-diff grad [-0.00909708  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747 -0.00577055\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747 -0.00577055\n",
      "  0.00501849  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747 -0.00577055\n",
      "  0.00501849  0.00837604  0.          0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747 -0.00577055\n",
      "  0.00501849  0.00837604  0.00287597  0.        ]\n",
      "finite-diff grad [-0.00909708 -0.00545394  0.00478947 -0.00422981  0.00870747 -0.00577055\n",
      "  0.00501849  0.00837604  0.00287597 -0.00755021]\n",
      "finite-diff grad [0.00477038 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053 -0.00511733\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053 -0.00511733\n",
      " -0.00417678  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053 -0.00511733\n",
      " -0.00417678 -0.00572857  0.          0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053 -0.00511733\n",
      " -0.00417678 -0.00572857  0.00496824  0.        ]\n",
      "finite-diff grad [ 0.00477038 -0.00525905 -0.00633504  0.00873442  0.00283053 -0.00511733\n",
      " -0.00417678 -0.00572857  0.00496824  0.00839013]\n",
      "finite-diff grad [-0.00558157  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.00879023\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.00879023\n",
      " -0.00364003  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.00879023\n",
      " -0.00364003 -0.00507482  0.          0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.00879023\n",
      " -0.00364003 -0.00507482  0.00486335  0.        ]\n",
      "finite-diff grad [-0.00558157  0.00280764  0.00850308  0.00480499 -0.00476422  0.00879023\n",
      " -0.00364003 -0.00507482  0.00486335 -0.00461727]\n",
      "finite-diff grad [0.00846281 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303 -0.0042184\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303 -0.0042184\n",
      " -0.00432611  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303 -0.0042184\n",
      " -0.00432611  0.00481284  0.          0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303 -0.0042184\n",
      " -0.00432611  0.00481284  0.00880506  0.        ]\n",
      "finite-diff grad [ 0.00846281 -0.00606163 -0.00571468  0.00283475  0.00491303 -0.0042184\n",
      " -0.00432611  0.00481284  0.00880506 -0.00450073]\n",
      "finite-diff grad [-0.00397393  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.00507993\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.00507993\n",
      "  0.00287261  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.00507993\n",
      "  0.00287261 -0.00364007  0.          0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.00507993\n",
      "  0.00287261 -0.00364007 -0.00625884  0.        ]\n",
      "finite-diff grad [-0.00397393 -0.00345894  0.00882831  0.00501448  0.00913793  0.00507993\n",
      "  0.00287261 -0.00364007 -0.00625884 -0.00358536]\n",
      "finite-diff grad [0.00298116 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00298116 0.00947186 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.00529696\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.00529696\n",
      " -0.00682997  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.00529696\n",
      " -0.00682997 -0.00401122  0.          0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.00529696\n",
      " -0.00682997 -0.00401122 -0.00246054  0.        ]\n",
      "finite-diff grad [ 0.00298116  0.00947186 -0.00376998  0.00921807 -0.00331039  0.00529696\n",
      " -0.00682997 -0.00401122 -0.00246054  0.00520352]\n",
      "finite-diff grad [-0.00208057  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744 -0.00712345\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744 -0.00712345\n",
      " -0.00359912  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744 -0.00712345\n",
      " -0.00359912 -0.00332568  0.          0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744 -0.00712345\n",
      " -0.00359912 -0.00332568 -0.00283523  0.        ]\n",
      "finite-diff grad [-0.00208057  0.00501018  0.0089515   0.00294728  0.00919744 -0.00712345\n",
      " -0.00359912 -0.00332568 -0.00283523  0.00505007]\n",
      "finite-diff grad [-0.00356014  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.00531355\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.00531355\n",
      "  0.00517592  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.00531355\n",
      "  0.00517592 -0.00264115  0.          0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.00531355\n",
      "  0.00517592 -0.00264115 -0.00290262  0.        ]\n",
      "finite-diff grad [-0.00356014  0.00300692 -0.00167948  0.00913288 -0.00710367  0.00531355\n",
      "  0.00517592 -0.00264115 -0.00290262  0.00921691]\n",
      "finite-diff grad [0.00545934 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.00957558\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.00957558\n",
      " -0.00280922  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.00957558\n",
      " -0.00280922 -0.00373994  0.          0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.00957558\n",
      " -0.00280922 -0.00373994  0.00317409  0.        ]\n",
      "finite-diff grad [ 0.00545934 -0.00185024  0.00966515 -0.00261893 -0.00757203  0.00957558\n",
      " -0.00280922 -0.00373994  0.00317409  0.00543996]\n",
      "Epoch 010/50 | Train-Loss 0.0139 | Test-MSE 9.3018 | Regret 0.0003 | Fair-Val 0.0000\n",
      "finite-diff grad [0.00939387 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448 -0.00206299\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448 -0.00206299\n",
      "  0.00546923  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448 -0.00206299\n",
      "  0.00546923 -0.00252868  0.          0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448 -0.00206299\n",
      "  0.00546923 -0.00252868 -0.00402104  0.        ]\n",
      "finite-diff grad [ 0.00939387 -0.00794356  0.00936933 -0.00135819  0.00309448 -0.00206299\n",
      "  0.00546923 -0.00252868 -0.00402104  0.00533455]\n",
      "finite-diff grad [0.00950433 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621 -0.00215078\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621 -0.00215078\n",
      " -0.00188453  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621 -0.00215078\n",
      " -0.00188453  0.00529347  0.          0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621 -0.00215078\n",
      " -0.00188453  0.00529347 -0.00259523  0.        ]\n",
      "finite-diff grad [ 0.00950433 -0.00149719 -0.00912618  0.00309833  0.00515621 -0.00215078\n",
      " -0.00188453  0.00529347 -0.00259523  0.00925197]\n",
      "finite-diff grad [0.00546475 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00546475 0.00951274 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.00909611\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.00909611\n",
      " -0.00090315  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.00909611\n",
      " -0.00090315 -0.00193793  0.          0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.00909611\n",
      " -0.00090315 -0.00193793  0.00320857  0.        ]\n",
      "finite-diff grad [ 0.00546475  0.00951274 -0.00185245  0.00536409 -0.00370192  0.00909611\n",
      " -0.00090315 -0.00193793  0.00320857 -0.00819233]\n",
      "finite-diff grad [-0.00176131  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.00519197\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.00519197\n",
      "  0.00954435  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.00519197\n",
      "  0.00954435 -0.00134671  0.          0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.00519197\n",
      "  0.00954435 -0.00134671  0.00321299  0.        ]\n",
      "finite-diff grad [-0.00176131  0.00930924 -0.00984207 -0.00090609  0.0055068   0.00519197\n",
      "  0.00954435 -0.00134671  0.00321299 -0.00343318]\n",
      "finite-diff grad [-0.00191921  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.00328681\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.00328681\n",
      "  0.00553467  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.00328681\n",
      "  0.00553467 -0.00121788  0.          0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.00328681\n",
      "  0.00553467 -0.00121788 -0.01014936  0.        ]\n",
      "finite-diff grad [-0.00191921  0.00963082  0.0088247  -0.00063317 -0.00306781  0.00328681\n",
      "  0.00553467 -0.00121788 -0.01014936  0.00546911]\n",
      "finite-diff grad [0.00946282 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00946282 0.00518347 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799 -0.00876725\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799 -0.00876725\n",
      " -0.00063603  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799 -0.00876725\n",
      " -0.00063603 -0.00310584  0.          0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799 -0.00876725\n",
      " -0.00063603 -0.00310584  0.00534263  0.        ]\n",
      "finite-diff grad [ 0.00946282  0.00518347 -0.00141214 -0.00107749  0.00901799 -0.00876725\n",
      " -0.00063603 -0.00310584  0.00534263  0.00318642]\n",
      "finite-diff grad [-0.00756829  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.00794255\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.00794255\n",
      " -0.0030304   0.          0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.00794255\n",
      " -0.0030304  -0.00151908  0.          0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.00794255\n",
      " -0.0030304  -0.00151908 -0.00104898  0.        ]\n",
      "finite-diff grad [-0.00756829  0.00527768 -0.00026475  0.00460162  0.00901686  0.00794255\n",
      " -0.0030304  -0.00151908 -0.00104898  0.00301046]\n",
      "finite-diff grad [0.00547405 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.00521442\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.00521442\n",
      "  0.00906474  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.00521442\n",
      "  0.00906474 -0.00231467  0.          0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.00521442\n",
      "  0.00906474 -0.00231467  0.00891602  0.        ]\n",
      "finite-diff grad [ 0.00547405 -0.00030009  0.00319448 -0.00095467 -0.01069866  0.00521442\n",
      "  0.00906474 -0.00231467  0.00891602 -0.00150818]\n",
      "finite-diff grad [-0.01018766  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036 -0.00032323\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036 -0.00032323\n",
      "  0.00817938  0.          0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036 -0.00032323\n",
      "  0.00817938  0.00551005  0.          0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036 -0.00032323\n",
      "  0.00817938  0.00551005  0.0053945   0.        ]\n",
      "finite-diff grad [-0.01018766  0.00346091 -0.00230888 -0.00087712  0.00973036 -0.00032323\n",
      "  0.00817938  0.00551005  0.0053945  -0.00113845]\n",
      "finite-diff grad [0.00528283 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00528283 0.00315526 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00528283 0.00315526 0.00484362 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195 -0.00113784  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195 -0.00113784  0.00839194\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195 -0.00113784  0.00839194\n",
      " -0.01155028  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195 -0.00113784  0.00839194\n",
      " -0.01155028  0.00869519  0.          0.        ]\n",
      "finite-diff grad [ 0.00528283  0.00315526  0.00484362 -0.00263195 -0.00113784  0.00839194\n",
      " -0.01155028  0.00869519 -0.00065384  0.        ]\n",
      "finite-diff grad [ 5.2828295e-03  3.1552622e-03  4.8436173e-03 -2.6319521e-03\n",
      " -1.1378411e-03  8.3919391e-03 -1.1550279e-02  8.6951908e-03\n",
      " -6.5383536e-04 -8.2348284e-05]\n",
      "Epoch 020/50 | Train-Loss 0.0071 | Test-MSE 9.5246 | Regret 0.0003 | Fair-Val 0.0000\n",
      "finite-diff grad [0.00868562 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00868562 0.0031828  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.00497123\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.00497123\n",
      " -0.00094985  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.00497123\n",
      " -0.00094985  0.00491125  0.          0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.00497123\n",
      " -0.00094985  0.00491125  0.00914379  0.        ]\n",
      "finite-diff grad [ 0.00868562  0.0031828  -0.00032091 -0.00904803 -0.00216398  0.00497123\n",
      " -0.00094985  0.00491125  0.00914379 -0.00068861]\n",
      "finite-diff grad [-0.00064474  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867 -0.00067761\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867 -0.00067761\n",
      " -0.01113198  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867 -0.00067761\n",
      " -0.01113198  0.00538049  0.          0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867 -0.00067761\n",
      " -0.01113198  0.00538049 -0.00017172  0.        ]\n",
      "finite-diff grad [-0.00064474  0.00605666  0.008987    0.00480866 -0.00149867 -0.00067761\n",
      " -0.01113198  0.00538049 -0.00017172  0.00320502]\n",
      "finite-diff grad [-0.00058893  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.00905474\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.00905474\n",
      "  0.00557883  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.00905474\n",
      "  0.00557883 -0.00068843  0.          0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.00905474\n",
      "  0.00557883 -0.00068843 -0.01048837  0.        ]\n",
      "finite-diff grad [-0.00058893 -0.00192848  0.0053746   0.00433263  0.00293197  0.00905474\n",
      "  0.00557883 -0.00068843 -0.01048837 -0.00011257]\n",
      "finite-diff grad [-0.00120311  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00120311 -0.00039639  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03 -1.0010910e-02  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03 -1.0010910e-02  4.7740331e-03  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03 -1.0010910e-02  4.7740331e-03  4.8621730e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03 -1.0010910e-02  4.7740331e-03  4.8621730e-03\n",
      " -6.4023997e-04  0.0000000e+00]\n",
      "finite-diff grad [-1.2031109e-03 -3.9639251e-04 -2.5859867e-06  3.0972760e-03\n",
      "  5.6317565e-03 -1.0010910e-02  4.7740331e-03  4.8621730e-03\n",
      " -6.4023997e-04  7.9753371e-03]\n",
      "finite-diff grad [-0.00986505  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.00456472  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.00456472 -0.00029941  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.00456472 -0.00029941  0.00858717\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.00456472 -0.00029941  0.00858717\n",
      " -0.00099487  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00986505 -0.00056594  0.00411082  0.00456472 -0.00029941  0.00858717\n",
      " -0.00099487  0.00300793  0.          0.        ]\n",
      "finite-diff grad [-9.8650530e-03 -5.6593819e-04  4.1108192e-03  4.5647235e-03\n",
      " -2.9940676e-04  8.5871741e-03 -9.9487335e-04  3.0079258e-03\n",
      " -1.9899862e-05  0.0000000e+00]\n",
      "finite-diff grad [-9.8650530e-03 -5.6593819e-04  4.1108192e-03  4.5647235e-03\n",
      " -2.9940676e-04  8.5871741e-03 -9.9487335e-04  3.0079258e-03\n",
      " -1.9899862e-05  4.5781042e-03]\n",
      "finite-diff grad [-0.01162939  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336    0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336   -0.00039594  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336   -0.00039594 -0.00048388  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336   -0.00039594 -0.00048388  0.00794933\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336   -0.00039594 -0.00048388  0.00794933\n",
      "  0.00471876  0.          0.          0.        ]\n",
      "finite-diff grad [-0.01162939  0.00281077  0.008336   -0.00039594 -0.00048388  0.00794933\n",
      "  0.00471876 -0.00156581  0.          0.        ]\n",
      "finite-diff grad [-1.16293905e-02  2.81076622e-03  8.33600294e-03 -3.95940122e-04\n",
      " -4.83876152e-04  7.94933271e-03  4.71875770e-03 -1.56581483e-03\n",
      "  4.51980195e-05  0.00000000e+00]\n",
      "finite-diff grad [-1.16293905e-02  2.81076622e-03  8.33600294e-03 -3.95940122e-04\n",
      " -4.83876152e-04  7.94933271e-03  4.71875770e-03 -1.56581483e-03\n",
      "  4.51980195e-05  3.69665376e-03]\n",
      "finite-diff grad [-0.01001955  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509  -0.00123194  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509  -0.00123194  0.0075786   0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509  -0.00123194  0.0075786   0.0028982\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509  -0.00123194  0.0075786   0.0028982\n",
      " -0.00062681  0.          0.          0.        ]\n",
      "finite-diff grad [-0.01001955  0.00807777 -0.0004509  -0.00123194  0.0075786   0.0028982\n",
      " -0.00062681  0.00367016  0.          0.        ]\n",
      "finite-diff grad [-1.0019547e-02  8.0777695e-03 -4.5089744e-04 -1.2319374e-03\n",
      "  7.5786016e-03  2.8981955e-03 -6.2681315e-04  3.6701583e-03\n",
      "  8.1916267e-05  0.0000000e+00]\n",
      "finite-diff grad [-1.0019547e-02  8.0777695e-03 -4.5089744e-04 -1.2319374e-03\n",
      "  7.5786016e-03  2.8981955e-03 -6.2681315e-04  3.6701583e-03\n",
      "  8.1916267e-05  3.5240853e-03]\n",
      "finite-diff grad [0.0032459 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.       ]\n",
      "finite-diff grad [0.0032459  0.00368784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.0032459  0.00368784 0.00549386 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.0032459  0.00368784 0.00549386 0.00320094 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.0032459   0.00368784  0.00549386  0.00320094 -0.00026053  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.0032459   0.00368784  0.00549386  0.00320094 -0.00026053 -0.00028445\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.0032459   0.00368784  0.00549386  0.00320094 -0.00026053 -0.00028445\n",
      "  0.00853692  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.0032459   0.00368784  0.00549386  0.00320094 -0.00026053 -0.00028445\n",
      "  0.00853692 -0.00895808  0.          0.        ]\n",
      "finite-diff grad [ 0.0032459   0.00368784  0.00549386  0.00320094 -0.00026053 -0.00028445\n",
      "  0.00853692 -0.00895808 -0.00136282  0.        ]\n",
      "finite-diff grad [ 3.2458974e-03  3.6878381e-03  5.4938588e-03  3.2009422e-03\n",
      " -2.6053374e-04 -2.8444754e-04  8.5369218e-03 -8.9580752e-03\n",
      " -1.3628217e-03  1.6948232e-05]\n",
      "finite-diff grad [0.00303851 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00303851 0.0001518  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00303851 0.0001518  0.00393385 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00303851 0.0001518  0.00393385 0.00289794 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229 -0.00035093\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229 -0.00035093\n",
      " -0.00034624  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229 -0.00035093\n",
      " -0.00034624  0.00565057  0.          0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229 -0.00035093\n",
      " -0.00034624  0.00565057 -0.00095199  0.        ]\n",
      "finite-diff grad [ 0.00303851  0.0001518   0.00393385  0.00289794 -0.01027229 -0.00035093\n",
      " -0.00034624  0.00565057 -0.00095199  0.00793391]\n",
      "finite-diff grad [-0.00080925  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00080925  0.00782473  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00080925  0.00782473 -0.0004231   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  6.1838604e-03  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  6.1838604e-03  2.0889295e-03  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  6.1838604e-03  2.0889295e-03  2.7160731e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  6.1838604e-03  2.0889295e-03  2.7160731e-03\n",
      " -1.8952004e-04  0.0000000e+00]\n",
      "finite-diff grad [-8.0924650e-04  7.8247339e-03 -4.2309580e-04  7.2837224e-06\n",
      "  3.3735665e-03  6.1838604e-03  2.0889295e-03  2.7160731e-03\n",
      " -1.8952004e-04 -6.6642649e-03]\n",
      "Epoch 030/50 | Train-Loss 0.0030 | Test-MSE 9.5686 | Regret 0.0003 | Fair-Val 0.0000\n",
      "finite-diff grad [-0.00013495  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.00787323  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.00787323  0.00284696\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.00787323  0.00284696\n",
      "  0.00381697  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.00787323  0.00284696\n",
      "  0.00381697 -0.0007554   0.          0.        ]\n",
      "finite-diff grad [-0.00013495 -0.00024891  0.00321707  0.00434115  0.00787323  0.00284696\n",
      "  0.00381697 -0.0007554  -0.01039164  0.        ]\n",
      "finite-diff grad [-1.34946255e-04 -2.48907687e-04  3.21707223e-03  4.34115296e-03\n",
      "  7.87322875e-03  2.84695905e-03  3.81696713e-03 -7.55398301e-04\n",
      " -1.03916405e-02  5.07246586e-05]\n",
      "finite-diff grad [-0.00018488  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00018488  0.00182353  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00018488  0.00182353 -0.00030448  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00018488  0.00182353 -0.00030448 -0.00060395  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00018488  0.00182353 -0.00030448 -0.00060395  0.00648295  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00018488  0.00182353 -0.00030448 -0.00060395  0.00648295  0.00696868\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-1.8487696e-04  1.8235295e-03 -3.0448247e-04 -6.0394517e-04\n",
      "  6.4829504e-03  6.9686817e-03  9.5688782e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.8487696e-04  1.8235295e-03 -3.0448247e-04 -6.0394517e-04\n",
      "  6.4829504e-03  6.9686817e-03  9.5688782e-05  2.5143742e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.8487696e-04  1.8235295e-03 -3.0448247e-04 -6.0394517e-04\n",
      "  6.4829504e-03  6.9686817e-03  9.5688782e-05  2.5143742e-03\n",
      "  3.4164987e-03  0.0000000e+00]\n",
      "finite-diff grad [-1.8487696e-04  1.8235295e-03 -3.0448247e-04 -6.0394517e-04\n",
      "  6.4829504e-03  6.9686817e-03  9.5688782e-05  2.5143742e-03\n",
      "  3.4164987e-03 -7.7481763e-03]\n",
      "finite-diff grad [-0.00915583  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00915583  0.00590388  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00915583  0.00590388  0.00470501  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00915583  0.00590388  0.00470501 -0.00113811  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00915583  0.00590388  0.00470501 -0.00113811 -0.00028589  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00915583  0.00590388  0.00470501 -0.00113811 -0.00028589  0.00640774\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-9.1558266e-03  5.9038829e-03  4.7050091e-03 -1.1381144e-03\n",
      " -2.8588888e-04  6.4077415e-03  8.1345868e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.1558266e-03  5.9038829e-03  4.7050091e-03 -1.1381144e-03\n",
      " -2.8588888e-04  6.4077415e-03  8.1345868e-05 -2.8604030e-04\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.1558266e-03  5.9038829e-03  4.7050091e-03 -1.1381144e-03\n",
      " -2.8588888e-04  6.4077415e-03  8.1345868e-05 -2.8604030e-04\n",
      "  3.3938473e-03  0.0000000e+00]\n",
      "finite-diff grad [-9.1558266e-03  5.9038829e-03  4.7050091e-03 -1.1381144e-03\n",
      " -2.8588888e-04  6.4077415e-03  8.1345868e-05 -2.8604030e-04\n",
      "  3.3938473e-03  2.6093435e-03]\n",
      "finite-diff grad [-0.00031594  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.00729744\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.00729744\n",
      "  0.00127492  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.00729744\n",
      "  0.00127492 -0.00749668  0.          0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.00729744\n",
      "  0.00127492 -0.00749668 -0.00052216  0.        ]\n",
      "finite-diff grad [-0.00031594  0.00016233  0.00264167  0.00248875  0.00525717  0.00729744\n",
      "  0.00127492 -0.00749668 -0.00052216 -0.00017834]\n",
      "finite-diff grad [0.0024461 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.       ]\n",
      "finite-diff grad [0.0024461  0.00235449 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.0024461  0.00235449 0.00011736 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.0024461  0.00235449 0.00011736 0.00204611 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  4.20732563e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  4.20732563e-03  7.26735126e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  4.20732563e-03  7.26735126e-03 -7.58267764e-04\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  4.20732563e-03  7.26735126e-03 -7.58267764e-04\n",
      " -3.09479074e-04  0.00000000e+00]\n",
      "finite-diff grad [ 2.44610454e-03  2.35448941e-03  1.17362266e-04  2.04611314e-03\n",
      " -9.17325378e-05  4.20732563e-03  7.26735126e-03 -7.58267764e-04\n",
      " -3.09479074e-04 -6.52996637e-03]\n",
      "finite-diff grad [-0.0055508  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "finite-diff grad [-0.0055508  -0.00016744  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.0055508  -0.00016744  0.00782255  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.0055508  -0.00016744  0.00782255  0.00265551  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  7.4367585e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  7.4367585e-05 -2.0207938e-04  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  7.4367585e-05 -2.0207938e-04 -8.1283995e-04\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  7.4367585e-05 -2.0207938e-04 -8.1283995e-04\n",
      "  1.8866909e-03  0.0000000e+00]\n",
      "finite-diff grad [-5.5507990e-03 -1.6743723e-04  7.8225536e-03  2.6555096e-03\n",
      " -2.3489785e-05  7.4367585e-05 -2.0207938e-04 -8.1283995e-04\n",
      "  1.8866909e-03  5.5688126e-03]\n",
      "finite-diff grad [-9.025302e-05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04 -6.6841571e-03  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04 -6.6841571e-03  1.5975562e-03  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04 -6.6841571e-03  1.5975562e-03  2.4256851e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04 -6.6841571e-03  1.5975562e-03  2.4256851e-03\n",
      "  4.9108500e-03  0.0000000e+00]\n",
      "finite-diff grad [-9.0253023e-05 -5.9732603e-05 -3.3323158e-04  7.4056010e-03\n",
      "  6.5282284e-04 -6.6841571e-03  1.5975562e-03  2.4256851e-03\n",
      "  4.9108500e-03  9.3137001e-05]\n",
      "finite-diff grad [0.00813028 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00813028 0.00137588 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00813028 0.00137588 0.00417665 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [8.1302812e-03 1.3758797e-03 4.1766460e-03 9.0503330e-05 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "finite-diff grad [8.1302812e-03 1.3758797e-03 4.1766460e-03 9.0503330e-05 2.0683024e-03\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "finite-diff grad [ 8.1302812e-03  1.3758797e-03  4.1766460e-03  9.0503330e-05\n",
      "  2.0683024e-03 -1.8303777e-04  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 8.1302812e-03  1.3758797e-03  4.1766460e-03  9.0503330e-05\n",
      "  2.0683024e-03 -1.8303777e-04 -5.6362911e-03  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 8.1302812e-03  1.3758797e-03  4.1766460e-03  9.0503330e-05\n",
      "  2.0683024e-03 -1.8303777e-04 -5.6362911e-03 -8.3457715e-05\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 8.1302812e-03  1.3758797e-03  4.1766460e-03  9.0503330e-05\n",
      "  2.0683024e-03 -1.8303777e-04 -5.6362911e-03 -8.3457715e-05\n",
      "  2.3010666e-03  0.0000000e+00]\n",
      "finite-diff grad [ 8.1302812e-03  1.3758797e-03  4.1766460e-03  9.0503330e-05\n",
      "  2.0683024e-03 -1.8303777e-04 -5.6362911e-03 -8.3457715e-05\n",
      "  2.3010666e-03 -7.7989895e-04]\n",
      "finite-diff grad [0.00463209 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561 -0.00011404\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561 -0.00011404\n",
      "  0.00014084  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561 -0.00011404\n",
      "  0.00014084 -0.00016259  0.          0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561 -0.00011404\n",
      "  0.00014084 -0.00016259  0.00023397  0.        ]\n",
      "finite-diff grad [ 0.00463209 -0.00040544  0.00237105 -0.00614409 -0.00239561 -0.00011404\n",
      "  0.00014084 -0.00016259  0.00023397  0.00877894]\n",
      "finite-diff grad [-0.00010356  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.00262675  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.00262675 -0.00544223\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.00262675 -0.00544223\n",
      "  0.00014796  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.00262675 -0.00544223\n",
      "  0.00014796 -0.0001445   0.          0.        ]\n",
      "finite-diff grad [-0.00010356  0.00328716  0.00243505  0.00580697  0.00262675 -0.00544223\n",
      "  0.00014796 -0.0001445  -0.00030591  0.        ]\n",
      "finite-diff grad [-1.0355583e-04  3.2871633e-03  2.4350458e-03  5.8069695e-03\n",
      "  2.6267543e-03 -5.4422305e-03  1.4796374e-04 -1.4449842e-04\n",
      " -3.0590734e-04 -6.3209154e-05]\n",
      "Epoch 040/50 | Train-Loss 0.0020 | Test-MSE 9.5024 | Regret 0.0002 | Fair-Val 0.0000\n",
      "finite-diff grad [-0.00437687  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.00012494\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.00012494\n",
      " -0.00141454  0.          0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.00012494\n",
      " -0.00141454  0.00018577  0.          0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.00012494\n",
      " -0.00141454  0.00018577 -0.00044778  0.        ]\n",
      "finite-diff grad [-0.00437687  0.00552745 -0.00025568  0.00812198 -0.00011593  0.00012494\n",
      " -0.00141454  0.00018577 -0.00044778  0.00236393]\n",
      "finite-diff grad [-0.00031209  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031209 -0.004593    0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031209 -0.004593    0.00070596  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031209 -0.004593    0.00070596 -0.00523024  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00031209 -0.004593    0.00070596 -0.00523024  0.00766066  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-3.1209021e-04 -4.5930021e-03  7.0595625e-04 -5.2302415e-03\n",
      "  7.6606637e-03  2.5635447e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-3.1209021e-04 -4.5930021e-03  7.0595625e-04 -5.2302415e-03\n",
      "  7.6606637e-03  2.5635447e-05 -7.6335105e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-3.1209021e-04 -4.5930021e-03  7.0595625e-04 -5.2302415e-03\n",
      "  7.6606637e-03  2.5635447e-05 -7.6335105e-05  9.3632756e-05\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-3.1209021e-04 -4.5930021e-03  7.0595625e-04 -5.2302415e-03\n",
      "  7.6606637e-03  2.5635447e-05 -7.6335105e-05  9.3632756e-05\n",
      "  1.9430611e-03  0.0000000e+00]\n",
      "finite-diff grad [-3.1209021e-04 -4.5930021e-03  7.0595625e-04 -5.2302415e-03\n",
      "  7.6606637e-03  2.5635447e-05 -7.6335105e-05  9.3632756e-05\n",
      "  1.9430611e-03  4.7944230e-03]\n",
      "finite-diff grad [-0.00023472  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00023472  0.00764115  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00023472  0.00764115  0.00465485  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00023472  0.00764115  0.00465485 -0.00029435  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00023472  0.00764115  0.00465485 -0.00029435 -0.00085647  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00023472  0.00764115  0.00465485 -0.00029435 -0.00085647 -0.00609249\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-2.3472038e-04  7.6411506e-03  4.6548536e-03 -2.9435469e-04\n",
      " -8.5646909e-04 -6.0924874e-03 -8.3637671e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-2.3472038e-04  7.6411506e-03  4.6548536e-03 -2.9435469e-04\n",
      " -8.5646909e-04 -6.0924874e-03 -8.3637671e-05  1.1008896e-04\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-2.3472038e-04  7.6411506e-03  4.6548536e-03 -2.9435469e-04\n",
      " -8.5646909e-04 -6.0924874e-03 -8.3637671e-05  1.1008896e-04\n",
      " -2.9745409e-03  0.0000000e+00]\n",
      "finite-diff grad [-2.3472038e-04  7.6411506e-03  4.6548536e-03 -2.9435469e-04\n",
      " -8.5646909e-04 -6.0924874e-03 -8.3637671e-05  1.1008896e-04\n",
      " -2.9745409e-03  2.1964454e-03]\n",
      "finite-diff grad [0.00656842 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00656842 0.00320042 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [6.5684184e-03 3.2004190e-03 1.4902753e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03 -1.4328695e-04  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03 -1.4328695e-04  6.6161633e-04  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03 -1.4328695e-04  6.6161633e-04 -7.9263514e-04\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03 -1.4328695e-04  6.6161633e-04 -7.9263514e-04\n",
      "  1.3258333e-03  0.0000000e+00]\n",
      "finite-diff grad [ 6.5684184e-03  3.2004190e-03  1.4902753e-06 -2.1482749e-04\n",
      " -3.5340544e-03 -1.4328695e-04  6.6161633e-04 -7.9263514e-04\n",
      "  1.3258333e-03  1.1725346e-04]\n",
      "finite-diff grad [0.00010226 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00010226 0.00021596 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [1.02263206e-04 2.15963082e-04 6.81839010e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "finite-diff grad [1.02263206e-04 2.15963082e-04 6.81839010e-05 2.23810063e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "finite-diff grad [1.02263206e-04 2.15963082e-04 6.81839010e-05 2.23810063e-03\n",
      " 7.93092884e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "finite-diff grad [ 1.02263206e-04  2.15963082e-04  6.81839010e-05  2.23810063e-03\n",
      "  7.93092884e-03 -1.47232157e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 1.02263206e-04  2.15963082e-04  6.81839010e-05  2.23810063e-03\n",
      "  7.93092884e-03 -1.47232157e-03 -4.30623349e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 1.02263206e-04  2.15963082e-04  6.81839010e-05  2.23810063e-03\n",
      "  7.93092884e-03 -1.47232157e-03 -4.30623349e-03  2.36811000e-03\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [ 1.02263206e-04  2.15963082e-04  6.81839010e-05  2.23810063e-03\n",
      "  7.93092884e-03 -1.47232157e-03 -4.30623349e-03  2.36811000e-03\n",
      " -5.66769252e-03  0.00000000e+00]\n",
      "finite-diff grad [ 1.02263206e-04  2.15963082e-04  6.81839010e-05  2.23810063e-03\n",
      "  7.93092884e-03 -1.47232157e-03 -4.30623349e-03  2.36811000e-03\n",
      " -5.66769252e-03 -1.50030712e-04]\n",
      "finite-diff grad [0.00083824 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00083824 0.00158556 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00083824 0.00158556 0.00345789 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00083824 0.00158556 0.00345789 0.00115015 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00083824 0.00158556 0.00345789 0.00115015 0.00015176 0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [0.00083824 0.00158556 0.00345789 0.00115015 0.00015176 0.00644974\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00083824  0.00158556  0.00345789  0.00115015  0.00015176  0.00644974\n",
      " -0.00490324  0.          0.          0.        ]\n",
      "finite-diff grad [ 0.00083824  0.00158556  0.00345789  0.00115015  0.00015176  0.00644974\n",
      " -0.00490324 -0.00018095  0.          0.        ]\n",
      "finite-diff grad [ 8.3823624e-04  1.5855565e-03  3.4578948e-03  1.1501497e-03\n",
      "  1.5175578e-04  6.4497422e-03 -4.9032434e-03 -1.8095000e-04\n",
      " -3.2704102e-05  0.0000000e+00]\n",
      "finite-diff grad [ 8.3823624e-04  1.5855565e-03  3.4578948e-03  1.1501497e-03\n",
      "  1.5175578e-04  6.4497422e-03 -4.9032434e-03 -1.8095000e-04\n",
      " -3.2704102e-05 -1.2496460e-04]\n",
      "finite-diff grad [-0.0031506  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03 -4.67709871e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03 -4.67709871e-03  1.38051121e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03 -4.67709871e-03  1.38051121e-03 -1.37329043e-04\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03 -4.67709871e-03  1.38051121e-03 -1.37329043e-04\n",
      " -2.56937463e-04  0.00000000e+00]\n",
      "finite-diff grad [-3.15059884e-03  1.11498175e-05  6.40492141e-03  8.47950650e-05\n",
      "  4.63126367e-03 -4.67709871e-03  1.38051121e-03 -1.37329043e-04\n",
      " -2.56937463e-04  4.15765571e-05]\n",
      "finite-diff grad [-0.00013411  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013411  0.00729874  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013411  0.00729874  0.0040202   0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013411  0.00729874  0.0040202   0.00175365  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-0.00013411  0.00729874  0.0040202   0.00175365  0.00100711  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [-1.3410614e-04  7.2987378e-03  4.0202015e-03  1.7536533e-03\n",
      "  1.0071077e-03 -3.5008346e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.3410614e-04  7.2987378e-03  4.0202015e-03  1.7536533e-03\n",
      "  1.0071077e-03 -3.5008346e-05  1.0741780e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.3410614e-04  7.2987378e-03  4.0202015e-03  1.7536533e-03\n",
      "  1.0071077e-03 -3.5008346e-05  1.0741780e-05 -5.2015521e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [-1.3410614e-04  7.2987378e-03  4.0202015e-03  1.7536533e-03\n",
      "  1.0071077e-03 -3.5008346e-05  1.0741780e-05 -5.2015521e-03\n",
      " -1.4781095e-04  0.0000000e+00]\n",
      "finite-diff grad [-1.3410614e-04  7.2987378e-03  4.0202015e-03  1.7536533e-03\n",
      "  1.0071077e-03 -3.5008346e-05  1.0741780e-05 -5.2015521e-03\n",
      " -1.4781095e-04 -2.7699121e-03]\n",
      "finite-diff grad [0.00159729 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [1.5972903e-03 6.4564731e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03 -2.2273457e-04  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03 -2.2273457e-04 -9.7616881e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03 -2.2273457e-04 -9.7616881e-05  6.1877812e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03 -2.2273457e-04 -9.7616881e-05  6.1877812e-03\n",
      " -2.7308466e-03  0.0000000e+00]\n",
      "finite-diff grad [ 1.5972903e-03  6.4564731e-05 -1.2962751e-03  6.6384533e-03\n",
      " -4.8505333e-03 -2.2273457e-04 -9.7616881e-05  6.1877812e-03\n",
      " -2.7308466e-03 -7.6170174e-05]\n",
      "finite-diff grad [0.00017352 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "finite-diff grad [ 0.00017352 -0.00340932  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04 -9.5786141e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04 -9.5786141e-05 -1.1495565e-06  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04 -9.5786141e-05 -1.1495565e-06 -3.3383414e-03\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04 -9.5786141e-05 -1.1495565e-06 -3.3383414e-03\n",
      "  4.2588795e-03  0.0000000e+00]\n",
      "finite-diff grad [ 1.7352414e-04 -3.4093198e-03 -7.7896519e-05  5.8785887e-03\n",
      " -1.8070806e-04 -9.5786141e-05 -1.1495565e-06 -3.3383414e-03\n",
      "  4.2588795e-03  1.8181228e-03]\n",
      "Epoch 050/50 | Train-Loss 0.0013 | Test-MSE 9.4480 | Regret 0.0002 | Fair-Val 0.0000\n",
      "Training finished in 0.52s.\n"
     ]
    }
   ],
   "source": [
    "# Fix the NaN values in the DataFrame\n",
    "hyperparams = {\n",
    "    \"alpha\": 0.8,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 1,\n",
    "    \"fairness_type\": \"mad\",   \n",
    "    \"group\": True,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"finite-diff\",\n",
    "    \"num_epochs\": 50,        \n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
