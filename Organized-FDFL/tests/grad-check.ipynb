{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "737584dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.utils.myOptimization import (\n",
    "    AlphaFairness, AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form,\n",
    "    solve_group, compute_coupled_group_obj,\n",
    "    solve_group_grad, compute_gradient_closed_form,\n",
    "    compute_group_gradient_analytical, solve_d_and_gradient_analytical\n",
    ")\n",
    "\n",
    "\n",
    "def solve_d_and_gradient_analytical(g, r, c, alpha, Q):\n",
    "    \"\"\"\n",
    "    Computes the optimal decision d* and its analytical gradient w.r.t. r.\n",
    "\n",
    "    Args:\n",
    "        g (np.ndarray): Gain factors, shape (n,).\n",
    "        r (np.ndarray): Predicted risk values, shape (n,).\n",
    "        c (np.ndarray): Cost values, shape (n,).\n",
    "        alpha (float or str): Fairness parameter.\n",
    "        Q (float): Total budget.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]:\n",
    "            - d_star (np.ndarray): The optimal decision vector, shape (n,).\n",
    "            - grad_d_r (np.ndarray): The Jacobian matrix (gradient) of d* w.r.t. r,\n",
    "                                     shape (n, n), where grad[i, k] = ∂d_i*/∂r_k.\n",
    "    \"\"\"\n",
    "    n = len(r)\n",
    "    g, r, c = map(np.atleast_1d, [g, r, c])\n",
    "    utility = g * r\n",
    "    grad_d_r = np.zeros((n, n))\n",
    "\n",
    "    # --- 1. Solve for d* ---\n",
    "    if alpha == 0:\n",
    "        # Utilitarian: non-differentiable, handled below.\n",
    "        i_star = np.argmax(utility / c)\n",
    "        d_star = np.zeros(n)\n",
    "        d_star[i_star] = Q / c[i_star]\n",
    "    elif alpha == 1:\n",
    "        d_star = Q / (n * c)\n",
    "    elif alpha == 'inf':\n",
    "        sum_cost_per_utility = np.sum(c / utility)\n",
    "        d_star = (1 / utility) * (Q / sum_cost_per_utility)\n",
    "    else: # General alpha\n",
    "        common_terms = np.power(c, -1/alpha) * np.power(utility, (1-alpha)/alpha)\n",
    "        denominator = np.sum(c * common_terms)\n",
    "        d_star = (Q * common_terms) / denominator\n",
    "\n",
    "    # --- 2. Compute Gradient ∂d*/∂r ---\n",
    "    if alpha == 0:\n",
    "        # The argmax operation is non-differentiable.\n",
    "        # The gradient is 0 almost everywhere. Finite difference is the practical approach.\n",
    "        pass # grad_d_r is already zeros\n",
    "    elif alpha == 1:\n",
    "        # d* does not depend on r, so the gradient is zero.\n",
    "        pass # grad_d_r is already zeros\n",
    "    elif alpha == 'inf':\n",
    "        for i in range(n):\n",
    "            for k in range(n):\n",
    "                if i == k: # Diagonal: ∂d_i*/∂r_i\n",
    "                    grad_d_r[i, i] = -d_star[i] / r[i] * (1 - c[i] * d_star[i] / Q)\n",
    "                else: # Off-diagonal: ∂d_i*/∂r_k\n",
    "                    grad_d_r[i, k] = d_star[i] * d_star[k] * c[k] / (r[k] * Q)\n",
    "    else: # General alpha\n",
    "        # Precompute the common term from the derivative\n",
    "        term = (1 - alpha) / (alpha * r) # Shape (n,)\n",
    "        for i in range(n):\n",
    "            for k in range(n):\n",
    "                if i == k: # Diagonal: ∂d_i*/∂r_i\n",
    "                    grad_d_r[i, i] = d_star[i] * term[i] * (1 - c[i] * d_star[i] / Q)\n",
    "                else: # Off-diagonal: ∂d_i*/∂r_k\n",
    "                    grad_d_r[i, k] = -d_star[i] * d_star[k] * c[k] * term[k] / Q\n",
    "\n",
    "    return d_star, grad_d_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1605eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlphaFairness(util, alpha):\n",
    "    if isinstance(util, torch.Tensor):\n",
    "        util = util.detach().cpu().numpy() if isinstance(util, torch.Tensor) else util\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(util))\n",
    "    elif alpha == 0:\n",
    "        return np.sum(util)\n",
    "    elif alpha == 'inf':\n",
    "        return np.min(util)\n",
    "    else:\n",
    "        return np.sum(util**(1-alpha) / (1-alpha))\n",
    "\n",
    "\n",
    "def solve_closed_form(g, r, c, alpha, Q):\n",
    "\n",
    "    g = g.detach().cpu().numpy() if isinstance(g, torch.Tensor) else g\n",
    "    r = r.detach().cpu().numpy() if isinstance(r, torch.Tensor) else r\n",
    "    c = c.detach().cpu().numpy() if isinstance(c, torch.Tensor) else c\n",
    "    if c.shape != r.shape or c.shape != g.shape:\n",
    "        raise ValueError(\"c, r, and g must have the same shape.\")\n",
    "    if np.any(c <= 0):\n",
    "        raise ValueError(\"All cost values must be positive.\")\n",
    "    if np.any(r <= 0):\n",
    "        raise ValueError(\"All risk values must be positive.\")\n",
    "    if np.any(g <= 0):\n",
    "        raise ValueError(\"All gain factors must be positive.\")\n",
    "    \n",
    "    n = len(c)\n",
    "    utility = r * g\n",
    "    \n",
    "    if alpha == 0:\n",
    "        ratios = utility / c\n",
    "        sorted_indices = np.argsort(-ratios)  # Descending order\n",
    "        d_star_closed = np.zeros(n)\n",
    "        d_star_closed[sorted_indices[0]] = Q / c[sorted_indices[0]]\n",
    "        \n",
    "    elif alpha == 1:\n",
    "        d_star_closed = Q / (n * c)\n",
    "    \n",
    "    elif alpha == 'inf':\n",
    "        d_star_closed = (Q * c) / (utility * np.sum(c * c / utility))\n",
    "        \n",
    "    else:\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Alpha must be positive for general case.\")\n",
    "        \n",
    "        # This vector is common to the numerator of d_i and the terms in the sum\n",
    "        # It corresponds to c_i^(-1/alpha) * utility_i^(1/alpha - 1)\n",
    "        common_terms = np.power(c, -1/alpha) * np.power(utility, 1/alpha - 1)\n",
    "        \n",
    "        # The correct denominator is Σ_j(c_j * common_term_j)\n",
    "        denominator = np.sum(c * common_terms)\n",
    "        \n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Denominator is zero in closed-form solution.\")\n",
    "            \n",
    "        # The numerator of d_i is Q * common_term_i\n",
    "        d_star_closed = (Q * common_terms) / denominator\n",
    "    \n",
    "    # if not np.isclose(np.sum(c * d_star_closed), Q, rtol=1e-5):\n",
    "    #     raise ValueError(\"Solution does not satisfy budget constraint.\")\n",
    "    obj = AlphaFairness(d_star_closed * utility, alpha)\n",
    "        \n",
    "    return d_star_closed, obj\n",
    "\n",
    "def compute_gradient_closed_form(g, r, c, alpha, Q):\n",
    "    \"\"\"\n",
    "    Compute the analytical gradient of the optimal solution with respect to r.\n",
    "\n",
    "    This function computes the gradient matrix where each element (i, k) is the partial derivative\n",
    "    of d_i* with respect to r_k.\n",
    "\n",
    "    Parameters:\n",
    "    - g (np.ndarray): Gain factors (g_i), shape (n,)\n",
    "    - r (np.ndarray): Risk values (r_i), shape (n,)\n",
    "    - c (np.ndarray): Cost values (c_i), shape (n,)\n",
    "    - alpha (float or str): Fairness parameter. Can be 0, 1, 'inf', or a positive real number.\n",
    "    - Q (float): Total budget.\n",
    "\n",
    "    Returns:\n",
    "    - gradient (np.ndarray): Gradient matrix of shape (n, n)\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        S = np.sum(c / (r * g))\n",
    "\n",
    "    if alpha == 0:\n",
    "        # Utilitarian case: Allocate everything to the individual with the highest ratio\n",
    "        ratios = (r * g) / c\n",
    "        i_star = np.argmax(ratios)\n",
    "        # Gradient is Q * g_i / c_i at the allocated index, zero elsewhere\n",
    "        gradient[i_star, i_star] = Q * g[i_star] / c[i_star]\n",
    "        return gradient\n",
    "\n",
    "    elif alpha == 'inf':\n",
    "        # Maximin case\n",
    "        n = len(c)\n",
    "        utility = r * g  # Shape: (n,)\n",
    "        S = np.sum(c**2 / utility)  # Scalar\n",
    "\n",
    "        # Compute d_star\n",
    "        d_star, _ = solve_closed_form(g,r,c, alpha='inf', Q=Q)  # Shape: (n,)\n",
    "\n",
    "        # Initialize gradient matrix\n",
    "        gradient = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for k in range(n):\n",
    "                if i == k:\n",
    "                    # ∂d_i*/∂r_i = -d_i*/r_i - (d_i* * c_i) / (r_i * g_i * S)\n",
    "                    gradient[i, k] = -d_star[i] / r[i] - (d_star[i] * c[i]) / (r[i] * g[i] * S)\n",
    "                else:\n",
    "                    # ∂d_i*/∂r_k = (d_i* * c_k^2) / (c_i * r_k^2 * g_k * S)\n",
    "                    gradient[i, k] = (d_star[i] * c[k]**2) / (c[i] * r[k]**2 * g[k] * S)\n",
    "        return gradient\n",
    "\n",
    "    else:\n",
    "        # General alpha case\n",
    "        if not isinstance(alpha, (int, float)):\n",
    "            raise TypeError(\"Alpha must be a positive real number, 0, 1, or 'inf'.\")\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Alpha must be positive for gradient computation.\")\n",
    "\n",
    "        # Compute the optimal decision variables\n",
    "        d_star, _ = solve_closed_form(g, r, c, alpha, Q)  # Shape: (n,)\n",
    "\n",
    "        # Compute the term (1/alpha - 1) * g / r\n",
    "        term = (1.0 / alpha - 1.0) * g / r  # Shape: (n,)\n",
    "\n",
    "        # Compute the outer product for off-diagonal elements\n",
    "        # Each element (i, k) = -d_star[i] * d_star[k] * term[k] / Q\n",
    "        gradient = -np.outer(d_star, d_star * term*c) / Q  # Shape: (n, n)\n",
    "\n",
    "        # Compute the diagonal elements\n",
    "        # Each diagonal element (i, i) = d_star[i] * term[i] * (1 - d_star[i]/Q)\n",
    "        diag_elements = d_star * term * (1 - c*d_star / Q)  # Shape: (n,)\n",
    "\n",
    "        # Set the diagonal elements\n",
    "        np.fill_diagonal(gradient, diag_elements)\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5397e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Analytical Gradients ---\n",
      "\n",
      "Alpha = 0.5  | Status: ✅ Matches\n",
      "Alpha = 2.0  | Status: ✅ Matches\n",
      "\n",
      "Note: alpha=0 is non-differentiable and its analytical gradient is zero almost everywhere.\n",
      "Finite difference is the only practical method for alpha=0 if a gradient is needed.\n"
     ]
    }
   ],
   "source": [
    "def get_finite_difference_gradient(g, r, c, alpha, Q, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the gradient of d* w.r.t. r using finite differences for verification.\n",
    "    \"\"\"\n",
    "    n = len(r)\n",
    "    grad_d_r_fd = np.zeros((n, n))\n",
    "    \n",
    "    # Base case d*\n",
    "    d_star_base, _ = solve_d_and_gradient_analytical(g, r, c, alpha, Q)\n",
    "\n",
    "    for k in range(n):\n",
    "        # Perturb r_k\n",
    "        r_plus = r.copy()\n",
    "        r_plus[k] += eps\n",
    "        d_star_plus, _ = solve_d_and_gradient_analytical(g, r_plus, c, alpha, Q)\n",
    "        \n",
    "        # Compute the change\n",
    "        grad_d_r_fd[:, k] = (d_star_plus - d_star_base) / eps\n",
    "        \n",
    "    return grad_d_r_fd\n",
    "\n",
    "# --- Main Verification Script ---\n",
    "if __name__ == '__main__':\n",
    "    # Setup problem\n",
    "    n_items = 50\n",
    "    np.random.seed(42)\n",
    "    g_test = np.ones(n_items)\n",
    "    r_test = np.random.rand(n_items) + 0.1\n",
    "    c_test = np.random.rand(n_items) + 0.1\n",
    "    Q_test = 100.0\n",
    "    \n",
    "    print(\"--- Verifying Analytical Gradients ---\\n\")\n",
    "    \n",
    "    # Test for different alpha values\n",
    "    alphas_to_test = [0.5, 2.0]\n",
    "\n",
    "    for alpha_val in alphas_to_test:\n",
    "        try:\n",
    "            # 1. Get analytical solution\n",
    "            _, grad_analytical = solve_d_and_gradient_analytical(g_test, r_test, c_test, alpha_val, Q_test)\n",
    "            \n",
    "            # 2. Get finite-difference solution\n",
    "            grad_fd = get_finite_difference_gradient(g_test, r_test, c_test, alpha_val, Q_test)\n",
    "            \n",
    "            # 3. Compare the results\n",
    "            is_close = np.allclose(grad_analytical, grad_fd, atol=1e-4)\n",
    "            status = \"✅ Matches\" if is_close else \"❌ Mismatch\"\n",
    "            \n",
    "            print(f\"Alpha = {str(alpha_val):<4} | Status: {status}\")\n",
    "            if not is_close:\n",
    "                # Print the difference if they don't match\n",
    "                print(\"Max difference:\", np.max(np.abs(grad_analytical - grad_fd)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Alpha = {str(alpha_val):<4} | Error: {e}\")\n",
    "\n",
    "    # Note on alpha = 0\n",
    "    print(\"\\nNote: alpha=0 is non-differentiable and its analytical gradient is zero almost everywhere.\")\n",
    "    print(\"Finite difference is the only practical method for alpha=0 if a gradient is needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3faba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient:\n",
      " [[-2.54822573e+00  1.90985158e-02  2.95407406e-02 ...  3.27083560e-02\n",
      "   1.49729316e-02  6.59437082e-02]\n",
      " [ 5.16835705e-02 -8.64201573e-01  2.19475534e-02 ...  2.43009611e-02\n",
      "   1.11242714e-02  4.89934586e-02]\n",
      " [ 5.32917166e-02  1.46309172e-02 -1.12119446e+00 ...  2.50570911e-02\n",
      "   1.14704056e-02  5.05179014e-02]\n",
      " ...\n",
      " [ 8.66530860e-02  2.37900787e-02  3.67974429e-02 ... -2.45479780e+00\n",
      "   1.86510420e-02  8.21428231e-02]\n",
      " [ 1.74018351e-01  4.77756818e-02  7.38973143e-02 ...  8.18212276e-02\n",
      "  -4.76767629e+00  1.64960757e-01]\n",
      " [ 2.03658255e-01  5.59131375e-02  8.64839714e-02 ...  9.57575356e-02\n",
      "   4.38350075e-02 -1.25742405e+01]]\n",
      "\n",
      "Finite Difference Gradient:\n",
      " [[-2.54822178e+00  1.90985023e-02  2.95407143e-02 ...  3.27083169e-02\n",
      "   1.49729145e-02  6.59435360e-02]\n",
      " [ 5.16834902e-02 -8.64200962e-01  2.19475338e-02 ...  2.43009322e-02\n",
      "   1.11242588e-02  4.89933309e-02]\n",
      " [ 5.32916340e-02  1.46309072e-02 -1.12119346e+00 ...  2.50570613e-02\n",
      "   1.14703926e-02  5.05177697e-02]\n",
      " ...\n",
      " [ 8.66529515e-02  2.37900624e-02  3.67974105e-02 ... -2.45479486e+00\n",
      "   1.86510212e-02  8.21426092e-02]\n",
      " [ 1.74018081e-01  4.77756483e-02  7.38972483e-02 ...  8.18211294e-02\n",
      "  -4.76767078e+00  1.64960326e-01]\n",
      " [ 2.03657939e-01  5.59130982e-02  8.64838938e-02 ...  9.57574215e-02\n",
      "   4.38349579e-02 -1.25742077e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Analytical Gradient:\\n\", grad_analytical)\n",
    "print(\"\\nFinite Difference Gradient:\\n\", grad_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b52f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SLOW version...\n",
      "\n",
      "Running FAST version...\n",
      "\n",
      "Results from both versions match.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
