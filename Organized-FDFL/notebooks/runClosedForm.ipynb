{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0469d29",
   "metadata": {},
   "source": [
    "- Change min-risk from 0.001 to 1\n",
    "\n",
    "- Write fold-opt subsection.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9450ea9",
   "metadata": {},
   "source": [
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "     - Closed-Form Done\n",
    "     - <b>2-Stage Done</b>\n",
    "\n",
    "6. <b>For Fold-OPT Change PGD closed-form to solver.</b>\n",
    "\n",
    "8. Verify Individual and Group Regret Performance Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "66fb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "     AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6818df",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "39fa5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = pd.read_csv('E:\\\\myREPO\\\\Fairness-Decision-Focused-Loss\\\\Organized-FDFL\\\\src\\\\data\\\\data.csv')\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 1) \n",
    "# benefit = benefit + 1\n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f20f2",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cef7387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd6cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df314909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d661549",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "37802e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ea0565",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a78a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "22d84524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([2.])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1000])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([7.5626])\n",
      "sol_group: tensor([7.5626])\n",
      "obj_group: tensor([-218.5607])\n",
      "obj_ind: tensor([-218.5607])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847c1c",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2886a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e: # type: ignore[catch]\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2ee06d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "def _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group):\n",
    "    \"\"\"Helper to compute regret and the decision variable d_hat.\"\"\"\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        regret = obj_val_at_d_star - obj_val_at_d_hat\n",
    "        # regret = np.log1p(np.exp(regret * 10)) / 10\n",
    "        return regret, d_hat_np\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e: # type: ignore[catch]\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        # Return a zero regret and a placeholder for d_hat\n",
    "        return 0.0, np.zeros_like(pred_r_np)\n",
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd Function for regret with a closed-form or finite-difference gradient.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method):\n",
    "        # --- Loss Calculation (Regret) ---\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(\n",
    "            lambda t: to_numpy_1d(t.detach()), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "\n",
    "        regret, d_hat_np = _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group)\n",
    "        regret_loss = torch.tensor(regret, dtype=pred_r.dtype, device=pred_r.device)\n",
    "        # regret_loss = F.softplus(torch.tensor(regret, dtype=pred_r.dtype,device=pred_r.device), beta=10)\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): # type: ignore[override]\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 8\n",
    "\n",
    "        try:\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                # (Closed-form gradient calculation remains the same)\n",
    "                if params['group']:\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    v_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    Jac_mat = solve_coupled_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    vT_J_np = v_np @ Jac_mat\n",
    "                    grad_regret = -torch.from_numpy(vT_J_np).to(pred_r.device,dtype=pred_r.dtype)\n",
    "\n",
    "                else:\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - params['alpha']) * d_hat ** (-params['alpha']) # Grad of alpha-fairness obj\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "            \n",
    "            elif params['grad_method'] == 'finite-diff':\n",
    "\n",
    "                pred_r_np = to_numpy_1d(pred_r)\n",
    "                grad_regret_np = np.zeros_like(pred_r_np)\n",
    "\n",
    "                eps = 1e-3                                    # relative 0.1 %\n",
    "                eps_vec = eps * np.maximum(1.0, np.abs(pred_r_np))\n",
    "\n",
    "                # Detach and convert tensors needed for perturbations once\n",
    "                true_r_np, gainF_np, cost_np, race_np = map(\n",
    "                    lambda t: to_numpy_1d(t.detach()), [true_r, gainF, cost, race]\n",
    "                )\n",
    "\n",
    "                for i in range(len(pred_r_np)):\n",
    "                    # Perturb pred_r for forward and backward steps\n",
    "                    pred_r_plus = pred_r_np.copy(); pred_r_plus[i]  += eps_vec[i]\n",
    "                    pred_r_minus = pred_r_np.copy(); pred_r_minus[i] -= eps_vec[i]\n",
    "\n",
    "                    regret_plus, _ = _calculate_regret_and_d_hat(pred_r_plus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "                    regret_minus, _ = _calculate_regret_and_d_hat(pred_r_minus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "\n",
    "                    grad_regret_np[i] = (regret_plus - regret_minus) / (2 * eps_vec[i])\n",
    "\n",
    "                # The gradient of the loss is the negative of the gradient of the regret\n",
    "                grad_regret = torch.from_numpy(grad_regret_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Gradient calculation failed: {e}. Returning zero grad.\")\n",
    "\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Regret loss from the custom function\n",
    "        regret_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method)\n",
    "\n",
    "        # 2. Fairness penalty using standard PyTorch autograd\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "        \n",
    "        # 3. Total loss\n",
    "        total_loss = regret_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a09dba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b0ebf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "\n",
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor via direct regret minimization, logging detailed metrics\n",
    "    at each evaluation point.\n",
    "    \"\"\"\n",
    "    # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # --- Initialize Logs ---\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "    unique_groups = torch.unique(race_test).cpu().numpy()\n",
    "    per_group_mse_log = {g: [] for g in unique_groups}\n",
    "    per_group_obj_log = {g: [] for g in unique_groups}\n",
    "    per_group_pred_benefit_log = {g: [] for g in unique_groups}\n",
    "    per_group_pred_sol_log = {g: [] for g in unique_groups}\n",
    "    per_group_true_sol_log = {g: [] for g in unique_groups}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze().clamp(min=1)\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # --- Periodic Evaluation on Test Set ---\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test).squeeze().clamp(min=1)\n",
    "                # Overall MSE\n",
    "                mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "                mse_log.append(mse_val)\n",
    "\n",
    "                # Overall Regret\n",
    "                _, _, d_pred_np = _calculate_loss_and_decision(pred_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                _, _, d_true_np = _calculate_loss_and_decision(y_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                if d_pred_np is not None and d_true_np is not None:\n",
    "                    y_test_np = to_numpy_1d(y_test)\n",
    "                    race_test_np = to_numpy_1d(race_test)\n",
    "                    if group:\n",
    "                        true_obj = compute_coupled_group_obj(d_true_np, y_test_np, race_test_np, alpha)\n",
    "                        pred_obj = compute_coupled_group_obj(d_pred_np, y_test_np, race_test_np, alpha)\n",
    "                    else:\n",
    "                        true_obj = AlphaFairness(y_test_np * d_true_np, alpha)\n",
    "                        pred_obj = AlphaFairness(y_test_np * d_pred_np, alpha)\n",
    "                    norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "                else:\n",
    "                    norm_regret = np.nan\n",
    "                regret_log.append(norm_regret)\n",
    "\n",
    "                # Overall Fairness\n",
    "                fair_val = 0.0\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=0.5, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "                fairness_log.append(fair_val)\n",
    "\n",
    "                # Group-wise Metrics\n",
    "                for g in unique_groups:\n",
    "                    mask = (race_test == g)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    # Group MSE\n",
    "                    per_group_mse_log[g].append(((pred_test[mask] - y_test[mask]).pow(2)).mean().item())\n",
    "                    # Group pred Benefit\n",
    "                    per_group_pred_benefit_log[g].append(pred_test[mask].mean().item())\n",
    "\n",
    "\n",
    "                    # Group Decision Objective\n",
    "                    if d_pred_np is not None:\n",
    "                        group_mask_np = (race_test_np == g) # type: ignore\n",
    "                        # We use the true benefits (y_test) to evaluate the utility of the decisions (d_pred_np)\n",
    "                        group_utility = y_test_np[group_mask_np] * d_pred_np[group_mask_np] # type: ignore\n",
    "                        # For simplicity, we report the mean utility as the objective\n",
    "                        per_group_obj_log[g].append(group_utility.mean())\n",
    "                    else:\n",
    "                        per_group_obj_log[g].append(np.nan)\n",
    "\n",
    "                    # # Group Decision Variables\n",
    "                    # if d_pred_np is not None:\n",
    "                    #     per_group_pred_sol_log[g].append(d_pred_np[mask].mean().item())\n",
    "                    # else:\n",
    "                    #     per_group_pred_sol_log[g].append(np.nan)\n",
    "                    \n",
    "                    # # True Decision Variables\n",
    "                    # if d_true_np is not None:\n",
    "                    #     per_group_true_sol_log[g].append(d_true_np[mask].mean().item().cpu())\n",
    "                    # else:\n",
    "                    #     per_group_true_sol_log[g].append(np.nan)\n",
    "                    \n",
    "\n",
    "                print(f\"Epoch {epoch:03d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_log[-1]:.4f} | Regret {regret_log[-1]:.4f} | Fair-Val {fairness_log[-1]:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "    # Return a dictionary of all logs\n",
    "    return model, {\n",
    "        \"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log,\n",
    "        \"training_time\": total_time,\n",
    "        \"per_group_mse\": per_group_mse_log,\n",
    "        \"per_group_decision_objective\": per_group_obj_log,\n",
    "        \"per_group_true_benefit\": per_group_pred_benefit_log\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f8386a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "23a66413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha = 2\n",
    "# hyperparams = {\n",
    "#     \"alpha\":alpha,\n",
    "#     \"Q\": 1000,\n",
    "#     \"lambda_fair\": 0,\n",
    "#     \"fairness_type\": \"atkinson\",   \n",
    "#     \"group\": True,            # Set to True for group fairness, False for individual\n",
    "#     \"grad_method\": \"finite-diff\",\n",
    "#     \"num_epochs\": 20,        \n",
    "#     \"lr\": 0.005,\n",
    "#     \"batch_size\": len(b_train),\n",
    "#     \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# }\n",
    "\n",
    "# final_model, logs = train_model_regret(\n",
    "#     X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "#     X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "#     model_class=FairRiskPredictor,\n",
    "#     input_dim=feats_train.shape[1],\n",
    "#     **hyperparams\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ab4122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1.  MULTI-TRIAL REGRET TRAINING WITH FULL LOGGING\n",
    "# ---------------------------------------------------------------------\n",
    "def train_many_trials_regret(n_trials=3, base_seed=2025, **train_args):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` different seeds.\n",
    "    Returns a FLAT dict whose keys are:\n",
    "        regret, regret_std, mse, mse_std, fairness, fairness_std, …,\n",
    "        G0_mse, G0_mse_std, G0_decision_obj, G0_decision_obj_std, …\n",
    "    \"\"\"\n",
    "    # -------------------- run all trials -----------------------------\n",
    "    per_trial_metrics = defaultdict(list)      # collects trial-level scalars\n",
    "\n",
    "    final_model = None\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        final_model, logs = train_model_regret(**train_args)   # one full run\n",
    "\n",
    "        # ---- overall scalars ---------------------------------------\n",
    "        per_trial_metrics['regret'       ].append(logs['regret_log']  [-1])\n",
    "        per_trial_metrics['mse'          ].append(logs['mse_log']     [-1])\n",
    "        per_trial_metrics['fairness'     ].append(logs['fairness_log'][-1])\n",
    "        per_trial_metrics['training_time'].append(logs['training_time'])\n",
    "\n",
    "        # ---- per-group metrics (final epoch) -----------------------\n",
    "        for g_id, g_log in logs['per_group_mse'].items():\n",
    "            if g_log:                      # just in case\n",
    "                per_trial_metrics[f'G{int(g_id)}_mse'          ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_decision_objective'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_decision_obj' ].append(g_log[-1])\n",
    "\n",
    "\n",
    "    # -------------------- aggregate over trials ----------------------\n",
    "    avg_results = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      AVERAGED RESULTS ACROSS ALL TRIALS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for key, values in per_trial_metrics.items():\n",
    "        μ, sigma = np.mean(values), np.std(values)\n",
    "        avg_results[key]      = μ\n",
    "        avg_results[f'{key}_std'] = sigma\n",
    "        print(f\"[{key.upper():>20s}]  μ = {μ:.4f} | σ = {sigma:.4f}\")\n",
    "\n",
    "    return avg_results, final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175a7d",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68991548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.5233 | Test-MSE 362.5882 | Regret 0.0654 | Fair-Val 85.7126\n",
      "Epoch 010/50 | Train-Loss 28.0600 | Test-MSE 359.5536 | Regret 0.0629 | Fair-Val 84.0788\n",
      "Epoch 020/50 | Train-Loss 24.5694 | Test-MSE 349.9480 | Regret 0.0559 | Fair-Val 79.6147\n",
      "Epoch 030/50 | Train-Loss 22.4776 | Test-MSE 338.2161 | Regret 0.0529 | Fair-Val 74.0088\n",
      "Epoch 040/50 | Train-Loss 21.4223 | Test-MSE 330.0174 | Regret 0.0515 | Fair-Val 68.6312\n",
      "Epoch 050/50 | Train-Loss 20.2540 | Test-MSE 325.5493 | Regret 0.0493 | Fair-Val 64.6542\n",
      "Training finished in 267.64s.\n",
      "Epoch 001/50 | Train-Loss 29.5398 | Test-MSE 362.6682 | Regret 0.0654 | Fair-Val 85.7927\n",
      "Epoch 010/50 | Train-Loss 28.2099 | Test-MSE 360.7528 | Regret 0.0632 | Fair-Val 83.7940\n",
      "Epoch 020/50 | Train-Loss 24.9695 | Test-MSE 352.4016 | Regret 0.0568 | Fair-Val 78.6377\n",
      "Epoch 030/50 | Train-Loss 22.7432 | Test-MSE 341.1671 | Regret 0.0533 | Fair-Val 72.6761\n",
      "Epoch 040/50 | Train-Loss 21.2087 | Test-MSE 332.7242 | Regret 0.0513 | Fair-Val 67.6399\n",
      "Epoch 050/50 | Train-Loss 20.0087 | Test-MSE 327.6390 | Regret 0.0486 | Fair-Val 63.7942\n",
      "Training finished in 445.98s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0490 | σ = 0.0003\n",
      "[                 MSE]  μ = 326.5941 | σ = 1.0449\n",
      "[            FAIRNESS]  μ = 64.2242 | σ = 0.4300\n",
      "[       TRAINING_TIME]  μ = 356.8104 | σ = 89.1687\n",
      "[              G0_MSE]  μ = 311.2831 | σ = 1.1474\n",
      "[              G1_MSE]  μ = 439.7315 | σ = 0.2874\n",
      "[     G0_DECISION_OBJ]  μ = 30.1280 | σ = 0.1536\n",
      "[     G1_DECISION_OBJ]  μ = 216.1720 | σ = 4.1176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0804 | Test-MSE 362.6652 | Regret 0.0013 | Fair-Val 85.7816\n",
      "Epoch 010/50 | Train-Loss 0.0806 | Test-MSE 362.6914 | Regret 0.0013 | Fair-Val 85.8060\n",
      "Epoch 020/50 | Train-Loss 0.0805 | Test-MSE 362.7132 | Regret 0.0013 | Fair-Val 85.8104\n",
      "Epoch 030/50 | Train-Loss 0.0806 | Test-MSE 362.7295 | Regret 0.0013 | Fair-Val 85.8202\n",
      "Epoch 040/50 | Train-Loss 0.0806 | Test-MSE 362.7379 | Regret 0.0013 | Fair-Val 85.8313\n",
      "Epoch 050/50 | Train-Loss 0.0807 | Test-MSE 362.7411 | Regret 0.0013 | Fair-Val 85.8445\n",
      "Training finished in 464.27s.\n",
      "Epoch 001/50 | Train-Loss 0.0805 | Test-MSE 362.6976 | Regret 0.0013 | Fair-Val 85.8439\n",
      "Epoch 010/50 | Train-Loss 0.0803 | Test-MSE 362.7106 | Regret 0.0013 | Fair-Val 85.8435\n",
      "Epoch 020/50 | Train-Loss 0.0802 | Test-MSE 362.7162 | Regret 0.0013 | Fair-Val 85.8457\n",
      "Epoch 030/50 | Train-Loss 0.0803 | Test-MSE 362.7271 | Regret 0.0013 | Fair-Val 85.8447\n",
      "Epoch 040/50 | Train-Loss 0.0804 | Test-MSE 362.7357 | Regret 0.0013 | Fair-Val 85.8449\n",
      "Epoch 050/50 | Train-Loss 0.0805 | Test-MSE 362.7398 | Regret 0.0013 | Fair-Val 85.8452\n",
      "Training finished in 362.32s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0013 | σ = 0.0000\n",
      "[                 MSE]  μ = 362.7404 | σ = 0.0006\n",
      "[            FAIRNESS]  μ = 85.8448 | σ = 0.0004\n",
      "[       TRAINING_TIME]  μ = 413.2925 | σ = 50.9744\n",
      "[              G0_MSE]  μ = 342.2750 | σ = 0.0007\n",
      "[              G1_MSE]  μ = 513.9647 | σ = 0.0000\n",
      "[     G0_DECISION_OBJ]  μ = 18.7367 | σ = 0.0001\n",
      "[     G1_DECISION_OBJ]  μ = 155.4197 | σ = 0.0002\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.6700 | Test-MSE 362.5711 | Regret 0.0277 | Fair-Val 85.7275\n",
      "Epoch 010/50 | Train-Loss 2.3361 | Test-MSE 356.0368 | Regret 0.0244 | Fair-Val 83.2741\n",
      "Epoch 020/50 | Train-Loss 1.9996 | Test-MSE 341.6579 | Regret 0.0213 | Fair-Val 77.9836\n",
      "Epoch 030/50 | Train-Loss 1.8232 | Test-MSE 328.7763 | Regret 0.0197 | Fair-Val 72.8149\n",
      "Epoch 040/50 | Train-Loss 1.6750 | Test-MSE 320.3246 | Regret 0.0184 | Fair-Val 69.1289\n",
      "Epoch 050/50 | Train-Loss 1.5218 | Test-MSE 314.2570 | Regret 0.0170 | Fair-Val 67.0112\n",
      "Training finished in 352.64s.\n",
      "Epoch 001/50 | Train-Loss 2.6796 | Test-MSE 362.6305 | Regret 0.0277 | Fair-Val 85.7628\n",
      "Epoch 010/50 | Train-Loss 2.3915 | Test-MSE 356.9067 | Regret 0.0248 | Fair-Val 82.3280\n",
      "Epoch 020/50 | Train-Loss 2.0206 | Test-MSE 342.3160 | Regret 0.0216 | Fair-Val 76.4700\n",
      "Epoch 030/50 | Train-Loss 1.8436 | Test-MSE 328.2391 | Regret 0.0200 | Fair-Val 71.3114\n",
      "Epoch 040/50 | Train-Loss 1.6985 | Test-MSE 319.4410 | Regret 0.0188 | Fair-Val 67.9619\n",
      "Epoch 050/50 | Train-Loss 1.5419 | Test-MSE 313.8950 | Regret 0.0173 | Fair-Val 66.1429\n",
      "Training finished in 342.59s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0171 | σ = 0.0002\n",
      "[                 MSE]  μ = 314.0760 | σ = 0.1810\n",
      "[            FAIRNESS]  μ = 66.5771 | σ = 0.4341\n",
      "[       TRAINING_TIME]  μ = 347.6172 | σ = 5.0244\n",
      "[              G0_MSE]  μ = 298.2040 | σ = 0.0776\n",
      "[              G1_MSE]  μ = 431.3581 | σ = 0.9458\n",
      "[     G0_DECISION_OBJ]  μ = 18.6739 | σ = 0.0086\n",
      "[     G1_DECISION_OBJ]  μ = 60.4789 | σ = 0.1573\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 77.0923 | Test-MSE 362.5725 | Regret 0.3399 | Fair-Val 85.7344\n",
      "Epoch 010/50 | Train-Loss 66.8899 | Test-MSE 355.8989 | Regret 0.2944 | Fair-Val 83.3396\n",
      "Epoch 020/50 | Train-Loss 56.4135 | Test-MSE 341.2964 | Regret 0.2517 | Fair-Val 78.1670\n",
      "Epoch 030/50 | Train-Loss 50.4986 | Test-MSE 328.4142 | Regret 0.2307 | Fair-Val 72.9110\n",
      "Epoch 040/50 | Train-Loss 45.6707 | Test-MSE 319.5847 | Regret 0.2160 | Fair-Val 68.9799\n",
      "Epoch 050/50 | Train-Loss 41.1907 | Test-MSE 312.7964 | Regret 0.2000 | Fair-Val 66.6828\n",
      "Training finished in 402.52s.\n",
      "Epoch 001/50 | Train-Loss 77.3852 | Test-MSE 362.6297 | Regret 0.3406 | Fair-Val 85.7649\n",
      "Epoch 010/50 | Train-Loss 68.6658 | Test-MSE 356.7938 | Regret 0.3013 | Fair-Val 82.4005\n",
      "Epoch 020/50 | Train-Loss 57.1655 | Test-MSE 342.0331 | Regret 0.2571 | Fair-Val 76.6522\n",
      "Epoch 030/50 | Train-Loss 51.2218 | Test-MSE 327.9701 | Regret 0.2354 | Fair-Val 71.5093\n",
      "Epoch 040/50 | Train-Loss 46.5796 | Test-MSE 318.9726 | Regret 0.2199 | Fair-Val 67.9160\n",
      "Epoch 050/50 | Train-Loss 41.7972 | Test-MSE 312.8373 | Regret 0.2039 | Fair-Val 65.7456\n",
      "Training finished in 268.11s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.2020 | σ = 0.0019\n",
      "[                 MSE]  μ = 312.8168 | σ = 0.0204\n",
      "[            FAIRNESS]  μ = 66.2142 | σ = 0.4686\n",
      "[       TRAINING_TIME]  μ = 335.3185 | σ = 67.2040\n",
      "[              G0_MSE]  μ = 297.0313 | σ = 0.1321\n",
      "[              G1_MSE]  μ = 429.4598 | σ = 0.8050\n",
      "[     G0_DECISION_OBJ]  μ = 19.3481 | σ = 0.0170\n",
      "[     G1_DECISION_OBJ]  μ = 25.8111 | σ = 0.0934\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 130.2154 | Test-MSE 362.6483 | Regret 0.0654 | Fair-Val 85.7719\n",
      "Epoch 010/50 | Train-Loss 125.1227 | Test-MSE 360.9868 | Regret 0.0643 | Fair-Val 84.5118\n",
      "Epoch 020/50 | Train-Loss 112.3825 | Test-MSE 353.3299 | Regret 0.0585 | Fair-Val 79.4169\n",
      "Epoch 030/50 | Train-Loss 97.0297 | Test-MSE 341.2158 | Regret 0.0561 | Fair-Val 71.6073\n",
      "Epoch 040/50 | Train-Loss 79.6001 | Test-MSE 325.7060 | Regret 0.0583 | Fair-Val 61.9023\n",
      "Epoch 050/50 | Train-Loss 62.5274 | Test-MSE 308.4339 | Regret 0.0639 | Fair-Val 52.0551\n",
      "Training finished in 346.73s.\n",
      "Epoch 001/50 | Train-Loss 130.2901 | Test-MSE 362.6556 | Regret 0.0654 | Fair-Val 85.7719\n",
      "Epoch 010/50 | Train-Loss 123.4417 | Test-MSE 360.3678 | Regret 0.0631 | Fair-Val 83.1680\n",
      "Epoch 020/50 | Train-Loss 110.7962 | Test-MSE 352.4832 | Regret 0.0582 | Fair-Val 77.3392\n",
      "Epoch 030/50 | Train-Loss 96.0271 | Test-MSE 339.6193 | Regret 0.0561 | Fair-Val 69.0095\n",
      "Epoch 040/50 | Train-Loss 78.8885 | Test-MSE 323.5952 | Regret 0.0589 | Fair-Val 59.0856\n",
      "Epoch 050/50 | Train-Loss 63.4397 | Test-MSE 306.4568 | Regret 0.0648 | Fair-Val 48.9863\n",
      "Training finished in 296.03s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0644 | σ = 0.0005\n",
      "[                 MSE]  μ = 307.4454 | σ = 0.9886\n",
      "[            FAIRNESS]  μ = 50.5207 | σ = 1.5344\n",
      "[       TRAINING_TIME]  μ = 321.3794 | σ = 25.3500\n",
      "[              G0_MSE]  μ = 295.4012 | σ = 0.6228\n",
      "[              G1_MSE]  μ = 396.4426 | σ = 3.6916\n",
      "[     G0_DECISION_OBJ]  μ = 29.8892 | σ = 0.1038\n",
      "[     G1_DECISION_OBJ]  μ = 226.6886 | σ = 5.0464\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 100.7725 | Test-MSE 362.6483 | Regret 0.0013 | Fair-Val 85.7719\n",
      "Epoch 010/50 | Train-Loss 96.5796 | Test-MSE 360.9876 | Regret 0.0012 | Fair-Val 84.5123\n",
      "Epoch 020/50 | Train-Loss 86.7496 | Test-MSE 353.3367 | Regret 0.0011 | Fair-Val 79.4182\n",
      "Epoch 030/50 | Train-Loss 72.7795 | Test-MSE 341.2287 | Regret 0.0010 | Fair-Val 71.6066\n",
      "Epoch 040/50 | Train-Loss 54.5527 | Test-MSE 325.7184 | Regret 0.0011 | Fair-Val 61.8955\n",
      "Epoch 050/50 | Train-Loss 35.3403 | Test-MSE 308.4192 | Regret 0.0011 | Fair-Val 52.0435\n",
      "Training finished in 324.78s.\n",
      "Epoch 001/50 | Train-Loss 100.8308 | Test-MSE 362.6556 | Regret 0.0013 | Fair-Val 85.7719\n",
      "Epoch 010/50 | Train-Loss 95.4344 | Test-MSE 360.3693 | Regret 0.0012 | Fair-Val 83.1687\n",
      "Epoch 020/50 | Train-Loss 85.4720 | Test-MSE 352.4933 | Regret 0.0011 | Fair-Val 77.3413\n",
      "Epoch 030/50 | Train-Loss 72.1862 | Test-MSE 339.6256 | Regret 0.0010 | Fair-Val 69.0098\n",
      "Epoch 040/50 | Train-Loss 54.5452 | Test-MSE 323.5974 | Regret 0.0011 | Fair-Val 59.0837\n",
      "Epoch 050/50 | Train-Loss 36.5848 | Test-MSE 306.4532 | Regret 0.0011 | Fair-Val 48.9802\n",
      "Training finished in 279.26s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0011 | σ = 0.0000\n",
      "[                 MSE]  μ = 307.4362 | σ = 0.9830\n",
      "[            FAIRNESS]  μ = 50.5118 | σ = 1.5317\n",
      "[       TRAINING_TIME]  μ = 302.0203 | σ = 22.7581\n",
      "[              G0_MSE]  μ = 295.3941 | σ = 0.6178\n",
      "[              G1_MSE]  μ = 396.4178 | σ = 3.6811\n",
      "[     G0_DECISION_OBJ]  μ = 19.3110 | σ = 0.0099\n",
      "[     G1_DECISION_OBJ]  μ = 162.8253 | σ = 0.5470\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 103.3621 | Test-MSE 362.6467 | Regret 0.0277 | Fair-Val 85.7721\n",
      "Epoch 010/50 | Train-Loss 98.9615 | Test-MSE 360.8216 | Regret 0.0265 | Fair-Val 84.4563\n",
      "Epoch 020/50 | Train-Loss 88.7549 | Test-MSE 352.6399 | Regret 0.0234 | Fair-Val 79.2661\n",
      "Epoch 030/50 | Train-Loss 74.4656 | Test-MSE 339.9374 | Regret 0.0218 | Fair-Val 71.4170\n",
      "Epoch 040/50 | Train-Loss 56.0584 | Test-MSE 323.8929 | Regret 0.0216 | Fair-Val 61.7597\n",
      "Epoch 050/50 | Train-Loss 37.0417 | Test-MSE 306.6014 | Regret 0.0224 | Fair-Val 52.1142\n",
      "Training finished in 296.73s.\n",
      "Epoch 001/50 | Train-Loss 103.4299 | Test-MSE 362.6548 | Regret 0.0277 | Fair-Val 85.7708\n",
      "Epoch 010/50 | Train-Loss 97.8700 | Test-MSE 360.2820 | Regret 0.0262 | Fair-Val 83.1282\n",
      "Epoch 020/50 | Train-Loss 87.5531 | Test-MSE 352.0337 | Regret 0.0234 | Fair-Val 77.2430\n",
      "Epoch 030/50 | Train-Loss 74.1029 | Test-MSE 338.7573 | Regret 0.0217 | Fair-Val 68.8983\n",
      "Epoch 040/50 | Train-Loss 56.4744 | Test-MSE 322.4345 | Regret 0.0215 | Fair-Val 59.0208\n",
      "Epoch 050/50 | Train-Loss 38.6476 | Test-MSE 304.9985 | Regret 0.0223 | Fair-Val 48.9706\n",
      "Training finished in 268.70s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0224 | σ = 0.0001\n",
      "[                 MSE]  μ = 305.8000 | σ = 0.8014\n",
      "[            FAIRNESS]  μ = 50.5424 | σ = 1.5718\n",
      "[       TRAINING_TIME]  μ = 282.7134 | σ = 14.0170\n",
      "[              G0_MSE]  μ = 293.7507 | σ = 0.4267\n",
      "[              G1_MSE]  μ = 394.8355 | σ = 3.5704\n",
      "[     G0_DECISION_OBJ]  μ = 18.7271 | σ = 0.0081\n",
      "[     G1_DECISION_OBJ]  μ = 59.3638 | σ = 0.0822\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 177.7844 | Test-MSE 362.5771 | Regret 0.3400 | Fair-Val 85.7359\n",
      "Epoch 010/50 | Train-Loss 163.1482 | Test-MSE 356.4633 | Regret 0.2972 | Fair-Val 83.2471\n",
      "Epoch 020/50 | Train-Loss 142.1007 | Test-MSE 341.9572 | Regret 0.2544 | Fair-Val 77.4566\n",
      "Epoch 030/50 | Train-Loss 123.3582 | Test-MSE 325.2898 | Regret 0.2364 | Fair-Val 69.9401\n",
      "Epoch 040/50 | Train-Loss 104.9167 | Test-MSE 308.7340 | Regret 0.2292 | Fair-Val 61.1807\n",
      "Epoch 050/50 | Train-Loss 88.6840 | Test-MSE 292.8383 | Regret 0.2271 | Fair-Val 52.2089\n",
      "Training finished in 268.26s.\n",
      "Epoch 001/50 | Train-Loss 178.1356 | Test-MSE 362.6313 | Regret 0.3406 | Fair-Val 85.7629\n",
      "Epoch 010/50 | Train-Loss 164.6715 | Test-MSE 357.3351 | Regret 0.3046 | Fair-Val 82.2790\n",
      "Epoch 020/50 | Train-Loss 141.7579 | Test-MSE 342.8730 | Regret 0.2601 | Fair-Val 75.7928\n",
      "Epoch 030/50 | Train-Loss 123.0221 | Test-MSE 325.2637 | Regret 0.2415 | Fair-Val 68.1689\n",
      "Epoch 040/50 | Train-Loss 104.4815 | Test-MSE 308.2598 | Regret 0.2354 | Fair-Val 59.7016\n",
      "Epoch 050/50 | Train-Loss 88.5161 | Test-MSE 292.5441 | Regret 0.2331 | Fair-Val 51.1753\n",
      "Training finished in 268.45s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.2301 | σ = 0.0030\n",
      "[                 MSE]  μ = 292.6912 | σ = 0.1471\n",
      "[            FAIRNESS]  μ = 51.6921 | σ = 0.5168\n",
      "[       TRAINING_TIME]  μ = 268.3563 | σ = 0.0935\n",
      "[              G0_MSE]  μ = 280.3678 | σ = 0.0239\n",
      "[              G1_MSE]  μ = 383.7521 | σ = 1.0576\n",
      "[     G0_DECISION_OBJ]  μ = 19.2860 | σ = 0.0003\n",
      "[     G1_DECISION_OBJ]  μ = 24.9230 | σ = 0.0714\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 30.5302 | Test-MSE 362.6432 | Regret 0.0654 | Fair-Val 85.7556\n",
      "Epoch 010/50 | Train-Loss 29.5437 | Test-MSE 360.8899 | Regret 0.0642 | Fair-Val 84.4460\n",
      "Epoch 020/50 | Train-Loss 26.3543 | Test-MSE 352.7033 | Regret 0.0582 | Fair-Val 79.2786\n",
      "Epoch 030/50 | Train-Loss 24.6967 | Test-MSE 340.0005 | Regret 0.0556 | Fair-Val 71.5930\n",
      "Epoch 040/50 | Train-Loss 25.1136 | Test-MSE 324.0827 | Regret 0.0577 | Fair-Val 62.1698\n",
      "Epoch 050/50 | Train-Loss 26.8329 | Test-MSE 307.1001 | Regret 0.0628 | Fair-Val 52.6012\n",
      "Training finished in 266.47s.\n",
      "Epoch 001/50 | Train-Loss 30.5473 | Test-MSE 362.6559 | Regret 0.0654 | Fair-Val 85.7722\n",
      "Epoch 010/50 | Train-Loss 28.9897 | Test-MSE 360.2559 | Regret 0.0630 | Fair-Val 83.0967\n",
      "Epoch 020/50 | Train-Loss 26.0722 | Test-MSE 351.8758 | Regret 0.0578 | Fair-Val 77.1533\n",
      "Epoch 030/50 | Train-Loss 24.4456 | Test-MSE 338.5999 | Regret 0.0559 | Fair-Val 68.8598\n",
      "Epoch 040/50 | Train-Loss 24.6349 | Test-MSE 322.4050 | Regret 0.0586 | Fair-Val 59.1644\n",
      "Epoch 050/50 | Train-Loss 26.6595 | Test-MSE 305.2552 | Regret 0.0641 | Fair-Val 49.2956\n",
      "Training finished in 419.09s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0635 | σ = 0.0006\n",
      "[                 MSE]  μ = 306.1777 | σ = 0.9225\n",
      "[            FAIRNESS]  μ = 50.9484 | σ = 1.6528\n",
      "[       TRAINING_TIME]  μ = 342.7807 | σ = 76.3143\n",
      "[              G0_MSE]  μ = 294.0316 | σ = 0.5284\n",
      "[              G1_MSE]  μ = 395.9283 | σ = 3.8340\n",
      "[     G0_DECISION_OBJ]  μ = 30.1655 | σ = 0.0515\n",
      "[     G1_DECISION_OBJ]  μ = 227.1473 | σ = 4.8200\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 1.0873 | Test-MSE 362.6432 | Regret 0.0012 | Fair-Val 85.7556\n",
      "Epoch 010/50 | Train-Loss 1.0415 | Test-MSE 360.9721 | Regret 0.0012 | Fair-Val 84.4836\n",
      "Epoch 020/50 | Train-Loss 0.9340 | Test-MSE 353.3260 | Regret 0.0011 | Fair-Val 79.3717\n",
      "Epoch 030/50 | Train-Loss 0.7887 | Test-MSE 341.1286 | Regret 0.0010 | Fair-Val 71.5154\n",
      "Epoch 040/50 | Train-Loss 0.6041 | Test-MSE 325.3916 | Regret 0.0011 | Fair-Val 61.7520\n",
      "Epoch 050/50 | Train-Loss 0.4148 | Test-MSE 308.0678 | Regret 0.0011 | Fair-Val 51.9945\n",
      "Training finished in 340.55s.\n",
      "Epoch 001/50 | Train-Loss 1.0880 | Test-MSE 362.6558 | Regret 0.0013 | Fair-Val 85.7721\n",
      "Epoch 010/50 | Train-Loss 1.0294 | Test-MSE 360.3712 | Regret 0.0012 | Fair-Val 83.1614\n",
      "Epoch 020/50 | Train-Loss 0.9218 | Test-MSE 352.5085 | Regret 0.0011 | Fair-Val 77.3232\n",
      "Epoch 030/50 | Train-Loss 0.7838 | Test-MSE 339.6726 | Regret 0.0010 | Fair-Val 68.9828\n",
      "Epoch 040/50 | Train-Loss 0.6069 | Test-MSE 323.6768 | Regret 0.0011 | Fair-Val 59.0530\n",
      "Epoch 050/50 | Train-Loss 0.4308 | Test-MSE 306.5899 | Regret 0.0011 | Fair-Val 48.9691\n",
      "Training finished in 361.94s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0011 | σ = 0.0000\n",
      "[                 MSE]  μ = 307.3289 | σ = 0.7390\n",
      "[            FAIRNESS]  μ = 50.4818 | σ = 1.5127\n",
      "[       TRAINING_TIME]  μ = 351.2439 | σ = 10.6984\n",
      "[              G0_MSE]  μ = 295.2940 | σ = 0.3783\n",
      "[              G1_MSE]  μ = 396.2576 | σ = 3.4037\n",
      "[     G0_DECISION_OBJ]  μ = 19.3103 | σ = 0.0071\n",
      "[     G1_DECISION_OBJ]  μ = 162.8144 | σ = 0.5578\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3.6769 | Test-MSE 362.5726 | Regret 0.0277 | Fair-Val 85.7273\n",
      "Epoch 010/50 | Train-Loss 3.2996 | Test-MSE 356.2181 | Regret 0.0245 | Fair-Val 83.2347\n",
      "Epoch 020/50 | Train-Loss 2.8634 | Test-MSE 341.7350 | Regret 0.0213 | Fair-Val 77.5835\n",
      "Epoch 030/50 | Train-Loss 2.5789 | Test-MSE 326.7568 | Regret 0.0198 | Fair-Val 71.0170\n",
      "Epoch 040/50 | Train-Loss 2.3359 | Test-MSE 314.2299 | Regret 0.0188 | Fair-Val 64.5092\n",
      "Epoch 050/50 | Train-Loss 2.1223 | Test-MSE 303.4616 | Regret 0.0177 | Fair-Val 58.6577\n",
      "Training finished in 385.22s.\n",
      "Epoch 001/50 | Train-Loss 3.6871 | Test-MSE 362.6311 | Regret 0.0277 | Fair-Val 85.7630\n",
      "Epoch 010/50 | Train-Loss 3.3516 | Test-MSE 357.0846 | Regret 0.0249 | Fair-Val 82.2713\n",
      "Epoch 020/50 | Train-Loss 2.8716 | Test-MSE 342.4451 | Regret 0.0217 | Fair-Val 75.9838\n",
      "Epoch 030/50 | Train-Loss 2.5876 | Test-MSE 326.3674 | Regret 0.0202 | Fair-Val 69.4149\n",
      "Epoch 040/50 | Train-Loss 2.3497 | Test-MSE 313.2939 | Regret 0.0193 | Fair-Val 63.2335\n",
      "Epoch 050/50 | Train-Loss 2.1405 | Test-MSE 302.7037 | Regret 0.0183 | Fair-Val 57.8042\n",
      "Training finished in 269.27s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0180 | σ = 0.0003\n",
      "[                 MSE]  μ = 303.0827 | σ = 0.3790\n",
      "[            FAIRNESS]  μ = 58.2310 | σ = 0.4268\n",
      "[       TRAINING_TIME]  μ = 327.2468 | σ = 57.9761\n",
      "[              G0_MSE]  μ = 289.2003 | σ = 0.2772\n",
      "[              G1_MSE]  μ = 405.6623 | σ = 1.1308\n",
      "[     G0_DECISION_OBJ]  μ = 18.6525 | σ = 0.0026\n",
      "[     G1_DECISION_OBJ]  μ = 59.9648 | σ = 0.1821\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 78.0992 | Test-MSE 362.5724 | Regret 0.3399 | Fair-Val 85.7343\n",
      "Epoch 010/50 | Train-Loss 67.8506 | Test-MSE 355.8994 | Regret 0.2944 | Fair-Val 83.3360\n",
      "Epoch 020/50 | Train-Loss 57.2858 | Test-MSE 341.2715 | Regret 0.2517 | Fair-Val 78.1373\n",
      "Epoch 030/50 | Train-Loss 51.2969 | Test-MSE 328.2801 | Regret 0.2307 | Fair-Val 72.7832\n",
      "Epoch 040/50 | Train-Loss 46.4348 | Test-MSE 319.2548 | Regret 0.2162 | Fair-Val 68.6675\n",
      "Epoch 050/50 | Train-Loss 41.9582 | Test-MSE 312.2520 | Regret 0.2004 | Fair-Val 66.1505\n",
      "Training finished in 272.84s.\n",
      "Epoch 001/50 | Train-Loss 78.3927 | Test-MSE 362.6296 | Regret 0.3406 | Fair-Val 85.7648\n",
      "Epoch 010/50 | Train-Loss 69.6245 | Test-MSE 356.7945 | Regret 0.3013 | Fair-Val 82.3959\n",
      "Epoch 020/50 | Train-Loss 58.0225 | Test-MSE 342.0085 | Regret 0.2571 | Fair-Val 76.6216\n",
      "Epoch 030/50 | Train-Loss 52.0082 | Test-MSE 327.8282 | Regret 0.2354 | Fair-Val 71.3934\n",
      "Epoch 040/50 | Train-Loss 47.3309 | Test-MSE 318.6347 | Regret 0.2201 | Fair-Val 67.6292\n",
      "Epoch 050/50 | Train-Loss 42.5579 | Test-MSE 312.2805 | Regret 0.2043 | Fair-Val 65.2584\n",
      "Training finished in 394.81s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.2024 | σ = 0.0019\n",
      "[                 MSE]  μ = 312.2662 | σ = 0.0142\n",
      "[            FAIRNESS]  μ = 65.7045 | σ = 0.4460\n",
      "[       TRAINING_TIME]  μ = 333.8248 | σ = 60.9813\n",
      "[              G0_MSE]  μ = 296.6023 | σ = 0.1206\n",
      "[              G1_MSE]  μ = 428.0112 | σ = 0.7715\n",
      "[     G0_DECISION_OBJ]  μ = 19.3478 | σ = 0.0158\n",
      "[     G1_DECISION_OBJ]  μ = 25.7841 | σ = 0.0940\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.5290 | Test-MSE 362.6038 | Regret 0.0654 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 28.1536 | Test-MSE 359.7046 | Regret 0.0630 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 24.5714 | Test-MSE 349.8254 | Regret 0.0561 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 22.6066 | Test-MSE 337.6429 | Regret 0.0532 | Fair-Val 0.0043\n",
      "Epoch 040/50 | Train-Loss 21.5381 | Test-MSE 328.5183 | Regret 0.0521 | Fair-Val 0.0038\n",
      "Epoch 050/50 | Train-Loss 20.4785 | Test-MSE 322.7372 | Regret 0.0504 | Fair-Val 0.0034\n",
      "Training finished in 476.59s.\n",
      "Epoch 001/50 | Train-Loss 29.5455 | Test-MSE 362.6614 | Regret 0.0654 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 28.0794 | Test-MSE 360.3412 | Regret 0.0629 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 24.8359 | Test-MSE 351.3416 | Regret 0.0569 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 22.6804 | Test-MSE 339.3116 | Regret 0.0537 | Fair-Val 0.0040\n",
      "Epoch 040/50 | Train-Loss 21.3164 | Test-MSE 329.8042 | Regret 0.0521 | Fair-Val 0.0036\n",
      "Epoch 050/50 | Train-Loss 20.3925 | Test-MSE 323.6020 | Regret 0.0501 | Fair-Val 0.0032\n",
      "Training finished in 286.72s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0502 | σ = 0.0002\n",
      "[                 MSE]  μ = 323.1696 | σ = 0.4324\n",
      "[            FAIRNESS]  μ = 0.0033 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 381.6539 | σ = 94.9354\n",
      "[              G0_MSE]  μ = 308.6552 | σ = 0.5856\n",
      "[              G1_MSE]  μ = 430.4202 | σ = 0.6997\n",
      "[     G0_DECISION_OBJ]  μ = 29.8097 | σ = 0.0583\n",
      "[     G1_DECISION_OBJ]  μ = 217.9158 | σ = 4.0867\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0861 | Test-MSE 362.6319 | Regret 0.0012 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.0830 | Test-MSE 361.4759 | Regret 0.0012 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 0.0757 | Test-MSE 356.8882 | Regret 0.0011 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 0.0691 | Test-MSE 348.6361 | Regret 0.0011 | Fair-Val 0.0041\n",
      "Epoch 040/50 | Train-Loss 0.0652 | Test-MSE 336.9047 | Regret 0.0010 | Fair-Val 0.0034\n",
      "Epoch 050/50 | Train-Loss 0.0646 | Test-MSE 323.1457 | Regret 0.0011 | Fair-Val 0.0028\n",
      "Training finished in 260.78s.\n",
      "Epoch 001/50 | Train-Loss 0.0862 | Test-MSE 362.6518 | Regret 0.0013 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.0818 | Test-MSE 360.7405 | Regret 0.0012 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 0.0745 | Test-MSE 355.1739 | Regret 0.0011 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 0.0678 | Test-MSE 345.5626 | Regret 0.0011 | Fair-Val 0.0039\n",
      "Epoch 040/50 | Train-Loss 0.0645 | Test-MSE 332.9325 | Regret 0.0010 | Fair-Val 0.0032\n",
      "Epoch 050/50 | Train-Loss 0.0650 | Test-MSE 318.0532 | Regret 0.0011 | Fair-Val 0.0026\n",
      "Training finished in 260.93s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0011 | σ = 0.0000\n",
      "[                 MSE]  μ = 320.5994 | σ = 2.5462\n",
      "[            FAIRNESS]  μ = 0.0027 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 260.8556 | σ = 0.0724\n",
      "[              G0_MSE]  μ = 307.6782 | σ = 2.1684\n",
      "[              G1_MSE]  μ = 416.0777 | σ = 5.3379\n",
      "[     G0_DECISION_OBJ]  μ = 19.1765 | σ = 0.0562\n",
      "[     G1_DECISION_OBJ]  μ = 162.5349 | σ = 0.3621\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.6757 | Test-MSE 362.5711 | Regret 0.0277 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 2.3415 | Test-MSE 356.0378 | Regret 0.0244 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 2.0045 | Test-MSE 341.6594 | Regret 0.0213 | Fair-Val 0.0047\n",
      "Epoch 030/50 | Train-Loss 1.8276 | Test-MSE 328.7594 | Regret 0.0197 | Fair-Val 0.0044\n",
      "Epoch 040/50 | Train-Loss 1.6791 | Test-MSE 320.2709 | Regret 0.0184 | Fair-Val 0.0042\n",
      "Epoch 050/50 | Train-Loss 1.5259 | Test-MSE 314.1585 | Regret 0.0170 | Fair-Val 0.0041\n",
      "Training finished in 262.43s.\n",
      "Epoch 001/50 | Train-Loss 2.6853 | Test-MSE 362.6305 | Regret 0.0277 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 2.3969 | Test-MSE 356.9092 | Regret 0.0248 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 2.0253 | Test-MSE 342.3135 | Regret 0.0216 | Fair-Val 0.0045\n",
      "Epoch 030/50 | Train-Loss 1.8478 | Test-MSE 328.2204 | Regret 0.0200 | Fair-Val 0.0043\n",
      "Epoch 040/50 | Train-Loss 1.7034 | Test-MSE 319.3936 | Regret 0.0188 | Fair-Val 0.0041\n",
      "Epoch 050/50 | Train-Loss 1.5477 | Test-MSE 313.8046 | Regret 0.0173 | Fair-Val 0.0040\n",
      "Training finished in 260.92s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0172 | σ = 0.0002\n",
      "[                 MSE]  μ = 313.9816 | σ = 0.1770\n",
      "[            FAIRNESS]  μ = 0.0041 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 261.6765 | σ = 0.7584\n",
      "[              G0_MSE]  μ = 298.1332 | σ = 0.0747\n",
      "[              G1_MSE]  μ = 431.0894 | σ = 0.9327\n",
      "[     G0_DECISION_OBJ]  μ = 18.6739 | σ = 0.0083\n",
      "[     G1_DECISION_OBJ]  μ = 60.4759 | σ = 0.1593\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 77.0980 | Test-MSE 362.5725 | Regret 0.3399 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 66.8953 | Test-MSE 355.8989 | Regret 0.2944 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 56.4184 | Test-MSE 341.2963 | Regret 0.2517 | Fair-Val 0.0047\n",
      "Epoch 030/50 | Train-Loss 50.5031 | Test-MSE 328.4134 | Regret 0.2307 | Fair-Val 0.0045\n",
      "Epoch 040/50 | Train-Loss 45.6749 | Test-MSE 319.5829 | Regret 0.2160 | Fair-Val 0.0042\n",
      "Epoch 050/50 | Train-Loss 41.1950 | Test-MSE 312.7910 | Regret 0.2000 | Fair-Val 0.0041\n",
      "Training finished in 259.58s.\n",
      "Epoch 001/50 | Train-Loss 77.3910 | Test-MSE 362.6297 | Regret 0.3406 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 68.6712 | Test-MSE 356.7938 | Regret 0.3013 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 57.1702 | Test-MSE 342.0330 | Regret 0.2571 | Fair-Val 0.0045\n",
      "Epoch 030/50 | Train-Loss 51.2262 | Test-MSE 327.9689 | Regret 0.2354 | Fair-Val 0.0043\n",
      "Epoch 040/50 | Train-Loss 46.5838 | Test-MSE 318.9694 | Regret 0.2199 | Fair-Val 0.0041\n",
      "Epoch 050/50 | Train-Loss 41.8016 | Test-MSE 312.8323 | Regret 0.2039 | Fair-Val 0.0040\n",
      "Training finished in 260.83s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.2020 | σ = 0.0019\n",
      "[                 MSE]  μ = 312.8117 | σ = 0.0207\n",
      "[            FAIRNESS]  μ = 0.0041 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 260.2085 | σ = 0.6247\n",
      "[              G0_MSE]  μ = 297.0271 | σ = 0.1324\n",
      "[              G1_MSE]  μ = 429.4478 | σ = 0.8047\n",
      "[     G0_DECISION_OBJ]  μ = 19.3481 | σ = 0.0170\n",
      "[     G1_DECISION_OBJ]  μ = 25.8110 | σ = 0.0933\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.5806 | Test-MSE 362.6355 | Regret 0.0654 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 28.6529 | Test-MSE 360.9622 | Regret 0.0642 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 25.4188 | Test-MSE 352.9271 | Regret 0.0581 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 23.5175 | Test-MSE 341.6461 | Regret 0.0552 | Fair-Val 0.0041\n",
      "Epoch 040/50 | Train-Loss 22.9987 | Test-MSE 330.0063 | Regret 0.0552 | Fair-Val 0.0035\n",
      "Epoch 050/50 | Train-Loss 22.6031 | Test-MSE 319.3196 | Regret 0.0556 | Fair-Val 0.0029\n",
      "Training finished in 259.83s.\n",
      "Epoch 001/50 | Train-Loss 29.5971 | Test-MSE 362.6551 | Regret 0.0654 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 28.0454 | Test-MSE 360.1558 | Regret 0.0628 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 25.0484 | Test-MSE 351.6490 | Regret 0.0575 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 23.1864 | Test-MSE 339.7278 | Regret 0.0553 | Fair-Val 0.0039\n",
      "Epoch 040/50 | Train-Loss 22.4376 | Test-MSE 328.1502 | Regret 0.0556 | Fair-Val 0.0033\n",
      "Epoch 050/50 | Train-Loss 22.4326 | Test-MSE 317.6123 | Regret 0.0562 | Fair-Val 0.0027\n",
      "Training finished in 260.49s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0559 | σ = 0.0003\n",
      "[                 MSE]  μ = 318.4660 | σ = 0.8536\n",
      "[            FAIRNESS]  μ = 0.0028 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 260.1610 | σ = 0.3322\n",
      "[              G0_MSE]  μ = 305.2962 | σ = 0.5932\n",
      "[              G1_MSE]  μ = 415.7805 | σ = 2.7780\n",
      "[     G0_DECISION_OBJ]  μ = 29.1375 | σ = 0.1208\n",
      "[     G1_DECISION_OBJ]  μ = 222.6461 | σ = 2.8655\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.1377 | Test-MSE 362.6393 | Regret 0.0012 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.1308 | Test-MSE 361.3857 | Regret 0.0012 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 0.1164 | Test-MSE 356.3894 | Regret 0.0011 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 0.1019 | Test-MSE 347.8393 | Regret 0.0011 | Fair-Val 0.0040\n",
      "Epoch 040/50 | Train-Loss 0.0891 | Test-MSE 336.4241 | Regret 0.0011 | Fair-Val 0.0033\n",
      "Epoch 050/50 | Train-Loss 0.0798 | Test-MSE 323.1142 | Regret 0.0011 | Fair-Val 0.0026\n",
      "Training finished in 260.83s.\n",
      "Epoch 001/50 | Train-Loss 0.1378 | Test-MSE 362.6504 | Regret 0.0013 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.1286 | Test-MSE 360.6392 | Regret 0.0012 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 0.1148 | Test-MSE 355.1926 | Regret 0.0011 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 0.1007 | Test-MSE 346.3723 | Regret 0.0011 | Fair-Val 0.0038\n",
      "Epoch 040/50 | Train-Loss 0.0875 | Test-MSE 334.7559 | Regret 0.0011 | Fair-Val 0.0031\n",
      "Epoch 050/50 | Train-Loss 0.0797 | Test-MSE 321.8490 | Regret 0.0011 | Fair-Val 0.0024\n",
      "Training finished in 408.94s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0011 | σ = 0.0000\n",
      "[                 MSE]  μ = 322.4816 | σ = 0.6326\n",
      "[            FAIRNESS]  μ = 0.0025 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 334.8877 | σ = 74.0534\n",
      "[              G0_MSE]  μ = 309.9310 | σ = 0.3677\n",
      "[              G1_MSE]  μ = 415.2212 | σ = 2.5901\n",
      "[     G0_DECISION_OBJ]  μ = 19.1314 | σ = 0.0211\n",
      "[     G1_DECISION_OBJ]  μ = 162.9914 | σ = 0.4096\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.7273 | Test-MSE 362.5713 | Regret 0.0277 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 2.3907 | Test-MSE 356.0499 | Regret 0.0244 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 2.0489 | Test-MSE 341.6688 | Regret 0.0213 | Fair-Val 0.0047\n",
      "Epoch 030/50 | Train-Loss 1.8671 | Test-MSE 328.6107 | Regret 0.0197 | Fair-Val 0.0044\n",
      "Epoch 040/50 | Train-Loss 1.7152 | Test-MSE 319.8196 | Regret 0.0185 | Fair-Val 0.0042\n",
      "Epoch 050/50 | Train-Loss 1.5627 | Test-MSE 313.3242 | Regret 0.0170 | Fair-Val 0.0040\n",
      "Training finished in 500.36s.\n",
      "Epoch 001/50 | Train-Loss 2.7369 | Test-MSE 362.6306 | Regret 0.0277 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 2.4456 | Test-MSE 356.9247 | Regret 0.0248 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 2.0681 | Test-MSE 342.3187 | Regret 0.0216 | Fair-Val 0.0045\n",
      "Epoch 030/50 | Train-Loss 1.8863 | Test-MSE 328.0780 | Regret 0.0200 | Fair-Val 0.0043\n",
      "Epoch 040/50 | Train-Loss 1.7378 | Test-MSE 318.9419 | Regret 0.0188 | Fair-Val 0.0041\n",
      "Epoch 050/50 | Train-Loss 1.5821 | Test-MSE 312.9915 | Regret 0.0174 | Fair-Val 0.0040\n",
      "Training finished in 466.79s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0172 | σ = 0.0002\n",
      "[                 MSE]  μ = 313.1578 | σ = 0.1663\n",
      "[            FAIRNESS]  μ = 0.0040 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 483.5744 | σ = 16.7827\n",
      "[              G0_MSE]  μ = 297.5373 | σ = 0.0784\n",
      "[              G1_MSE]  μ = 428.5819 | σ = 0.8161\n",
      "[     G0_DECISION_OBJ]  μ = 18.6751 | σ = 0.0081\n",
      "[     G1_DECISION_OBJ]  μ = 60.4228 | σ = 0.1619\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 77.1496 | Test-MSE 362.5725 | Regret 0.3399 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 66.9444 | Test-MSE 355.8990 | Regret 0.2944 | Fair-Val 0.0049\n",
      "Epoch 020/50 | Train-Loss 56.4633 | Test-MSE 341.2950 | Regret 0.2517 | Fair-Val 0.0047\n",
      "Epoch 030/50 | Train-Loss 50.5439 | Test-MSE 328.4061 | Regret 0.2307 | Fair-Val 0.0045\n",
      "Epoch 040/50 | Train-Loss 45.7133 | Test-MSE 319.5638 | Regret 0.2160 | Fair-Val 0.0042\n",
      "Epoch 050/50 | Train-Loss 41.2347 | Test-MSE 312.7614 | Regret 0.2001 | Fair-Val 0.0041\n",
      "Training finished in 319.28s.\n",
      "Epoch 001/50 | Train-Loss 77.4425 | Test-MSE 362.6297 | Regret 0.3406 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 68.7196 | Test-MSE 356.7939 | Regret 0.3013 | Fair-Val 0.0048\n",
      "Epoch 020/50 | Train-Loss 57.2132 | Test-MSE 342.0318 | Regret 0.2571 | Fair-Val 0.0045\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 0.8, 1.5, 2]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form', 'finite-diff']  # New parameter\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  GRID-SEARCH HARNESS  (only the inner loop changed)\n",
    "# ---------------------------------------------------------------------\n",
    "results_list = []\n",
    "final_model_cf = None\n",
    "final_model_fd = None\n",
    "\n",
    "for group in group_settings:\n",
    "    fairness_types = ['mad', 'atkinson']\n",
    "    for grad_method in grad_methods:\n",
    "        for fairness in fairness_types:\n",
    "            if fairness == 'mad':\n",
    "                fairness_lambdas = [0, 1, 0.01]\n",
    "            elif fairness == 'atkinson':\n",
    "                fairness_lambdas = [0, 1, 10]\n",
    "            else:\n",
    "                fairness_lambdas = [0]\n",
    "            for lam in fairness_lambdas:\n",
    "                if lam == 0 and fairness != fairness_types[0]:\n",
    "                    continue  # skip unattainable combos\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method,\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = dict(\n",
    "                        X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                        cost_train=cost_train, gainF_train=gainF_train,\n",
    "                        X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                        cost_test=cost_test, gainF_test=gainF_test,\n",
    "                        model_class=FairRiskPredictor,\n",
    "                        input_dim=feats_train.shape[1],\n",
    "                        alpha=alpha, Q=Q,\n",
    "                        lambda_fair=lam, fairness_type=fairness,\n",
    "                        group=group, grad_method=grad_method,\n",
    "                        num_epochs=50, lr=0.001\n",
    "                    )\n",
    "\n",
    "                    avg_results, final_model = train_many_trials_regret(\n",
    "                        n_trials=2, **train_args) #type: ignore[call-arg]\n",
    "\n",
    "                    # ---------------- build DataFrame row ------------\n",
    "                    row = run_params.copy()\n",
    "                    row.update(avg_results)          # every metric goes in\n",
    "                    results_list.append(row)\n",
    "                    # ---------------- save the final model ------------\n",
    "                    model_name = f\"predmodel_{fairness}_{lam}_{grad_method}_{group}_{alpha}_NN.pth\"\n",
    "                    model_path = f\"E:/myREPO/Fairness-Decision-Focused-Loss/Organized-FDFL/src/models/FDFL/{model_name}\"\n",
    "                    torch.save(final_model.state_dict(), model_path) # type: ignore[arg-type]\n",
    "\n",
    "# ---------------- DataFrame & LaTeX dump -----------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Put the hyper-parameters first; everything else follows automatically\n",
    "hp_cols = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness']\n",
    "other_cols = sorted([c for c in results_df.columns if c not in hp_cols])\n",
    "results_df = results_df[hp_cols + other_cols]\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Averaged Experimental Results Across Different Parameters.\",\n",
    "    label=\"tab:avg_exp_results_expanded\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None,\n",
    "                       'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n--- LaTeX Table Output ---\")\n",
    "print(latex_table)\n",
    "\n",
    "# Run NN first, lr = 1e-3\n",
    "results_df.to_csv(\"closed-form-res-NN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b7a9c",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440c88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3af208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear Reg with lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe681c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85beff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3415.9688 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 507.7022\n",
      "Epoch 010/50 | Train-Loss 2802.0605 | Test-MSE 374.4723 | Regret 0.1120 | Fair-Val 511.0275\n",
      "Epoch 020/50 | Train-Loss 4189.1445 | Test-MSE 382.4344 | Regret 0.1789 | Fair-Val 519.9187\n",
      "Epoch 030/50 | Train-Loss 6786.4160 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 523.4739\n",
      "Epoch 040/50 | Train-Loss 9089.9043 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 524.5817\n",
      "Epoch 050/50 | Train-Loss 7128.9688 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 524.8821\n",
      "Training finished in 2.71s.\n",
      "Epoch 001/50 | Train-Loss 3512.8027 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 508.3664\n",
      "Epoch 010/50 | Train-Loss 2822.3457 | Test-MSE 374.7670 | Regret 0.1102 | Fair-Val 511.2897\n",
      "Epoch 020/50 | Train-Loss 4341.7910 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 519.7877\n",
      "Epoch 030/50 | Train-Loss 6752.1914 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 523.4116\n",
      "Epoch 040/50 | Train-Loss 9232.6562 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 524.5447\n",
      "Epoch 050/50 | Train-Loss 7382.6035 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 524.8646\n",
      "Training finished in 2.68s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 524.8734 | σ = 0.0088\n",
      "[       TRAINING_TIME]  μ = 2.6946 | σ = 0.0192\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0133\n",
      "[     G0_DECISION_OBJ]  μ = 27.0203 | σ = 4.7743\n",
      "[     G1_DECISION_OBJ]  μ = 379.4148 | σ = 193.2612\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 143.6367 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 507.6813\n",
      "Epoch 010/50 | Train-Loss 110.6230 | Test-MSE 372.6393 | Regret 0.0052 | Fair-Val 508.6251\n",
      "Epoch 020/50 | Train-Loss 331.1816 | Test-MSE 380.4272 | Regret 0.0177 | Fair-Val 517.4680\n",
      "Epoch 030/50 | Train-Loss 607.3633 | Test-MSE 384.6749 | Regret 0.0288 | Fair-Val 522.4727\n",
      "Epoch 040/50 | Train-Loss 506.2715 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 524.0574\n",
      "Epoch 050/50 | Train-Loss 315.1445 | Test-MSE 386.4113 | Regret 0.0149 | Fair-Val 524.6382\n",
      "Training finished in 2.74s.\n",
      "Epoch 001/50 | Train-Loss 149.1523 | Test-MSE 370.2304 | Regret 0.0060 | Fair-Val 508.3608\n",
      "Epoch 010/50 | Train-Loss 109.8672 | Test-MSE 373.4153 | Regret 0.0050 | Fair-Val 509.4874\n",
      "Epoch 020/50 | Train-Loss 286.6328 | Test-MSE 380.8755 | Regret 0.0153 | Fair-Val 517.9122\n",
      "Epoch 030/50 | Train-Loss 593.2305 | Test-MSE 384.7013 | Regret 0.0285 | Fair-Val 522.4958\n",
      "Epoch 040/50 | Train-Loss 487.1230 | Test-MSE 386.0291 | Regret 0.0232 | Fair-Val 524.1398\n",
      "Epoch 050/50 | Train-Loss 295.9727 | Test-MSE 386.4234 | Regret 0.0142 | Fair-Val 524.6615\n",
      "Training finished in 2.71s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0145 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4174 | σ = 0.0060\n",
      "[            FAIRNESS]  μ = 524.6499 | σ = 0.0116\n",
      "[       TRAINING_TIME]  μ = 2.7235 | σ = 0.0115\n",
      "[              G0_MSE]  μ = 364.9638 | σ = 0.0024\n",
      "[              G1_MSE]  μ = 544.9433 | σ = 0.0320\n",
      "[     G0_DECISION_OBJ]  μ = 27.6426 | σ = 0.0480\n",
      "[     G1_DECISION_OBJ]  μ = 51.1901 | σ = 1.5952\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 78.9233 | Test-MSE 369.5992 | Regret 0.0590 | Fair-Val 508.4144\n",
      "Epoch 010/50 | Train-Loss 115.6484 | Test-MSE 357.3958 | Regret 0.0845 | Fair-Val 497.2656\n",
      "Epoch 020/50 | Train-Loss 130.9624 | Test-MSE 333.0449 | Regret 0.0959 | Fair-Val 465.8534\n",
      "Epoch 030/50 | Train-Loss 136.2502 | Test-MSE 311.3412 | Regret 0.0988 | Fair-Val 419.3470\n",
      "Epoch 040/50 | Train-Loss 135.2443 | Test-MSE 304.6464 | Regret 0.0983 | Fair-Val 361.2239\n",
      "Epoch 050/50 | Train-Loss 134.1367 | Test-MSE 324.1054 | Regret 0.0978 | Fair-Val 307.8857\n",
      "Training finished in 2.59s.\n",
      "Epoch 001/50 | Train-Loss 82.9224 | Test-MSE 369.9268 | Regret 0.0602 | Fair-Val 508.8384\n",
      "Epoch 010/50 | Train-Loss 109.3344 | Test-MSE 357.8008 | Regret 0.0786 | Fair-Val 497.8892\n",
      "Epoch 020/50 | Train-Loss 122.1057 | Test-MSE 331.0451 | Regret 0.0892 | Fair-Val 466.2614\n",
      "Epoch 030/50 | Train-Loss 129.8397 | Test-MSE 304.5727 | Regret 0.0941 | Fair-Val 418.4245\n",
      "Epoch 040/50 | Train-Loss 132.2141 | Test-MSE 292.1766 | Regret 0.0952 | Fair-Val 360.0242\n",
      "Epoch 050/50 | Train-Loss 131.9357 | Test-MSE 303.5035 | Regret 0.0951 | Fair-Val 297.4414\n",
      "Training finished in 2.70s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0964 | σ = 0.0013\n",
      "[                 MSE]  μ = 313.8045 | σ = 10.3010\n",
      "[            FAIRNESS]  μ = 302.6636 | σ = 5.2222\n",
      "[       TRAINING_TIME]  μ = 2.6451 | σ = 0.0541\n",
      "[              G0_MSE]  μ = 302.4423 | σ = 10.3903\n",
      "[              G1_MSE]  μ = 397.7625 | σ = 9.6407\n",
      "[     G0_DECISION_OBJ]  μ = 21.0484 | σ = 0.0193\n",
      "[     G1_DECISION_OBJ]  μ = 33.8340 | σ = 0.2519\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 368.9831 | Regret 0.3398 | Fair-Val 507.6001\n",
      "Epoch 010/50 | Train-Loss 87.1204 | Test-MSE 352.9514 | Regret 0.4233 | Fair-Val 491.5408\n",
      "Epoch 020/50 | Train-Loss 101.3864 | Test-MSE 323.6245 | Regret 0.4992 | Fair-Val 456.4656\n",
      "Epoch 030/50 | Train-Loss 103.7902 | Test-MSE 291.9113 | Regret 0.4985 | Fair-Val 405.1118\n",
      "Epoch 040/50 | Train-Loss 100.7530 | Test-MSE 268.1656 | Regret 0.4841 | Fair-Val 339.9048\n",
      "Epoch 050/50 | Train-Loss 99.3065 | Test-MSE 264.6647 | Regret 0.4766 | Fair-Val 267.5626\n",
      "Training finished in 2.64s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 369.2369 | Regret 0.3472 | Fair-Val 507.9189\n",
      "Epoch 010/50 | Train-Loss 82.6439 | Test-MSE 352.2357 | Regret 0.3927 | Fair-Val 490.3625\n",
      "Epoch 020/50 | Train-Loss 94.3698 | Test-MSE 320.1707 | Regret 0.4697 | Fair-Val 452.9274\n",
      "Epoch 030/50 | Train-Loss 102.0180 | Test-MSE 286.2388 | Regret 0.4962 | Fair-Val 399.3138\n",
      "Epoch 040/50 | Train-Loss 103.0727 | Test-MSE 262.2052 | Regret 0.4905 | Fair-Val 333.0533\n",
      "Epoch 050/50 | Train-Loss 100.8569 | Test-MSE 258.7193 | Regret 0.4781 | Fair-Val 262.3913\n",
      "Training finished in 2.69s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4773 | σ = 0.0007\n",
      "[                 MSE]  μ = 261.6920 | σ = 2.9727\n",
      "[            FAIRNESS]  μ = 264.9769 | σ = 2.5857\n",
      "[       TRAINING_TIME]  μ = 2.6623 | σ = 0.0267\n",
      "[              G0_MSE]  μ = 250.8123 | σ = 2.9817\n",
      "[              G1_MSE]  μ = 342.0845 | σ = 2.9063\n",
      "[     G0_DECISION_OBJ]  μ = 18.3818 | σ = 0.0082\n",
      "[     G1_DECISION_OBJ]  μ = 33.4390 | σ = 1.3831\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3953.8799 | Test-MSE 369.6822 | Regret 0.1205 | Fair-Val 507.7017\n",
      "Epoch 010/50 | Train-Loss 3341.0195 | Test-MSE 374.4593 | Regret 0.1120 | Fair-Val 511.0104\n",
      "Epoch 020/50 | Train-Loss 4739.0889 | Test-MSE 382.4295 | Regret 0.1789 | Fair-Val 519.9126\n",
      "Epoch 030/50 | Train-Loss 7344.4829 | Test-MSE 385.5072 | Regret 0.2948 | Fair-Val 523.4715\n",
      "Epoch 040/50 | Train-Loss 9647.7998 | Test-MSE 386.3705 | Regret 0.3913 | Fair-Val 524.5809\n",
      "Epoch 050/50 | Train-Loss 7680.3735 | Test-MSE 386.5898 | Regret 0.3329 | Fair-Val 524.8821\n",
      "Training finished in 2.70s.\n",
      "Epoch 001/50 | Train-Loss 4051.9204 | Test-MSE 370.2293 | Regret 0.1241 | Fair-Val 508.3653\n",
      "Epoch 010/50 | Train-Loss 3362.4014 | Test-MSE 374.7551 | Regret 0.1102 | Fair-Val 511.2748\n",
      "Epoch 020/50 | Train-Loss 4892.9854 | Test-MSE 382.3448 | Regret 0.1821 | Fair-Val 519.7814\n",
      "Epoch 030/50 | Train-Loss 7302.3428 | Test-MSE 385.4475 | Regret 0.2881 | Fair-Val 523.4105\n",
      "Epoch 040/50 | Train-Loss 9775.2217 | Test-MSE 386.3373 | Regret 0.3991 | Fair-Val 524.5442\n",
      "Epoch 050/50 | Train-Loss 7950.4463 | Test-MSE 386.5736 | Regret 0.4108 | Fair-Val 524.8646\n",
      "Training finished in 2.62s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3718 | σ = 0.0390\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 524.8734 | σ = 0.0087\n",
      "[       TRAINING_TIME]  μ = 2.6584 | σ = 0.0433\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0074\n",
      "[              G1_MSE]  μ = 545.3247 | σ = 0.0124\n",
      "[     G0_DECISION_OBJ]  μ = 27.0095 | σ = 4.6895\n",
      "[     G1_DECISION_OBJ]  μ = 377.6530 | σ = 190.0825\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 681.5478 | Test-MSE 369.6608 | Regret 0.0058 | Fair-Val 507.6636\n",
      "Epoch 010/50 | Train-Loss 646.9650 | Test-MSE 372.3972 | Regret 0.0053 | Fair-Val 508.3057\n",
      "Epoch 020/50 | Train-Loss 880.3593 | Test-MSE 380.2114 | Regret 0.0178 | Fair-Val 517.1793\n",
      "Epoch 030/50 | Train-Loss 1162.6594 | Test-MSE 384.6342 | Regret 0.0289 | Fair-Val 522.4189\n",
      "Epoch 040/50 | Train-Loss 1061.6234 | Test-MSE 385.9742 | Regret 0.0236 | Fair-Val 524.0582\n",
      "Epoch 050/50 | Train-Loss 870.0930 | Test-MSE 386.4202 | Regret 0.0148 | Fair-Val 524.6514\n",
      "Training finished in 2.74s.\n",
      "Epoch 001/50 | Train-Loss 688.2701 | Test-MSE 370.2173 | Regret 0.0060 | Fair-Val 508.3449\n",
      "Epoch 010/50 | Train-Loss 647.4316 | Test-MSE 373.1810 | Regret 0.0050 | Fair-Val 509.1758\n",
      "Epoch 020/50 | Train-Loss 832.0442 | Test-MSE 380.6599 | Regret 0.0151 | Fair-Val 517.6177\n",
      "Epoch 030/50 | Train-Loss 1141.6689 | Test-MSE 384.6658 | Regret 0.0284 | Fair-Val 522.4476\n",
      "Epoch 040/50 | Train-Loss 1037.2656 | Test-MSE 386.0438 | Regret 0.0229 | Fair-Val 524.1604\n",
      "Epoch 050/50 | Train-Loss 847.7223 | Test-MSE 386.4308 | Regret 0.0139 | Fair-Val 524.6727\n",
      "Training finished in 2.60s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0144 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4255 | σ = 0.0053\n",
      "[            FAIRNESS]  μ = 524.6620 | σ = 0.0106\n",
      "[       TRAINING_TIME]  μ = 2.6705 | σ = 0.0723\n",
      "[              G0_MSE]  μ = 364.9718 | σ = 0.0040\n",
      "[              G1_MSE]  μ = 544.9522 | σ = 0.0144\n",
      "[     G0_DECISION_OBJ]  μ = 27.6496 | σ = 0.0384\n",
      "[     G1_DECISION_OBJ]  μ = 51.2027 | σ = 1.6496\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 616.8344 | Test-MSE 369.4432 | Regret 0.0583 | Fair-Val 508.2133\n",
      "Epoch 010/50 | Train-Loss 633.2652 | Test-MSE 356.1486 | Regret 0.0795 | Fair-Val 495.6698\n",
      "Epoch 020/50 | Train-Loss 603.9327 | Test-MSE 327.3419 | Regret 0.0877 | Fair-Val 458.4466\n",
      "Epoch 030/50 | Train-Loss 530.8755 | Test-MSE 293.9179 | Regret 0.0853 | Fair-Val 394.7360\n",
      "Epoch 040/50 | Train-Loss 418.4904 | Test-MSE 268.4575 | Regret 0.0790 | Fair-Val 302.5624\n",
      "Epoch 050/50 | Train-Loss 331.0372 | Test-MSE 269.5360 | Regret 0.0726 | Fair-Val 237.2384\n",
      "Training finished in 2.68s.\n",
      "Epoch 001/50 | Train-Loss 622.0402 | Test-MSE 369.7164 | Regret 0.0593 | Fair-Val 508.5608\n",
      "Epoch 010/50 | Train-Loss 624.4081 | Test-MSE 355.4624 | Regret 0.0719 | Fair-Val 494.7910\n",
      "Epoch 020/50 | Train-Loss 588.5978 | Test-MSE 322.1503 | Regret 0.0793 | Fair-Val 454.4706\n",
      "Epoch 030/50 | Train-Loss 516.7760 | Test-MSE 282.4205 | Regret 0.0795 | Fair-Val 387.1905\n",
      "Epoch 040/50 | Train-Loss 409.2392 | Test-MSE 252.4633 | Regret 0.0756 | Fair-Val 296.3835\n",
      "Epoch 050/50 | Train-Loss 317.3625 | Test-MSE 255.3889 | Regret 0.0712 | Fair-Val 219.0571\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0719 | σ = 0.0007\n",
      "[                 MSE]  μ = 262.4625 | σ = 7.0735\n",
      "[            FAIRNESS]  μ = 228.1478 | σ = 9.0906\n",
      "[       TRAINING_TIME]  μ = 2.6297 | σ = 0.0500\n",
      "[              G0_MSE]  μ = 255.9634 | σ = 7.6713\n",
      "[              G1_MSE]  μ = 310.4863 | σ = 2.6567\n",
      "[     G0_DECISION_OBJ]  μ = 21.1076 | σ = 0.0210\n",
      "[     G1_DECISION_OBJ]  μ = 32.4676 | σ = 0.1848\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 612.5168 | Test-MSE 368.4211 | Regret 0.3254 | Fair-Val 506.7992\n",
      "Epoch 010/50 | Train-Loss 577.2137 | Test-MSE 345.8217 | Regret 0.3335 | Fair-Val 481.2682\n",
      "Epoch 020/50 | Train-Loss 517.9517 | Test-MSE 299.8217 | Regret 0.3698 | Fair-Val 423.6961\n",
      "Epoch 030/50 | Train-Loss 415.9778 | Test-MSE 240.7453 | Regret 0.3515 | Fair-Val 334.0703\n",
      "Epoch 040/50 | Train-Loss 295.8572 | Test-MSE 192.9868 | Regret 0.3337 | Fair-Val 226.6295\n",
      "Epoch 050/50 | Train-Loss 218.8607 | Test-MSE 188.8353 | Regret 0.3288 | Fair-Val 155.0357\n",
      "Training finished in 2.66s.\n",
      "Epoch 001/50 | Train-Loss 618.1407 | Test-MSE 368.7325 | Regret 0.3342 | Fair-Val 507.1826\n",
      "Epoch 010/50 | Train-Loss 573.1943 | Test-MSE 344.2766 | Regret 0.3166 | Fair-Val 478.9482\n",
      "Epoch 020/50 | Train-Loss 503.0676 | Test-MSE 292.8324 | Regret 0.3491 | Fair-Val 414.9620\n",
      "Epoch 030/50 | Train-Loss 402.5359 | Test-MSE 230.6384 | Regret 0.3449 | Fair-Val 321.1063\n",
      "Epoch 040/50 | Train-Loss 293.7167 | Test-MSE 188.7059 | Regret 0.3299 | Fair-Val 221.5115\n",
      "Epoch 050/50 | Train-Loss 227.1893 | Test-MSE 192.2306 | Regret 0.3248 | Fair-Val 159.5770\n",
      "Training finished in 2.56s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3268 | σ = 0.0020\n",
      "[                 MSE]  μ = 190.5330 | σ = 1.6976\n",
      "[            FAIRNESS]  μ = 157.3063 | σ = 2.2706\n",
      "[       TRAINING_TIME]  μ = 2.6112 | σ = 0.0531\n",
      "[              G0_MSE]  μ = 184.9535 | σ = 0.9648\n",
      "[              G1_MSE]  μ = 231.7614 | σ = 7.1128\n",
      "[     G0_DECISION_OBJ]  μ = 18.3872 | σ = 0.0628\n",
      "[     G1_DECISION_OBJ]  μ = 32.4145 | σ = 2.3101\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3421.3479 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 507.7022\n",
      "Epoch 010/50 | Train-Loss 2807.4517 | Test-MSE 374.4722 | Regret 0.1120 | Fair-Val 511.0273\n",
      "Epoch 020/50 | Train-Loss 4194.6421 | Test-MSE 382.4343 | Regret 0.1789 | Fair-Val 519.9186\n",
      "Epoch 030/50 | Train-Loss 6791.9502 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 523.4739\n",
      "Epoch 040/50 | Train-Loss 9095.4219 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 524.5816\n",
      "Epoch 050/50 | Train-Loss 7134.8306 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 524.8821\n",
      "Training finished in 2.63s.\n",
      "Epoch 001/50 | Train-Loss 3518.1938 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 508.3664\n",
      "Epoch 010/50 | Train-Loss 2827.7444 | Test-MSE 374.7669 | Regret 0.1102 | Fair-Val 511.2896\n",
      "Epoch 020/50 | Train-Loss 4347.2915 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 519.7876\n",
      "Epoch 030/50 | Train-Loss 6757.7314 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 523.4116\n",
      "Epoch 040/50 | Train-Loss 9238.2100 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 524.5447\n",
      "Epoch 050/50 | Train-Loss 7388.1274 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 524.8646\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 524.8734 | σ = 0.0088\n",
      "[       TRAINING_TIME]  μ = 2.6054 | σ = 0.0215\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0133\n",
      "[     G0_DECISION_OBJ]  μ = 27.0224 | σ = 4.7754\n",
      "[     G1_DECISION_OBJ]  μ = 379.4098 | σ = 193.2619\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 149.0158 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 507.6813\n",
      "Epoch 010/50 | Train-Loss 115.9895 | Test-MSE 372.6374 | Regret 0.0052 | Fair-Val 508.6226\n",
      "Epoch 020/50 | Train-Loss 336.6455 | Test-MSE 380.4258 | Regret 0.0177 | Fair-Val 517.4661\n",
      "Epoch 030/50 | Train-Loss 612.8864 | Test-MSE 384.6745 | Regret 0.0288 | Fair-Val 522.4721\n",
      "Epoch 040/50 | Train-Loss 511.7760 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 524.0573\n",
      "Epoch 050/50 | Train-Loss 320.5316 | Test-MSE 386.4115 | Regret 0.0149 | Fair-Val 524.6385\n",
      "Training finished in 2.52s.\n",
      "Epoch 001/50 | Train-Loss 154.5435 | Test-MSE 370.2300 | Regret 0.0060 | Fair-Val 508.3604\n",
      "Epoch 010/50 | Train-Loss 115.2515 | Test-MSE 373.4124 | Regret 0.0050 | Fair-Val 509.4836\n",
      "Epoch 020/50 | Train-Loss 292.1691 | Test-MSE 380.8723 | Regret 0.0153 | Fair-Val 517.9079\n",
      "Epoch 030/50 | Train-Loss 598.8092 | Test-MSE 384.7013 | Regret 0.0285 | Fair-Val 522.4960\n",
      "Epoch 040/50 | Train-Loss 492.7216 | Test-MSE 386.0286 | Regret 0.0232 | Fair-Val 524.1392\n",
      "Epoch 050/50 | Train-Loss 301.6955 | Test-MSE 386.4233 | Regret 0.0142 | Fair-Val 524.6615\n",
      "Training finished in 2.59s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0146 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4174 | σ = 0.0059\n",
      "[            FAIRNESS]  μ = 524.6500 | σ = 0.0115\n",
      "[       TRAINING_TIME]  μ = 2.5563 | σ = 0.0344\n",
      "[              G0_MSE]  μ = 364.9640 | σ = 0.0024\n",
      "[              G1_MSE]  μ = 544.9423 | σ = 0.0318\n",
      "[     G0_DECISION_OBJ]  μ = 27.6445 | σ = 0.0449\n",
      "[     G1_DECISION_OBJ]  μ = 51.1958 | σ = 1.5964\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 84.3025 | Test-MSE 369.5940 | Regret 0.0590 | Fair-Val 508.4081\n",
      "Epoch 010/50 | Train-Loss 120.8504 | Test-MSE 357.3875 | Regret 0.0844 | Fair-Val 497.2563\n",
      "Epoch 020/50 | Train-Loss 135.7873 | Test-MSE 333.0048 | Regret 0.0958 | Fair-Val 465.8018\n",
      "Epoch 030/50 | Train-Loss 140.5097 | Test-MSE 311.2223 | Regret 0.0986 | Fair-Val 419.1635\n",
      "Epoch 040/50 | Train-Loss 138.7142 | Test-MSE 304.3888 | Regret 0.0981 | Fair-Val 360.7170\n",
      "Epoch 050/50 | Train-Loss 136.8212 | Test-MSE 323.7140 | Regret 0.0974 | Fair-Val 307.2081\n",
      "Training finished in 2.50s.\n",
      "Epoch 001/50 | Train-Loss 88.3135 | Test-MSE 369.9257 | Regret 0.0602 | Fair-Val 508.8370\n",
      "Epoch 010/50 | Train-Loss 114.5675 | Test-MSE 357.7902 | Regret 0.0786 | Fair-Val 497.8749\n",
      "Epoch 020/50 | Train-Loss 126.9395 | Test-MSE 330.9873 | Regret 0.0891 | Fair-Val 466.1855\n",
      "Epoch 030/50 | Train-Loss 134.0638 | Test-MSE 304.4004 | Regret 0.0940 | Fair-Val 418.1803\n",
      "Epoch 040/50 | Train-Loss 135.6748 | Test-MSE 291.8304 | Regret 0.0950 | Fair-Val 359.4510\n",
      "Epoch 050/50 | Train-Loss 134.5580 | Test-MSE 302.9771 | Regret 0.0947 | Fair-Val 296.3731\n",
      "Training finished in 2.61s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0961 | σ = 0.0013\n",
      "[                 MSE]  μ = 313.3455 | σ = 10.3685\n",
      "[            FAIRNESS]  μ = 301.7906 | σ = 5.4175\n",
      "[       TRAINING_TIME]  μ = 2.5544 | σ = 0.0510\n",
      "[              G0_MSE]  μ = 302.0441 | σ = 10.4606\n",
      "[              G1_MSE]  μ = 396.8546 | σ = 9.6879\n",
      "[     G0_DECISION_OBJ]  μ = 21.0521 | σ = 0.0180\n",
      "[     G1_DECISION_OBJ]  μ = 33.8036 | σ = 0.2537\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0.01, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 79.9849 | Test-MSE 368.9787 | Regret 0.3397 | Fair-Val 507.5945\n",
      "Epoch 010/50 | Train-Loss 92.1157 | Test-MSE 352.8892 | Regret 0.4223 | Fair-Val 491.4502\n",
      "Epoch 020/50 | Train-Loss 105.8703 | Test-MSE 323.4166 | Regret 0.4976 | Fair-Val 456.1707\n",
      "Epoch 030/50 | Train-Loss 107.5470 | Test-MSE 291.3806 | Regret 0.4962 | Fair-Val 404.3451\n",
      "Epoch 040/50 | Train-Loss 103.5319 | Test-MSE 267.0563 | Regret 0.4806 | Fair-Val 338.1939\n",
      "Epoch 050/50 | Train-Loss 101.0588 | Test-MSE 262.9240 | Regret 0.4720 | Fair-Val 264.5333\n",
      "Training finished in 2.54s.\n",
      "Epoch 001/50 | Train-Loss 84.4141 | Test-MSE 369.2332 | Regret 0.3471 | Fair-Val 507.9138\n",
      "Epoch 010/50 | Train-Loss 87.6671 | Test-MSE 352.1667 | Regret 0.3918 | Fair-Val 490.2646\n",
      "Epoch 020/50 | Train-Loss 98.8326 | Test-MSE 319.9005 | Regret 0.4681 | Fair-Val 452.5547\n",
      "Epoch 030/50 | Train-Loss 105.6939 | Test-MSE 285.5776 | Regret 0.4935 | Fair-Val 398.3649\n",
      "Epoch 040/50 | Train-Loss 105.7453 | Test-MSE 260.9624 | Regret 0.4865 | Fair-Val 331.1371\n",
      "Epoch 050/50 | Train-Loss 102.5003 | Test-MSE 256.9506 | Regret 0.4730 | Fair-Val 259.2515\n",
      "Training finished in 2.59s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4725 | σ = 0.0005\n",
      "[                 MSE]  μ = 259.9373 | σ = 2.9867\n",
      "[            FAIRNESS]  μ = 261.8924 | σ = 2.6409\n",
      "[       TRAINING_TIME]  μ = 2.5685 | σ = 0.0244\n",
      "[              G0_MSE]  μ = 249.1977 | σ = 2.9986\n",
      "[              G1_MSE]  μ = 339.2952 | σ = 2.8988\n",
      "[     G0_DECISION_OBJ]  μ = 18.3859 | σ = 0.0086\n",
      "[     G1_DECISION_OBJ]  μ = 33.3712 | σ = 1.3689\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3416.5906 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2802.6633 | Test-MSE 374.4723 | Regret 0.1120 | Fair-Val 0.6054\n",
      "Epoch 020/50 | Train-Loss 4189.7432 | Test-MSE 382.4344 | Regret 0.1789 | Fair-Val 0.5995\n",
      "Epoch 030/50 | Train-Loss 6787.0146 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 0.5979\n",
      "Epoch 040/50 | Train-Loss 9090.5029 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7129.5654 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 0.5977\n",
      "Training finished in 2.54s.\n",
      "Epoch 001/50 | Train-Loss 3513.4246 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2822.9480 | Test-MSE 374.7670 | Regret 0.1102 | Fair-Val 0.6049\n",
      "Epoch 020/50 | Train-Loss 4342.3892 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 0.5993\n",
      "Epoch 030/50 | Train-Loss 6752.7881 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 0.5978\n",
      "Epoch 040/50 | Train-Loss 9233.2529 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7383.2002 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 0.5977\n",
      "Training finished in 2.70s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.6186 | σ = 0.0799\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0133\n",
      "[     G0_DECISION_OBJ]  μ = 27.0203 | σ = 4.7743\n",
      "[     G1_DECISION_OBJ]  μ = 379.4149 | σ = 193.2611\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 144.2585 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 111.2293 | Test-MSE 372.6392 | Regret 0.0052 | Fair-Val 0.6051\n",
      "Epoch 020/50 | Train-Loss 331.7802 | Test-MSE 380.4272 | Regret 0.0177 | Fair-Val 0.5997\n",
      "Epoch 030/50 | Train-Loss 607.9603 | Test-MSE 384.6749 | Regret 0.0288 | Fair-Val 0.5981\n",
      "Epoch 040/50 | Train-Loss 506.8681 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 315.7430 | Test-MSE 386.4113 | Regret 0.0149 | Fair-Val 0.5977\n",
      "Training finished in 2.79s.\n",
      "Epoch 001/50 | Train-Loss 149.7742 | Test-MSE 370.2304 | Regret 0.0060 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 110.4711 | Test-MSE 373.4153 | Regret 0.0050 | Fair-Val 0.6046\n",
      "Epoch 020/50 | Train-Loss 287.2308 | Test-MSE 380.8755 | Regret 0.0153 | Fair-Val 0.5992\n",
      "Epoch 030/50 | Train-Loss 593.8292 | Test-MSE 384.7013 | Regret 0.0285 | Fair-Val 0.5980\n",
      "Epoch 040/50 | Train-Loss 487.7157 | Test-MSE 386.0291 | Regret 0.0232 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 296.5731 | Test-MSE 386.4233 | Regret 0.0142 | Fair-Val 0.5977\n",
      "Training finished in 2.63s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0145 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4173 | σ = 0.0060\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.7068 | σ = 0.0786\n",
      "[              G0_MSE]  μ = 364.9638 | σ = 0.0024\n",
      "[              G1_MSE]  μ = 544.9434 | σ = 0.0322\n",
      "[     G0_DECISION_OBJ]  μ = 27.6425 | σ = 0.0481\n",
      "[     G1_DECISION_OBJ]  μ = 51.1904 | σ = 1.5955\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 79.5451 | Test-MSE 369.5992 | Regret 0.0590 | Fair-Val 0.6266\n",
      "Epoch 010/50 | Train-Loss 116.3017 | Test-MSE 357.3955 | Regret 0.0845 | Fair-Val 0.6597\n",
      "Epoch 020/50 | Train-Loss 131.6116 | Test-MSE 333.0457 | Regret 0.0959 | Fair-Val 0.6437\n",
      "Epoch 030/50 | Train-Loss 136.8471 | Test-MSE 311.3495 | Regret 0.0988 | Fair-Val 0.5779\n",
      "Epoch 040/50 | Train-Loss 135.7517 | Test-MSE 304.6819 | Regret 0.0983 | Fair-Val 0.4760\n",
      "Epoch 050/50 | Train-Loss 134.5319 | Test-MSE 324.1862 | Regret 0.0978 | Fair-Val 0.3698\n",
      "Training finished in 2.59s.\n",
      "Epoch 001/50 | Train-Loss 83.5442 | Test-MSE 369.9268 | Regret 0.0602 | Fair-Val 0.6264\n",
      "Epoch 010/50 | Train-Loss 109.9849 | Test-MSE 357.8005 | Regret 0.0786 | Fair-Val 0.6604\n",
      "Epoch 020/50 | Train-Loss 122.7631 | Test-MSE 331.0451 | Regret 0.0892 | Fair-Val 0.6587\n",
      "Epoch 030/50 | Train-Loss 130.4441 | Test-MSE 304.5735 | Regret 0.0941 | Fair-Val 0.5959\n",
      "Epoch 040/50 | Train-Loss 132.7158 | Test-MSE 292.1863 | Regret 0.0953 | Fair-Val 0.4876\n",
      "Epoch 050/50 | Train-Loss 132.3121 | Test-MSE 303.5395 | Regret 0.0951 | Fair-Val 0.3700\n",
      "Training finished in 2.51s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0965 | σ = 0.0013\n",
      "[                 MSE]  μ = 313.8628 | σ = 10.3233\n",
      "[            FAIRNESS]  μ = 0.3699 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 2.5477 | σ = 0.0389\n",
      "[              G0_MSE]  μ = 302.5021 | σ = 10.4118\n",
      "[              G1_MSE]  μ = 397.8102 | σ = 9.6692\n",
      "[     G0_DECISION_OBJ]  μ = 21.0487 | σ = 0.0193\n",
      "[     G1_DECISION_OBJ]  μ = 33.8343 | σ = 0.2523\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 75.2275 | Test-MSE 368.9831 | Regret 0.3398 | Fair-Val 0.6261\n",
      "Epoch 010/50 | Train-Loss 87.7622 | Test-MSE 352.9489 | Regret 0.4233 | Fair-Val 0.6596\n",
      "Epoch 020/50 | Train-Loss 102.0407 | Test-MSE 323.6200 | Regret 0.4992 | Fair-Val 0.6612\n",
      "Epoch 030/50 | Train-Loss 104.4179 | Test-MSE 291.9105 | Regret 0.4986 | Fair-Val 0.6104\n",
      "Epoch 040/50 | Train-Loss 101.2906 | Test-MSE 268.1856 | Regret 0.4842 | Fair-Val 0.5072\n",
      "Epoch 050/50 | Train-Loss 99.7093 | Test-MSE 264.7404 | Regret 0.4768 | Fair-Val 0.3769\n",
      "Training finished in 2.58s.\n",
      "Epoch 001/50 | Train-Loss 79.6448 | Test-MSE 369.2367 | Regret 0.3472 | Fair-Val 0.6259\n",
      "Epoch 010/50 | Train-Loss 83.2817 | Test-MSE 352.2339 | Regret 0.3926 | Fair-Val 0.6555\n",
      "Epoch 020/50 | Train-Loss 95.0301 | Test-MSE 320.1678 | Regret 0.4697 | Fair-Val 0.6679\n",
      "Epoch 030/50 | Train-Loss 102.6413 | Test-MSE 286.2333 | Regret 0.4962 | Fair-Val 0.6153\n",
      "Epoch 040/50 | Train-Loss 103.6052 | Test-MSE 262.2154 | Regret 0.4907 | Fair-Val 0.5056\n",
      "Epoch 050/50 | Train-Loss 101.2571 | Test-MSE 258.7828 | Regret 0.4782 | Fair-Val 0.3715\n",
      "Training finished in 2.51s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4775 | σ = 0.0007\n",
      "[                 MSE]  μ = 261.7616 | σ = 2.9788\n",
      "[            FAIRNESS]  μ = 0.3742 | σ = 0.0027\n",
      "[       TRAINING_TIME]  μ = 2.5442 | σ = 0.0391\n",
      "[              G0_MSE]  μ = 250.8910 | σ = 2.9904\n",
      "[              G1_MSE]  μ = 342.0882 | σ = 2.8931\n",
      "[     G0_DECISION_OBJ]  μ = 18.3828 | σ = 0.0081\n",
      "[     G1_DECISION_OBJ]  μ = 33.4433 | σ = 1.3908\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3422.1860 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2808.1050 | Test-MSE 374.4723 | Regret 0.1120 | Fair-Val 0.6054\n",
      "Epoch 020/50 | Train-Loss 4195.1274 | Test-MSE 382.4343 | Regret 0.1789 | Fair-Val 0.5995\n",
      "Epoch 030/50 | Train-Loss 6792.3682 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 0.5979\n",
      "Epoch 040/50 | Train-Loss 9095.8584 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7134.9263 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 0.5977\n",
      "Training finished in 2.61s.\n",
      "Epoch 001/50 | Train-Loss 3519.0212 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2828.3860 | Test-MSE 374.7670 | Regret 0.1102 | Fair-Val 0.6049\n",
      "Epoch 020/50 | Train-Loss 4347.7773 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 0.5993\n",
      "Epoch 030/50 | Train-Loss 6758.1553 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 0.5978\n",
      "Epoch 040/50 | Train-Loss 9238.6338 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7388.5459 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 0.5977\n",
      "Training finished in 2.50s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.5557 | σ = 0.0529\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0132\n",
      "[     G0_DECISION_OBJ]  μ = 27.0188 | σ = 4.7759\n",
      "[     G1_DECISION_OBJ]  μ = 379.5319 | σ = 193.3795\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 149.8540 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 116.6639 | Test-MSE 372.6391 | Regret 0.0052 | Fair-Val 0.6051\n",
      "Epoch 020/50 | Train-Loss 337.1553 | Test-MSE 380.4269 | Regret 0.0177 | Fair-Val 0.5997\n",
      "Epoch 030/50 | Train-Loss 613.3097 | Test-MSE 384.6749 | Regret 0.0288 | Fair-Val 0.5981\n",
      "Epoch 040/50 | Train-Loss 512.1985 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 320.9695 | Test-MSE 386.4115 | Regret 0.0149 | Fair-Val 0.5977\n",
      "Training finished in 2.61s.\n",
      "Epoch 001/50 | Train-Loss 155.3708 | Test-MSE 370.2304 | Regret 0.0060 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 115.9042 | Test-MSE 373.4149 | Regret 0.0050 | Fair-Val 0.6046\n",
      "Epoch 020/50 | Train-Loss 292.5936 | Test-MSE 380.8751 | Regret 0.0153 | Fair-Val 0.5992\n",
      "Epoch 030/50 | Train-Loss 599.1772 | Test-MSE 384.7011 | Regret 0.0285 | Fair-Val 0.5980\n",
      "Epoch 040/50 | Train-Loss 493.0303 | Test-MSE 386.0293 | Regret 0.0232 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 301.8991 | Test-MSE 386.4235 | Regret 0.0142 | Fair-Val 0.5977\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0145 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4175 | σ = 0.0060\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.5941 | σ = 0.0190\n",
      "[              G0_MSE]  μ = 364.9639 | σ = 0.0025\n",
      "[              G1_MSE]  μ = 544.9435 | σ = 0.0323\n",
      "[     G0_DECISION_OBJ]  μ = 27.6423 | σ = 0.0478\n",
      "[     G1_DECISION_OBJ]  μ = 51.1906 | σ = 1.5922\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 85.1407 | Test-MSE 369.5970 | Regret 0.0590 | Fair-Val 0.6266\n",
      "Epoch 010/50 | Train-Loss 122.1740 | Test-MSE 357.3918 | Regret 0.0845 | Fair-Val 0.6597\n",
      "Epoch 020/50 | Train-Loss 137.3999 | Test-MSE 333.0327 | Regret 0.0959 | Fair-Val 0.6437\n",
      "Epoch 030/50 | Train-Loss 142.1499 | Test-MSE 311.3553 | Regret 0.0988 | Fair-Val 0.5777\n",
      "Epoch 040/50 | Train-Loss 140.1467 | Test-MSE 304.8169 | Regret 0.0984 | Fair-Val 0.4754\n",
      "Epoch 050/50 | Train-Loss 137.8512 | Test-MSE 324.6721 | Regret 0.0978 | Fair-Val 0.3690\n",
      "Training finished in 2.49s.\n",
      "Epoch 001/50 | Train-Loss 89.1408 | Test-MSE 369.9268 | Regret 0.0602 | Fair-Val 0.6264\n",
      "Epoch 010/50 | Train-Loss 115.8403 | Test-MSE 357.7974 | Regret 0.0786 | Fair-Val 0.6604\n",
      "Epoch 020/50 | Train-Loss 128.6733 | Test-MSE 331.0376 | Regret 0.0892 | Fair-Val 0.6587\n",
      "Epoch 030/50 | Train-Loss 135.8894 | Test-MSE 304.5826 | Regret 0.0942 | Fair-Val 0.5958\n",
      "Epoch 040/50 | Train-Loss 137.2202 | Test-MSE 292.2607 | Regret 0.0953 | Fair-Val 0.4871\n",
      "Epoch 050/50 | Train-Loss 135.6459 | Test-MSE 303.8094 | Regret 0.0952 | Fair-Val 0.3691\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0965 | σ = 0.0013\n",
      "[                 MSE]  μ = 314.2408 | σ = 10.4314\n",
      "[            FAIRNESS]  μ = 0.3691 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 2.5385 | σ = 0.0459\n",
      "[              G0_MSE]  μ = 302.9189 | σ = 10.5287\n",
      "[              G1_MSE]  μ = 397.9014 | σ = 9.7121\n",
      "[     G0_DECISION_OBJ]  μ = 21.0509 | σ = 0.0189\n",
      "[     G1_DECISION_OBJ]  μ = 33.8342 | σ = 0.2512\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 10, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 80.8231 | Test-MSE 368.9831 | Regret 0.3398 | Fair-Val 0.6261\n",
      "Epoch 010/50 | Train-Loss 93.5443 | Test-MSE 352.9284 | Regret 0.4229 | Fair-Val 0.6596\n",
      "Epoch 020/50 | Train-Loss 107.9224 | Test-MSE 323.5775 | Regret 0.4991 | Fair-Val 0.6612\n",
      "Epoch 030/50 | Train-Loss 110.0703 | Test-MSE 291.9179 | Regret 0.4995 | Fair-Val 0.6099\n",
      "Epoch 040/50 | Train-Loss 106.1919 | Test-MSE 268.4673 | Regret 0.4859 | Fair-Val 0.5056\n",
      "Epoch 050/50 | Train-Loss 103.3959 | Test-MSE 265.7157 | Regret 0.4785 | Fair-Val 0.3741\n",
      "Training finished in 2.50s.\n",
      "Epoch 001/50 | Train-Loss 85.2414 | Test-MSE 369.2368 | Regret 0.3472 | Fair-Val 0.6259\n",
      "Epoch 010/50 | Train-Loss 89.0452 | Test-MSE 352.2129 | Regret 0.3923 | Fair-Val 0.6555\n",
      "Epoch 020/50 | Train-Loss 100.9462 | Test-MSE 320.1313 | Regret 0.4695 | Fair-Val 0.6678\n",
      "Epoch 030/50 | Train-Loss 108.2548 | Test-MSE 286.2050 | Regret 0.4966 | Fair-Val 0.6150\n",
      "Epoch 040/50 | Train-Loss 108.3739 | Test-MSE 262.2922 | Regret 0.4916 | Fair-Val 0.5044\n",
      "Epoch 050/50 | Train-Loss 104.8022 | Test-MSE 259.3328 | Regret 0.4792 | Fair-Val 0.3687\n",
      "Training finished in 2.54s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4788 | σ = 0.0004\n",
      "[                 MSE]  μ = 262.5242 | σ = 3.1915\n",
      "[            FAIRNESS]  μ = 0.3714 | σ = 0.0027\n",
      "[       TRAINING_TIME]  μ = 2.5230 | σ = 0.0208\n",
      "[              G0_MSE]  μ = 251.7176 | σ = 3.2130\n",
      "[              G1_MSE]  μ = 342.3773 | σ = 3.0320\n",
      "[     G0_DECISION_OBJ]  μ = 18.3919 | σ = 0.0073\n",
      "[     G1_DECISION_OBJ]  μ = 33.4224 | σ = 1.3667\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "==========================================================================================\n",
      "                           GRID SEARCH COMPLETE\n",
      "==========================================================================================\n",
      "    Group  Grad Method  Alpha  Lambda  Fairness  G0_decision_obj  G0_decision_obj_std      G0_mse  G0_mse_std  G0_true_benefit  G0_true_benefit_std  G1_decision_obj  G1_decision_obj_std      G1_mse  G1_mse_std  G1_true_benefit  G1_true_benefit_std    fairness  fairness_std         mse    mse_std    regret  regret_std  training_time  training_time_std\n",
      "0   False  closed-form    0.5    0.00       mad        27.020298             4.774341  365.098801    0.007339        11.959665                  0.0       379.414795           193.261154  545.324860    0.013275        16.267719                  0.0  524.873383      0.008759  386.581741   0.008072  0.371726    0.039384       2.694582           0.019169\n",
      "1   False  closed-form    0.8    0.00       mad        27.642559             0.048028  364.963821    0.002426        11.959665                  0.0        51.190063             1.595219  544.943298    0.032043        16.267719                  0.0  524.649872      0.011627  386.417358   0.006012  0.014549    0.000363       2.723539           0.011524\n",
      "2   False  closed-form    1.5    0.00       mad        21.048405             0.019268  302.442307   10.390305        11.959665                  0.0        33.833996             0.251884  397.762512    9.640747        16.267719                  0.0  302.663559      5.222153  313.804459  10.300980  0.096445    0.001309       2.645078           0.054061\n",
      "3   False  closed-form    2.0    0.00       mad        18.381802             0.008153  250.812347    2.981705        11.959665                  0.0        33.439049             1.383116  342.084488    2.906326        16.267719                  0.0  264.976929      2.585663  261.692001   2.972733  0.477345    0.000734       2.662290           0.026734\n",
      "4   False  closed-form    0.5    1.00       mad        27.009521             4.689495  365.098755    0.007446        11.959665                  0.0       377.653015           190.082520  545.324707    0.012390        16.267719                  0.0  524.873352      0.008728  386.581696   0.008057  0.371811    0.038960       2.658417           0.043333\n",
      "5   False  closed-form    0.8    1.00       mad        27.649586             0.038444  364.971848    0.004044        11.959665                  0.0        51.202721             1.649641  544.952240    0.014435        16.267719                  0.0  524.662048      0.010620  386.425507   0.005280  0.014370    0.000426       2.670478           0.072254\n",
      "6   False  closed-form    1.5    1.00       mad        21.107574             0.020972  255.963371    7.671272        11.959665                  0.0        32.467621             0.184771  310.486252    2.656693        16.267719                  0.0  228.147766      9.090637  262.462479   7.073532  0.071927    0.000692       2.629688           0.050039\n",
      "7   False  closed-form    2.0    1.00       mad        18.387234             0.062823  184.953506    0.964783        11.959665                  0.0        32.414486             2.310066  231.761375    7.112831        16.267719                  0.0  157.306343      2.270638  190.532982   1.697639  0.326789    0.001970       2.611210           0.053136\n",
      "8   False  closed-form    0.5    0.01       mad        27.022446             4.775387  365.098801    0.007339        11.959665                  0.0       379.409760           193.261871  545.324890    0.013306        16.267719                  0.0  524.873383      0.008759  386.581741   0.008072  0.371715    0.039394       2.605432           0.021492\n",
      "9   False  closed-form    0.8    0.01       mad        27.644484             0.044933  364.964035    0.002426        11.959665                  0.0        51.195755             1.596386  544.942291    0.031769        16.267719                  0.0  524.649994      0.011505  386.417419   0.005920  0.014553    0.000356       2.556281           0.034421\n",
      "10  False  closed-form    1.5    0.01       mad        21.052067             0.017958  302.044113   10.460556        11.959665                  0.0        33.803558             0.253717  396.854599    9.687881        16.267719                  0.0  301.790634      5.417496  313.345535  10.368454  0.096084    0.001336       2.554434           0.051047\n",
      "11  False  closed-form    2.0    0.01       mad        18.385931             0.008600  249.197693    2.998611        11.959665                  0.0        33.371185             1.368938  339.295227    2.898773        16.267719                  0.0  261.892365      2.640900  259.937302   2.986710  0.472510    0.000501       2.568529           0.024442\n",
      "12  False  closed-form    0.5    1.00  atkinson        27.020294             4.774338  365.098801    0.007339        11.959665                  0.0       379.414856           193.261093  545.324860    0.013275        16.267719                  0.0    0.597660      0.000005  386.581741   0.008072  0.371726    0.039384       2.618577           0.079859\n",
      "13  False  closed-form    0.8    1.00  atkinson        27.642513             0.048074  364.963806    0.002411        11.959665                  0.0        51.190369             1.595514  544.943420    0.032166        16.267719                  0.0    0.597663      0.000006  386.417343   0.005997  0.014549    0.000363       2.706792           0.078575\n",
      "14  False  closed-form    1.5    1.00  atkinson        21.048674             0.019276  302.502090   10.411819        11.959665                  0.0        33.834305             0.252300  397.810226    9.669205        16.267719                  0.0    0.369924      0.000108  313.862823  10.323334  0.096457    0.001316       2.547720           0.038932\n",
      "15  False  closed-form    2.0    1.00  atkinson        18.382778             0.008149  250.890953    2.990410        11.959665                  0.0        33.443314             1.390825  342.088150    2.893051        16.267719                  0.0    0.374190      0.002711  261.761642   2.978806  0.477495    0.000716       2.544180           0.039144\n",
      "16  False  closed-form    0.5   10.00  atkinson        27.018822             4.775860  365.098816    0.007324        11.959665                  0.0       379.531921           193.379471  545.324860    0.013214        16.267719                  0.0    0.597660      0.000005  386.581741   0.008072  0.371731    0.039389       2.555674           0.052936\n",
      "17  False  closed-form    0.8   10.00  atkinson        27.642262             0.047758  364.963882    0.002457        11.959665                  0.0        51.190643             1.592194  544.943451    0.032318        16.267719                  0.0    0.597663      0.000006  386.417480   0.006012  0.014546    0.000362       2.594055           0.019046\n",
      "18  False  closed-form    1.5   10.00  atkinson        21.050900             0.018893  302.918869   10.528732        11.959665                  0.0        33.834206             0.251154  397.901352    9.712143        16.267719                  0.0    0.369065      0.000050  314.240768  10.431381  0.096485    0.001331       2.538517           0.045884\n",
      "19  False  closed-form    2.0   10.00  atkinson        18.391907             0.007287  251.717583    3.213020        11.959665                  0.0        33.422436             1.366707  342.377258    3.031982        16.267719                  0.0    0.371389      0.002665  262.524216   3.191452  0.478848    0.000398       2.523028           0.020804\n",
      "\n",
      "--- LaTeX Table Output ---\n",
      "\\begin{table}\n",
      "\\caption{Averaged Experimental Results Across Different Parameters.}\n",
      "\\label{tab:avg_exp_results_expanded}\n",
      "\\begin{tabular}{rlrrlrrrrrrrrrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "Group & Grad Method & Alpha & Lambda & Fairness & G0_decision_obj & G0_decision_obj_std & G0_mse & G0_mse_std & G0_true_benefit & G0_true_benefit_std & G1_decision_obj & G1_decision_obj_std & G1_mse & G1_mse_std & G1_true_benefit & G1_true_benefit_std & fairness & fairness_std & mse & mse_std & regret & regret_std & training_time & training_time_std \\\\\n",
      "\\midrule\n",
      "False & closed-form & 0.5000 & 0.0000 & mad & 27.0203 & 4.7743 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.4148 & 193.2612 & 545.3249 & 0.0133 & 16.2677 & 0.0000 & 524.8734 & 0.0088 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.6946 & 0.0192 \\\\\n",
      "False & closed-form & 0.8000 & 0.0000 & mad & 27.6426 & 0.0480 & 364.9638 & 0.0024 & 11.9597 & 0.0000 & 51.1901 & 1.5952 & 544.9433 & 0.0320 & 16.2677 & 0.0000 & 524.6499 & 0.0116 & 386.4174 & 0.0060 & 0.0145 & 0.0004 & 2.7235 & 0.0115 \\\\\n",
      "False & closed-form & 1.5000 & 0.0000 & mad & 21.0484 & 0.0193 & 302.4423 & 10.3903 & 11.9597 & 0.0000 & 33.8340 & 0.2519 & 397.7625 & 9.6407 & 16.2677 & 0.0000 & 302.6636 & 5.2222 & 313.8045 & 10.3010 & 0.0964 & 0.0013 & 2.6451 & 0.0541 \\\\\n",
      "False & closed-form & 2.0000 & 0.0000 & mad & 18.3818 & 0.0082 & 250.8123 & 2.9817 & 11.9597 & 0.0000 & 33.4390 & 1.3831 & 342.0845 & 2.9063 & 16.2677 & 0.0000 & 264.9769 & 2.5857 & 261.6920 & 2.9727 & 0.4773 & 0.0007 & 2.6623 & 0.0267 \\\\\n",
      "False & closed-form & 0.5000 & 1.0000 & mad & 27.0095 & 4.6895 & 365.0988 & 0.0074 & 11.9597 & 0.0000 & 377.6530 & 190.0825 & 545.3247 & 0.0124 & 16.2677 & 0.0000 & 524.8734 & 0.0087 & 386.5817 & 0.0081 & 0.3718 & 0.0390 & 2.6584 & 0.0433 \\\\\n",
      "False & closed-form & 0.8000 & 1.0000 & mad & 27.6496 & 0.0384 & 364.9718 & 0.0040 & 11.9597 & 0.0000 & 51.2027 & 1.6496 & 544.9522 & 0.0144 & 16.2677 & 0.0000 & 524.6620 & 0.0106 & 386.4255 & 0.0053 & 0.0144 & 0.0004 & 2.6705 & 0.0723 \\\\\n",
      "False & closed-form & 1.5000 & 1.0000 & mad & 21.1076 & 0.0210 & 255.9634 & 7.6713 & 11.9597 & 0.0000 & 32.4676 & 0.1848 & 310.4863 & 2.6567 & 16.2677 & 0.0000 & 228.1478 & 9.0906 & 262.4625 & 7.0735 & 0.0719 & 0.0007 & 2.6297 & 0.0500 \\\\\n",
      "False & closed-form & 2.0000 & 1.0000 & mad & 18.3872 & 0.0628 & 184.9535 & 0.9648 & 11.9597 & 0.0000 & 32.4145 & 2.3101 & 231.7614 & 7.1128 & 16.2677 & 0.0000 & 157.3063 & 2.2706 & 190.5330 & 1.6976 & 0.3268 & 0.0020 & 2.6112 & 0.0531 \\\\\n",
      "False & closed-form & 0.5000 & 0.0100 & mad & 27.0224 & 4.7754 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.4098 & 193.2619 & 545.3249 & 0.0133 & 16.2677 & 0.0000 & 524.8734 & 0.0088 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.6054 & 0.0215 \\\\\n",
      "False & closed-form & 0.8000 & 0.0100 & mad & 27.6445 & 0.0449 & 364.9640 & 0.0024 & 11.9597 & 0.0000 & 51.1958 & 1.5964 & 544.9423 & 0.0318 & 16.2677 & 0.0000 & 524.6500 & 0.0115 & 386.4174 & 0.0059 & 0.0146 & 0.0004 & 2.5563 & 0.0344 \\\\\n",
      "False & closed-form & 1.5000 & 0.0100 & mad & 21.0521 & 0.0180 & 302.0441 & 10.4606 & 11.9597 & 0.0000 & 33.8036 & 0.2537 & 396.8546 & 9.6879 & 16.2677 & 0.0000 & 301.7906 & 5.4175 & 313.3455 & 10.3685 & 0.0961 & 0.0013 & 2.5544 & 0.0510 \\\\\n",
      "False & closed-form & 2.0000 & 0.0100 & mad & 18.3859 & 0.0086 & 249.1977 & 2.9986 & 11.9597 & 0.0000 & 33.3712 & 1.3689 & 339.2952 & 2.8988 & 16.2677 & 0.0000 & 261.8924 & 2.6409 & 259.9373 & 2.9867 & 0.4725 & 0.0005 & 2.5685 & 0.0244 \\\\\n",
      "False & closed-form & 0.5000 & 1.0000 & atkinson & 27.0203 & 4.7743 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.4149 & 193.2611 & 545.3249 & 0.0133 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.6186 & 0.0799 \\\\\n",
      "False & closed-form & 0.8000 & 1.0000 & atkinson & 27.6425 & 0.0481 & 364.9638 & 0.0024 & 11.9597 & 0.0000 & 51.1904 & 1.5955 & 544.9434 & 0.0322 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.4173 & 0.0060 & 0.0145 & 0.0004 & 2.7068 & 0.0786 \\\\\n",
      "False & closed-form & 1.5000 & 1.0000 & atkinson & 21.0487 & 0.0193 & 302.5021 & 10.4118 & 11.9597 & 0.0000 & 33.8343 & 0.2523 & 397.8102 & 9.6692 & 16.2677 & 0.0000 & 0.3699 & 0.0001 & 313.8628 & 10.3233 & 0.0965 & 0.0013 & 2.5477 & 0.0389 \\\\\n",
      "False & closed-form & 2.0000 & 1.0000 & atkinson & 18.3828 & 0.0081 & 250.8910 & 2.9904 & 11.9597 & 0.0000 & 33.4433 & 1.3908 & 342.0882 & 2.8931 & 16.2677 & 0.0000 & 0.3742 & 0.0027 & 261.7616 & 2.9788 & 0.4775 & 0.0007 & 2.5442 & 0.0391 \\\\\n",
      "False & closed-form & 0.5000 & 10.0000 & atkinson & 27.0188 & 4.7759 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.5319 & 193.3795 & 545.3249 & 0.0132 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.5557 & 0.0529 \\\\\n",
      "False & closed-form & 0.8000 & 10.0000 & atkinson & 27.6423 & 0.0478 & 364.9639 & 0.0025 & 11.9597 & 0.0000 & 51.1906 & 1.5922 & 544.9435 & 0.0323 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.4175 & 0.0060 & 0.0145 & 0.0004 & 2.5941 & 0.0190 \\\\\n",
      "False & closed-form & 1.5000 & 10.0000 & atkinson & 21.0509 & 0.0189 & 302.9189 & 10.5287 & 11.9597 & 0.0000 & 33.8342 & 0.2512 & 397.9014 & 9.7121 & 16.2677 & 0.0000 & 0.3691 & 0.0001 & 314.2408 & 10.4314 & 0.0965 & 0.0013 & 2.5385 & 0.0459 \\\\\n",
      "False & closed-form & 2.0000 & 10.0000 & atkinson & 18.3919 & 0.0073 & 251.7176 & 3.2130 & 11.9597 & 0.0000 & 33.4224 & 1.3667 & 342.3773 & 3.0320 & 16.2677 & 0.0000 & 0.3714 & 0.0027 & 262.5242 & 3.1915 & 0.4788 & 0.0004 & 2.5230 & 0.0208 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 0.8, 1.5, 2]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form', 'finite-diff']  # New parameter\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  GRID-SEARCH HARNESS  (only the inner loop changed)\n",
    "# ---------------------------------------------------------------------\n",
    "results_list = []\n",
    "final_model_cf = None\n",
    "final_model_fd = None\n",
    "\n",
    "for group in group_settings:\n",
    "    fairness_types = ['mad', 'atkinson']\n",
    "    for grad_method in grad_methods:\n",
    "        for fairness in fairness_types:\n",
    "            if fairness == 'mad':\n",
    "                fairness_lambdas = [0, 1, 0.01]\n",
    "            elif fairness == 'atkinson':\n",
    "                fairness_lambdas = [0, 1, 10]\n",
    "            else:\n",
    "                fairness_lambdas = [0]\n",
    "            for lam in fairness_lambdas:\n",
    "                if lam == 0 and fairness != fairness_types[0]:\n",
    "                    continue  # skip unattainable combos\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method,\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = dict(\n",
    "                        X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                        cost_train=cost_train, gainF_train=gainF_train,\n",
    "                        X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                        cost_test=cost_test, gainF_test=gainF_test,\n",
    "                        model_class=LinearRegressionModel,\n",
    "                        input_dim=feats_train.shape[1],\n",
    "                        alpha=alpha, Q=Q,\n",
    "                        lambda_fair=lam, fairness_type=fairness,\n",
    "                        group=group, grad_method=grad_method,\n",
    "                        num_epochs=50, lr=0.001\n",
    "                    )\n",
    "\n",
    "                    avg_results, final_model = train_many_trials_regret(\n",
    "                        n_trials=2, **train_args) # type: ignore[call-arg]\n",
    "                    \n",
    "\n",
    "                    # ---------------- build DataFrame row ------------\n",
    "                    row = run_params.copy()\n",
    "                    row.update(avg_results)          # every metric goes in\n",
    "                    results_list.append(row)\n",
    "                    # ---------------- save the final model ------------\n",
    "                    model_name = f\"predmodel_{fairness}_{lam}_{grad_method}_{group}_{alpha}_LR.pth\"\n",
    "                    model_path = f\"E:/myREPO/Fairness-Decision-Focused-Loss/Organized-FDFL/src/models/FDFL/{model_name}\"\n",
    "                    torch.save(final_model.state_dict(), model_path) # type: ignore[arg-type]\n",
    "\n",
    "# ---------------- DataFrame & LaTeX dump -----------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Put the hyper-parameters first; everything else follows automatically\n",
    "hp_cols = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness']\n",
    "other_cols = sorted([c for c in results_df.columns if c not in hp_cols])\n",
    "results_df = results_df[hp_cols + other_cols]\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Averaged Experimental Results Across Different Parameters.\",\n",
    "    label=\"tab:avg_exp_results_expanded\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None,\n",
    "                       'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n--- LaTeX Table Output ---\")\n",
    "print(latex_table)\n",
    "results_df.to_csv(\"closed-form-res-LR.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
