{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "qdKLeJiloqz8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdKLeJiloqz8",
    "outputId": "bf71fd1c-a013-4af5-ab56-fbbfc20e2308"
   },
   "outputs": [],
   "source": [
    "# # install once\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# # !pip install -e /content/drive/MyDrive/FDFL/Fairness-Decision-Focused-Loss/Organized-FDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7xcnT2aLpOkV",
   "metadata": {
    "id": "7xcnT2aLpOkV"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/FDFL/Fairness-Decision-Focused-Loss/Organized-FDFL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66fb1b8d",
   "metadata": {
    "id": "66fb1b8d"
   },
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "    AlphaFairness, AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form,\n",
    "    solve_group, compute_coupled_group_obj,\n",
    "    solve_group_grad, compute_gradient_closed_form,\n",
    "    compute_group_gradient_analytical, solve_d_and_gradient_analytical\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "from src.utils.features import get_all_features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import time\n",
    "import itertools\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69jlfwsivRz2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69jlfwsivRz2",
    "outputId": "d0fd33ba-3386-4c5f-c382-ea1bc676def1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa5508",
   "metadata": {
    "id": "39fa5508"
   },
   "source": [
    "# Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da3d9ba9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da3d9ba9",
    "outputId": "864787e4-06b3-4a5d-ae7a-8bb28444bac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n",
    "\n",
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "\n",
    "df = pd.read_csv(r'E:\\myREPO\\Fairness-Decision-Focused-Loss\\Organized-FDFL\\src\\data\\data.csv')\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range (add 1 to avoid error)\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 1)\n",
    "benefit = benefit + 1\n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "XSSJoyH13W1J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSSJoyH13W1J",
    "outputId": "c720b676-fe69-4364-83f9-415c4cdc5762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "0    4422\n",
      "1     578\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['race'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f20f2",
   "metadata": {
    "id": "371f20f2"
   },
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cef7387f",
   "metadata": {
    "id": "cef7387f"
   },
   "outputs": [],
   "source": [
    "# class FairRiskPredictor(nn.Module):\n",
    "#     def __init__(self, input_dim, dropout_rate=0.1):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout_rate),\n",
    "#             nn.Linear(64, 1),\n",
    "#             nn.Softplus()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09cd6cd6",
   "metadata": {
    "id": "09cd6cd6"
   },
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    An improved predictor model featuring Batch Normalization for stability\n",
    "    and Kaiming (He) weight initialization for faster convergence.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The squeeze operation is now done in the training loop for clarity\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df314909",
   "metadata": {
    "id": "df314909"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d661549",
   "metadata": {
    "id": "7d661549"
   },
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37802e7c",
   "metadata": {
    "id": "37802e7c"
   },
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_group(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773c7d",
   "metadata": {
    "id": "43773c7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ea0565",
   "metadata": {
    "id": "33ea0565"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a78a9423",
   "metadata": {
    "id": "a78a9423"
   },
   "outputs": [],
   "source": [
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_group(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22d84524",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22d84524",
    "outputId": "47fb85c4-72eb-4f8b-b4c7-ced17fa5773a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([2.])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([1.])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([1.4361])\n",
      "sol_group: tensor([1.4361])\n",
      "obj_group: tensor([-606.0760])\n",
      "obj_ind: tensor([-606.0760])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_group\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2886a5ab",
   "metadata": {
    "id": "2886a5ab"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_group(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_group(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e: # type: ignore[catch]\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np\n",
    "\n",
    "\n",
    "\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "def _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group):\n",
    "    \"\"\"Helper to compute regret and the decision variable d_hat.\"\"\"\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_group(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_group(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        regret = obj_val_at_d_star - obj_val_at_d_hat\n",
    "        # regret = np.log1p(np.exp(regret * 10)) / 10\n",
    "        return regret, d_hat_np\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e: # type: ignore[catch]\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        # Return a zero regret and a placeholder for d_hat\n",
    "        return 0.0, np.zeros_like(pred_r_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847c1c",
   "metadata": {
    "id": "73847c1c"
   },
   "source": [
    "# Regret Loss nn.Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ee06d7b",
   "metadata": {
    "id": "2ee06d7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd Function for regret with a closed-form or finite-difference gradient.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method, noise_std):\n",
    "        # --- Loss Calculation (Regret) ---\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(\n",
    "            lambda t: to_numpy_1d(t.detach()), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "\n",
    "        regret, d_hat_np = _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group)\n",
    "        regret_loss = torch.tensor(regret, dtype=pred_r.dtype, device=pred_r.device)\n",
    "        # regret_loss = F.softplus(torch.tensor(regret, dtype=pred_r.dtype,device=pred_r.device), beta=10)\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method, 'noise_std': noise_std}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): # type: ignore[override]\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "\n",
    "        # Default to a zero gradient if anything goes wrong\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 8\n",
    "\n",
    "        try:\n",
    "            grad_regret_np = np.zeros(pred_r.shape[0], dtype=np.float32)\n",
    "\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                if params['group']:\n",
    "                    # HYBRID APPROACH for group=True\n",
    "                    # Some helpers need Tensors, others need NumPy arrays.\n",
    "\n",
    "                    # 1. Call Tensor-expecting function with Tensors\n",
    "                    grad_obj_tensor = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    # Convert its output to NumPy\n",
    "                    v_np = grad_obj_tensor.detach().cpu().numpy()\n",
    "\n",
    "                    # 2. Convert inputs for NumPy-expecting function\n",
    "                    pred_r_np = pred_r.detach().cpu().numpy()\n",
    "                    cost_np = cost.detach().cpu().numpy()\n",
    "                    race_np = race.detach().cpu().numpy()\n",
    "\n",
    "                    # 3. Call NumPy-expecting function and compute final gradient\n",
    "                    jac_mat_np = solve_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    grad_regret_np = -(v_np @ jac_mat_np)\n",
    "                else:\n",
    "                    # PURE NUMPY approach for group=False (which we know works)\n",
    "                    pred_r_np = pred_r.detach().cpu().numpy()\n",
    "                    true_r_np = true_r.detach().cpu().numpy()\n",
    "                    d_hat_np = d_hat.detach().cpu().numpy()\n",
    "                    gainF_np = gainF.detach().cpu().numpy()\n",
    "                    cost_np = cost.detach().cpu().numpy()\n",
    "\n",
    "                    jac_np = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat_np = (true_r_np)**(1 - params['alpha']) * d_hat_np**(-params['alpha'])\n",
    "                    grad_regret_np = -(grad_obj_wrt_d_hat_np @ jac_np)\n",
    "\n",
    "                if params['noise_std'] > 0:\n",
    "                    noise = np.random.normal(loc=0.0, scale=params['noise_std'], size=grad_regret_np.shape)\n",
    "                    grad_regret_np += noise.astype(grad_regret_np.dtype)\n",
    "\n",
    "            elif params['grad_method'] == 'finite-diff':\n",
    "                # This whole block can be NumPy, with one exception for the group gradient helper\n",
    "                pred_r_np = pred_r.detach().cpu().numpy().astype(np.float32)\n",
    "                true_r_np = true_r.detach().cpu().numpy().astype(np.float32)\n",
    "                d_hat_np = d_hat.detach().cpu().numpy().astype(np.float32)\n",
    "                gainF_np = gainF.detach().cpu().numpy().astype(np.float32)\n",
    "                cost_np = cost.detach().cpu().numpy().astype(np.float32)\n",
    "                race_np = race.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "                if params['group']:\n",
    "                    # Call the Tensor-expecting helper and convert its output to NumPy\n",
    "                    grad_obj_tensor = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    grad_obj_wrt_d_hat_np = grad_obj_tensor.detach().cpu().numpy()\n",
    "                else:\n",
    "                    grad_obj_wrt_d_hat_np = (true_r_np)**(1 - params['alpha']) * d_hat_np**(-params['alpha'])\n",
    "\n",
    "                # The rest of the finite-difference loop remains in NumPy\n",
    "                n = len(pred_r_np)\n",
    "                eps = 1e-4\n",
    "                for i in range(n):\n",
    "                    try:\n",
    "                        h = eps * max(1.0, abs(pred_r_np[i]))\n",
    "                        if h == 0: continue\n",
    "                        pred_r_plus = pred_r_np.copy(); pred_r_plus[i] += h\n",
    "                        pred_r_minus = pred_r_np.copy(); pred_r_minus[i] -= h\n",
    "\n",
    "                        if params['group']:\n",
    "                            d_hat_np_plus = solve_group(pred_r_plus, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                            d_hat_np_minus = solve_group(pred_r_minus, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                        else:\n",
    "                            d_hat_np_plus, _ = solve_closed_form(gainF_np, pred_r_plus, cost_np, params['alpha'], params['Q'])\n",
    "                            d_hat_np_minus, _ = solve_closed_form(gainF_np, pred_r_minus, cost_np, params['alpha'], params['Q'])\n",
    "\n",
    "                        d_hat_np_plus = d_hat_np_plus.astype(np.float32)\n",
    "                        d_hat_np_minus = d_hat_np_minus.astype(np.float32)\n",
    "                        grad_d_wrt_ri = (d_hat_np_plus - d_hat_np_minus) / (2 * h)\n",
    "                        grad_regret_np[i] = -np.dot(grad_obj_wrt_d_hat_np, grad_d_wrt_ri)\n",
    "                    except Exception as loop_e:\n",
    "                        print(f\"Warning: Solver failed during FD at index {i}: {loop_e}. Setting grad component to 0.\")\n",
    "                        grad_regret_np[i] = 0.0\n",
    "\n",
    "            # Convert the final numpy gradient back to a PyTorch tensor\n",
    "            grad_regret = torch.from_numpy(grad_regret_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Gradient calculation failed: {e}. Returning zero grad.\")\n",
    "            # grad_regret is already a zero tensor, so we just proceed to the return\n",
    "\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "\n",
    "    Can compute loss using either custom regret or standard MSE for the primary task,\n",
    "    combined with a fairness penalty.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form', noise_std=0.0, loss_type='regret'):\n",
    "        \"\"\"\n",
    "        Initializes the loss module.\n",
    "\n",
    "        Args:\n",
    "            Q, alpha, lambdas, fairness_type, group, grad_method, noise_std: Existing parameters.\n",
    "            loss_type (str): The primary loss component. Can be 'regret' for decision-focused loss\n",
    "                             or 'mse' for Mean Squared Error. Defaults to 'regret'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Validate the loss_type parameter\n",
    "        if loss_type not in ['regret', 'mse']:\n",
    "            raise ValueError(f\"Unsupported loss_type: '{loss_type}'. Must be 'regret' or 'mse'.\")\n",
    "\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.loss_type = fairness_type, group, loss_type\n",
    "        self.grad_method, self.noise_std = grad_method, noise_std\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Primary task loss (Regret or MSE)\n",
    "        if self.loss_type == 'regret':\n",
    "            # Custom decision-focused regret loss\n",
    "            task_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method, self.noise_std)\n",
    "        elif self.loss_type == 'mse':\n",
    "            # Standard Mean Squared Error loss\n",
    "            task_loss = F.mse_loss(pred_r, true_r)\n",
    "\n",
    "        # 2. Fairness penalty using standard PyTorch autograd\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "        # 3. Total loss\n",
    "        total_loss = task_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a09dba",
   "metadata": {
    "id": "75a09dba"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52816196",
   "metadata": {
    "id": "52816196"
   },
   "outputs": [],
   "source": [
    "def _evaluate_performance(model, X, y, race, cost, gainF, Q, alpha, group, fairness_type):\n",
    "    \"\"\"\n",
    "    Helper function to compute all performance metrics, now with aligned per-group fairness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).squeeze(-1).clamp(min=1)\n",
    "        y_np, race_np = y.cpu().numpy(), race.cpu().numpy()\n",
    "\n",
    "        # --- Initialize Metrics Dictionaries ---\n",
    "        metrics = {}\n",
    "        # The per-group fairness key is now generic\n",
    "        per_group_metrics = {\n",
    "            'mse': {}, 'fairness': {}, 'objective': {}\n",
    "        }\n",
    "\n",
    "        # --- Overall Metrics (logic remains the same) ---\n",
    "        metrics['mse'] = ((pred - y).pow(2)).mean().item()\n",
    "\n",
    "        _, _, d_pred_np = _calculate_loss_and_decision(pred, y, gainF, cost, race, Q, alpha, 0, 'none', group)\n",
    "        _, _, d_true_np = _calculate_loss_and_decision(y, y, gainF, cost, race, Q, alpha, 0, 'none', group)\n",
    "\n",
    "        if d_pred_np is not None and d_true_np is not None:\n",
    "            if group:\n",
    "                true_obj = compute_coupled_group_obj(d_true_np, y_np, race_np, alpha)\n",
    "                pred_obj = compute_coupled_group_obj(d_pred_np, y_np, race_np, alpha)\n",
    "            else:\n",
    "                true_obj = AlphaFairness(y_np * d_true_np, alpha)\n",
    "                pred_obj = AlphaFairness(y_np * d_pred_np, alpha)\n",
    "            metrics['regret'] = (true_obj - pred_obj) / (abs(true_obj) + 1e-9)\n",
    "        else:\n",
    "            metrics['regret'] = np.nan\n",
    "\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == \"atkinson\":\n",
    "            metrics['fairness'] = atkinson_loss(pred, y, race, beta=0.5, mode=mode).item()\n",
    "        elif fairness_type == \"mad\":\n",
    "            metrics['fairness'] = mean_abs_dev(pred, y, race, mode=mode).item()\n",
    "        # Add other overall fairness calcs as needed\n",
    "\n",
    "        # --- Per-Group Metrics ---\n",
    "        unique_groups = np.unique(race_np)\n",
    "        for g in unique_groups:\n",
    "            mask = (race == g)\n",
    "            mask_np = (race_np == g)\n",
    "\n",
    "            if mask.sum() > 0:\n",
    "                # Group-wise MSE\n",
    "                per_group_metrics['mse'][g] = ((pred[mask] - y[mask])**2).mean().item()\n",
    "\n",
    "                # --- KEY CHANGE: Group-wise Fairness Calculation ---\n",
    "                # The fairness metric for the group now matches the overall fairness_type\n",
    "                if fairness_type == \"atkinson\":\n",
    "                    per_group_metrics['fairness'][g] = atkinson_loss(pred[mask], y[mask], race=race[mask], beta=0.5, mode='individual').item()\n",
    "                elif fairness_type == \"mad\":\n",
    "                    per_group_metrics['fairness'][g] = mean_abs_dev(pred[mask], y[mask], race=race[mask], mode='individual').item()\n",
    "\n",
    "                # Group-wise Objective Value\n",
    "                if d_pred_np is not None:\n",
    "                    group_utility = y_np[mask_np] * d_pred_np[mask_np]\n",
    "                    per_group_metrics['objective'][g] = np.mean(group_utility)\n",
    "\n",
    "        metrics['per_group'] = per_group_metrics\n",
    "\n",
    "    return metrics\n",
    "\n",
    "class EarlyStopper:\n",
    "    \"\"\"\n",
    "    A helper class to implement early stopping.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, validation_loss):\n",
    "        if self.best_loss - validation_loss > self.min_delta:\n",
    "            self.best_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0ebf186",
   "metadata": {
    "id": "b0ebf186"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "# def train_model_regret(\n",
    "#         X_train, y_train, race_train, cost_train, gainF_train,\n",
    "#         X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "#         model_class, input_dim,\n",
    "#         alpha, Q,\n",
    "#         lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "#         num_epochs=30, init_lr=1e-2, batch_size=None,\n",
    "#         dropout_rate=0.1, weight_decay=1e-4, scheduler_factor = 0.5, scheduler_patience = 5, min_lr = 5e-4, noise_std=0.0,\n",
    "#         device=torch.device(\"cpu\")):\n",
    "#     \"\"\"\n",
    "#     Train a predictor via direct regret minimization, logging detailed metrics\n",
    "#     for both training and test sets at each evaluation point.\n",
    "#     \"\"\"\n",
    "#     # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "#     tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "#     X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "#         torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "#     ]\n",
    "#     train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "#     if batch_size is None: batch_size = len(train_ds)\n",
    "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "#     model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "#     optim = torch.optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "#     crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method, noise_std)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optim, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=min_lr\n",
    "#     )\n",
    "\n",
    "#     # --- Initialize Logs for both Train and Test sets ---\n",
    "#     logs = {\n",
    "#         'train_loss': [], 'train_regret': [], 'train_mse': [],\n",
    "#         'test_regret': [], 'test_mse': [], 'test_fairness': []\n",
    "#     }\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Starting training with grad_method='{grad_method}', group={group}, alpha={alpha}\")\n",
    "\n",
    "#     # --- Training Loop ---\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "#         # NOTE: The variable order from the dataloader is: race, cost, gainF\n",
    "#         for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "#             pred_b = model(x_b).squeeze().clamp(min=1)\n",
    "#             # The FDFLLoss.apply call expects: pred, true, gainF, cost, race\n",
    "#             loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "#             optim.zero_grad()\n",
    "#             if loss.requires_grad:\n",
    "#                 loss.backward()\n",
    "#                 # Optional: Add gradient clipping here if needed\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#                 optim.step()\n",
    "#             epoch_loss += loss.item() * x_b.size(0)\n",
    "#         logs['train_loss'].append(epoch_loss / len(train_ds))\n",
    "\n",
    "#         # --- Periodic Evaluation on Train and Test Sets ---\n",
    "#         if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "#             # Evaluate on Test Set\n",
    "#             test_metrics = _evaluate_performance(\n",
    "#                 model, X_test, y_test, race_test, cost_test, gainF_test,\n",
    "#                 Q, alpha, group, fairness_type\n",
    "#             )\n",
    "#             logs['test_regret'].append(test_metrics['regret'])\n",
    "#             logs['test_mse'].append(test_metrics['mse'])\n",
    "#             logs['test_fairness'].append(test_metrics['fairness'])\n",
    "\n",
    "#             # Evaluate on Training Set\n",
    "#             train_metrics = _evaluate_performance(\n",
    "#                 model, X_train, y_train, race_train, cost_train, gainF_train,\n",
    "#                 Q, alpha, group, fairness_type\n",
    "#             )\n",
    "#             logs['train_regret'].append(train_metrics['regret'])\n",
    "#             logs['train_mse'].append(train_metrics['mse'])\n",
    "\n",
    "#             # Step the scheduler based on TEST set regret\n",
    "#             scheduler.step(test_metrics['regret'])\n",
    "#             current_lr = optim.param_groups[0]['lr']\n",
    "\n",
    "#             print(f\"Epoch {epoch:03d}/{num_epochs} | \"\n",
    "#                   f\"LR: {current_lr:.1e} | \"\n",
    "#                   f\"Train Loss: {logs['train_loss'][-1]:.4f} | \"\n",
    "#                   f\"Train Regret: {train_metrics['regret']:.4f} | \"\n",
    "#                   f\"Test Regret: {test_metrics['regret']:.4f} |\"\n",
    "#                   f\"Test Fairness: {test_metrics['fairness']:.4f}\")\n",
    "\n",
    "\n",
    "#         if optim.param_groups[0]['lr'] <= min_lr:\n",
    "#             print(\"Learning rate reached minimum. Stopping early.\")\n",
    "#             break\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "#     # Return a dictionary of all logs\n",
    "#     return model, {\n",
    "#         \"logs\": logs,\n",
    "#         \"training_time\": total_time\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hh_x9yxJsjq8",
   "metadata": {
    "id": "Hh_x9yxJsjq8"
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b95859b9",
   "metadata": {
    "id": "b95859b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model_regret(\n",
    "        # Data\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        # Model & Problem Params\n",
    "        model_class, input_dim, alpha, Q,\n",
    "        # Hyperparameters\n",
    "        grad_method='closed-form', group=True, fairness_type=\"none\", lambda_fair=0.0,\n",
    "        num_epochs=80, init_lr=1e-3, batch_size=None,\n",
    "        dropout_rate=0.2, weight_decay=1e-4, noise_std=0.0,\n",
    "        scheduler_factor=0.5, scheduler_patience=7, min_lr=1e-5,\n",
    "        early_stopping_patience=15,\n",
    "        clip_grad_norm=None, # e.g., 1.0\n",
    "        print_results=True, loss_type='regret',\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    An advanced training function incorporating best practices for performance and stability.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup (Data, Model, Optimizer) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    # Use more workers and pinned memory for faster data loading\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True) # Removed pin_memory=True\n",
    "\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    # Ensure FDFLLoss constructor matches the one you are using\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method, noise_std, loss_type=loss_type)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optim, mode='min', factor=scheduler_factor, patience=scheduler_patience, min_lr=min_lr\n",
    "    )\n",
    "    early_stopper = EarlyStopper(patience=early_stopping_patience)\n",
    "\n",
    "    # --- 2. Training Loop ---\n",
    "    # --- Initialize Logs, with updated per-group key ---\n",
    "    logs = {\n",
    "        'train_loss': [], 'train_regret': [], 'train_mse': [],\n",
    "        'test_regret': [], 'test_mse': [], 'test_fairness': [],\n",
    "        'test_per_group_mse': [], 'test_per_group_fairness': [], 'test_per_group_objective': []\n",
    "    }\n",
    "    best_model_state = None\n",
    "    best_test_regret = float('inf')\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze(-1).clamp(min=1) # Clamping at 1 to match original\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        logs['train_loss'].append(epoch_loss / len(train_ds))\n",
    "        # --- Periodic Evaluation ---\n",
    "        # Evaluate on Test Set\n",
    "        test_metrics = _evaluate_performance(model, X_test, y_test, race_test, cost_test, gainF_test, Q, alpha, group, fairness_type)\n",
    "        logs['test_regret'].append(test_metrics['regret'])\n",
    "        logs['test_mse'].append(test_metrics['mse'])\n",
    "        logs['test_fairness'].append(test_metrics['fairness'])\n",
    "\n",
    "        # KEY CHANGE: Append the entire dictionary of per-group metrics for this epoch\n",
    "\n",
    "        logs['test_per_group_mse'].append(test_metrics['per_group']['mse'])\n",
    "        logs['test_per_group_fairness'].append(test_metrics['per_group']['fairness'])\n",
    "        logs['test_per_group_objective'].append(test_metrics['per_group']['objective'])\n",
    "\n",
    "        # Evaluate on Training Set\n",
    "        train_metrics = _evaluate_performance(model, X_train, y_train, race_train, cost_train, gainF_train, Q, alpha, group, fairness_type)\n",
    "        logs['train_regret'].append(train_metrics['regret'])\n",
    "        logs['train_mse'].append(train_metrics['mse'])\n",
    "\n",
    "\n",
    "        current_lr = optim.param_groups[0]['lr']\n",
    "\n",
    "        if print_results:\n",
    "          print(f\"Epoch {epoch:03d}/{num_epochs} | \"\n",
    "                f\"LR: {current_lr:.1e} | \"\n",
    "                f\"Train Loss: {logs['train_loss'][-1]:.4f} | \"\n",
    "                f\"Train Regret: {train_metrics['regret']:.4f} | \"\n",
    "                f\"Train MSE: {train_metrics['mse']:.4f} | \"\n",
    "                f\"Test Regret: {test_metrics['regret']:.4f} | \"\n",
    "                f\"Test Fairness: {test_metrics['fairness']:.4f}\")\n",
    "\n",
    "        # --- 4. Checkpoints & Early Stopping ---\n",
    "        scheduler.step(test_metrics['regret'])\n",
    "        if test_metrics['regret'] < best_test_regret:\n",
    "            best_test_regret = test_metrics['regret']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        early_stopper(test_metrics['regret'])\n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s. Best Test Regret: {best_test_regret:.4f}\")\n",
    "\n",
    "    # Load the best performing model state for returning\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, {\"logs\": logs, \"training_time\": total_time, \"best_test_regret\": best_test_regret}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16462bdd",
   "metadata": {
    "id": "16462bdd"
   },
   "source": [
    "# Test methods one instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe147f1",
   "metadata": {
    "id": "8fe147f1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23a66413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23a66413",
    "outputId": "40c41fcd-5369-4b85-b69f-917acccf205d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/50 | LR: 5.0e-03 | Train Loss: 377.6513 | Train Regret: 0.2287 | Train MSE: 363.7606 | Test Regret: 0.2234 | Test Fairness: 72.0534\n",
      "Epoch 002/50 | LR: 5.0e-03 | Train Loss: 368.7505 | Train Regret: 0.2054 | Train MSE: 351.5675 | Test Regret: 0.2025 | Test Fairness: 66.1445\n",
      "Epoch 002/50 | LR: 5.0e-03 | Train Loss: 368.7505 | Train Regret: 0.2054 | Train MSE: 351.5675 | Test Regret: 0.2025 | Test Fairness: 66.1445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/50 | LR: 5.0e-03 | Train Loss: 359.3695 | Train Regret: 0.1957 | Train MSE: 338.8463 | Test Regret: 0.1953 | Test Fairness: 60.8861\n",
      "Epoch 004/50 | LR: 5.0e-03 | Train Loss: 346.8704 | Train Regret: 0.1919 | Train MSE: 326.1183 | Test Regret: 0.1921 | Test Fairness: 56.1037\n",
      "Epoch 005/50 | LR: 5.0e-03 | Train Loss: 337.5059 | Train Regret: 0.1911 | Train MSE: 313.8477 | Test Regret: 0.1920 | Test Fairness: 51.7818\n",
      "Epoch 006/50 | LR: 5.0e-03 | Train Loss: 327.6748 | Train Regret: 0.1922 | Train MSE: 302.2666 | Test Regret: 0.1938 | Test Fairness: 48.0071\n",
      "Epoch 007/50 | LR: 5.0e-03 | Train Loss: 317.8297 | Train Regret: 0.1943 | Train MSE: 291.4850 | Test Regret: 0.1970 | Test Fairness: 44.6369\n",
      "Epoch 007/50 | LR: 5.0e-03 | Train Loss: 317.8297 | Train Regret: 0.1943 | Train MSE: 291.4850 | Test Regret: 0.1970 | Test Fairness: 44.6369\n",
      "Epoch 008/50 | LR: 5.0e-03 | Train Loss: 309.1440 | Train Regret: 0.1968 | Train MSE: 281.6631 | Test Regret: 0.2007 | Test Fairness: 41.8240\n",
      "Epoch 009/50 | LR: 5.0e-03 | Train Loss: 301.6555 | Train Regret: 0.1995 | Train MSE: 272.9262 | Test Regret: 0.2044 | Test Fairness: 39.5152\n",
      "Epoch 008/50 | LR: 5.0e-03 | Train Loss: 309.1440 | Train Regret: 0.1968 | Train MSE: 281.6631 | Test Regret: 0.2007 | Test Fairness: 41.8240\n",
      "Epoch 009/50 | LR: 5.0e-03 | Train Loss: 301.6555 | Train Regret: 0.1995 | Train MSE: 272.9262 | Test Regret: 0.2044 | Test Fairness: 39.5152\n",
      "Epoch 010/50 | LR: 5.0e-03 | Train Loss: 296.2193 | Train Regret: 0.2022 | Train MSE: 265.2053 | Test Regret: 0.2082 | Test Fairness: 37.6347\n",
      "Epoch 011/50 | LR: 5.0e-03 | Train Loss: 289.4781 | Train Regret: 0.2045 | Train MSE: 258.3778 | Test Regret: 0.2115 | Test Fairness: 36.1381\n",
      "Epoch 012/50 | LR: 5.0e-03 | Train Loss: 283.0959 | Train Regret: 0.2065 | Train MSE: 252.3224 | Test Regret: 0.2145 | Test Fairness: 34.9720\n",
      "Epoch 013/50 | LR: 5.0e-03 | Train Loss: 276.2596 | Train Regret: 0.2080 | Train MSE: 246.9144 | Test Regret: 0.2170 | Test Fairness: 34.0480\n",
      "Epoch 014/50 | LR: 2.5e-03 | Train Loss: 271.3918 | Train Regret: 0.2072 | Train MSE: 245.9635 | Test Regret: 0.2165 | Test Fairness: 34.0642\n",
      "Epoch 010/50 | LR: 5.0e-03 | Train Loss: 296.2193 | Train Regret: 0.2022 | Train MSE: 265.2053 | Test Regret: 0.2082 | Test Fairness: 37.6347\n",
      "Epoch 011/50 | LR: 5.0e-03 | Train Loss: 289.4781 | Train Regret: 0.2045 | Train MSE: 258.3778 | Test Regret: 0.2115 | Test Fairness: 36.1381\n",
      "Epoch 012/50 | LR: 5.0e-03 | Train Loss: 283.0959 | Train Regret: 0.2065 | Train MSE: 252.3224 | Test Regret: 0.2145 | Test Fairness: 34.9720\n",
      "Epoch 013/50 | LR: 5.0e-03 | Train Loss: 276.2596 | Train Regret: 0.2080 | Train MSE: 246.9144 | Test Regret: 0.2170 | Test Fairness: 34.0480\n",
      "Epoch 014/50 | LR: 2.5e-03 | Train Loss: 271.3918 | Train Regret: 0.2072 | Train MSE: 245.9635 | Test Regret: 0.2165 | Test Fairness: 34.0642\n",
      "Epoch 015/50 | LR: 2.5e-03 | Train Loss: 269.4416 | Train Regret: 0.2063 | Train MSE: 244.8959 | Test Regret: 0.2160 | Test Fairness: 34.0488\n",
      "Epoch 015/50 | LR: 2.5e-03 | Train Loss: 269.4416 | Train Regret: 0.2063 | Train MSE: 244.8959 | Test Regret: 0.2160 | Test Fairness: 34.0488\n",
      "Epoch 016/50 | LR: 2.5e-03 | Train Loss: 266.4736 | Train Regret: 0.2054 | Train MSE: 243.7250 | Test Regret: 0.2155 | Test Fairness: 34.0023\n",
      "Epoch 017/50 | LR: 2.5e-03 | Train Loss: 263.8394 | Train Regret: 0.2044 | Train MSE: 242.4615 | Test Regret: 0.2150 | Test Fairness: 33.9240\n",
      "Epoch 016/50 | LR: 2.5e-03 | Train Loss: 266.4736 | Train Regret: 0.2054 | Train MSE: 243.7250 | Test Regret: 0.2155 | Test Fairness: 34.0023\n",
      "Epoch 017/50 | LR: 2.5e-03 | Train Loss: 263.8394 | Train Regret: 0.2044 | Train MSE: 242.4615 | Test Regret: 0.2150 | Test Fairness: 33.9240\n",
      "Epoch 018/50 | LR: 2.5e-03 | Train Loss: 260.3140 | Train Regret: 0.2034 | Train MSE: 241.1128 | Test Regret: 0.2145 | Test Fairness: 33.8311\n",
      "Epoch 018/50 | LR: 2.5e-03 | Train Loss: 260.3140 | Train Regret: 0.2034 | Train MSE: 241.1128 | Test Regret: 0.2145 | Test Fairness: 33.8311\n",
      "Epoch 019/50 | LR: 2.5e-03 | Train Loss: 257.4231 | Train Regret: 0.2024 | Train MSE: 239.6902 | Test Regret: 0.2139 | Test Fairness: 33.7237\n",
      "Epoch 020/50 | LR: 2.5e-03 | Train Loss: 256.0610 | Train Regret: 0.2013 | Train MSE: 238.1920 | Test Regret: 0.2133 | Test Fairness: 33.5881\n",
      "Early stopping triggered at epoch 20.\n",
      "Training finished in 0.91s. Best Test Regret: 0.1920\n",
      "Epoch 019/50 | LR: 2.5e-03 | Train Loss: 257.4231 | Train Regret: 0.2024 | Train MSE: 239.6902 | Test Regret: 0.2139 | Test Fairness: 33.7237\n",
      "Epoch 020/50 | LR: 2.5e-03 | Train Loss: 256.0610 | Train Regret: 0.2013 | Train MSE: 238.1920 | Test Regret: 0.2133 | Test Fairness: 33.5881\n",
      "Early stopping triggered at epoch 20.\n",
      "Training finished in 0.91s. Best Test Regret: 0.1920\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# alpha = 2\n",
    "hyperparams = {\n",
    "    \"alpha\":2,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"mad\",\n",
    "    \"group\": True,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"closed-form\",  # Options: 'closed-form' or 'finite-diff\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"init_lr\":0.005,\n",
    "    \"noise_std\": 0.00,  # Standard deviation for noise in gradient\n",
    "    'loss_type': 'mse',  # 'regret' for decision-focused loss, 'mse' for standard MSE\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "logs = logs[\"logs\"]\n",
    "visLearningCurve(logs[\"train_loss\"],\n",
    "                 logs[\"train_regret\"],\n",
    "                 logs[\"train_mse\"],\n",
    "                 logs[\"test_fairness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a666d249",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a666d249",
    "outputId": "2981aded-771e-4737-aae8-1c7c879bc7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# alpha = 2\n",
    "hyperparams = {\n",
    "    \"alpha\":2,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"mad\",\n",
    "    \"group\": False,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"finite-diff\",  # Options: 'closed-form' or 'finite-diff\n",
    "    \"num_epochs\": 80,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"init_lr\":1e-2\n",
    "    # \"scheduler_patience\":5\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "\n",
    "logs = logs[\"logs\"]\n",
    "# Visualise the averaged learning curves\n",
    "visLearningCurve(logs[\"train_loss\"],\n",
    "                 logs[\"train_regret\"],\n",
    "                 logs[\"train_mse\"],\n",
    "                 logs[\"test_fairness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c07b3b",
   "metadata": {
    "id": "d3c07b3b"
   },
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20612ee8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20612ee8",
    "outputId": "b848401c-28ca-4bf6-fbbb-3faf543e3a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# ==================================================\n",
    "# Grid Search Finished! Finite Diff\n",
    "# Best Overall Test Regret: 0.0417\n",
    "# Best Hyperparameters found:\n",
    "#   init_lr: 0.01\n",
    "#   weight_decay: 0.0001\n",
    "#   dropout_rate: 0.4\n",
    "#   noise_std: 0.01\n",
    "#   clip_grad_norm: None\n",
    "# ==================================================\n",
    "# Grid Search Finished! Closed Form\n",
    "# Best Overall Test Regret: 0.0559\n",
    "# Best Hyperparameters found:\n",
    "#   init_lr: 0.03\n",
    "#   weight_decay: 0.0001\n",
    "#   dropout_rate: 0.4\n",
    "#   noise_std: 0.01\n",
    "#   clip_grad_norm: None\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1. DEFINE THE CURATED GRID SEARCH SPACE\n",
    "# ===================================================================\n",
    "# A reasonable search space focusing on the most impactful parameters.\n",
    "param_grid = {\n",
    "    'init_lr': [3e-2, 1e-2, 5e-3],\n",
    "    'weight_decay': [1e-4],\n",
    "    'dropout_rate': [0.4],\n",
    "    'noise_std': [0.01, 0.02, 0.05],\n",
    "    'clip_grad_norm': [None] # Test with and without gradient clipping\n",
    "}\n",
    "\n",
    "# Base hyperparameters that will not change during this search\n",
    "base_hyperparams = {\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"mad\",\n",
    "    \"group\": False, # Set to False to test the individual case\n",
    "    \"grad_method\": \"closed-form\",\n",
    "    \"num_epochs\": 80, # Set high, let early stopping take care of it\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"scheduler_patience\": 7,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# 2. RUN THE GRID SEARCH\n",
    "# ===================================================================\n",
    "best_run_info = {\n",
    "    'logs': None,\n",
    "    'regret': float('inf'),\n",
    "    'params': None\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "total_trials = len(param_combinations)\n",
    "\n",
    "print(f\"Starting Grid Search with {total_trials} trials...\\n\")\n",
    "\n",
    "for i, params_to_tune in enumerate(param_combinations):\n",
    "    # Combine base parameters with the current set from the grid\n",
    "    current_hyperparams = {**base_hyperparams, **params_to_tune}\n",
    "\n",
    "    print(f\"--- [Trial {i+1}/{total_trials}] Running with: {params_to_tune} ---\")\n",
    "\n",
    "    try:\n",
    "        # Call the updated training function\n",
    "        _, results = train_model_regret(\n",
    "            X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "            X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "            model_class=FairRiskPredictor,\n",
    "            input_dim=feats_train.shape[1],\n",
    "            **current_hyperparams\n",
    "        )\n",
    "\n",
    "        # Get the best test regret from this run (returned by the function)\n",
    "        final_test_regret = results[\"best_test_regret\"]\n",
    "        print(f\"--- Trial {i+1} finished. Best Test Regret in this run: {final_test_regret:.4f} ---\\n\")\n",
    "\n",
    "        # Check if this run is the best so far\n",
    "        if final_test_regret < best_run_info['regret']:\n",
    "            best_run_info['regret'] = final_test_regret\n",
    "            best_run_info['logs'] = results[\"logs\"]\n",
    "            best_run_info['params'] = current_hyperparams\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--- Trial {i+1} failed with error: {e} ---\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. REPORT AND PLOT THE BEST RESULT\n",
    "# ===================================================================\n",
    "print(\"=\"*50)\n",
    "print(\"Grid Search Finished!\")\n",
    "if best_run_info['params']:\n",
    "    print(f\"Best Overall Test Regret: {best_run_info['regret']:.4f}\")\n",
    "    print(\"Best Hyperparameters found:\")\n",
    "    # Print only the parameters that were tuned\n",
    "    for key in param_grid.keys():\n",
    "        print(f\"  {key}: {best_run_info['params'].get(key)}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate plots only for the best performing model\n",
    "    best_logs = best_run_info['logs']\n",
    "    print(\"Generating learning curve plots for the best model...\")\n",
    "    # This call matches your original request\n",
    "    visLearningCurve(best_logs[\"train_loss\"],\n",
    "                     best_logs[\"train_regret\"],\n",
    "                     best_logs[\"test_mse\"],\n",
    "                     best_logs[\"test_fairness\"])\n",
    "else:\n",
    "    print(\"No successful runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc0bd7",
   "metadata": {
    "id": "efdc0bd7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "805420ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "805420ef",
    "outputId": "51ba4a93-c56a-4de4-cb2c-5c0eabafb36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# ===================================================================\n",
    "# 1. DEFINE THE CURATED GRID SEARCH SPACE\n",
    "# ===================================================================\n",
    "# A reasonable search space focusing on the most impactful parameters.\n",
    "param_grid = {\n",
    "    'init_lr': [3e-2, 1e-2, 1e-3],\n",
    "    'weight_decay': [1e-4],\n",
    "    'dropout_rate': [0.4],\n",
    "    'noise_std': [0.0, 0.01],\n",
    "    'clip_grad_norm': [None, 1.0] # Test with and without gradient clipping\n",
    "}\n",
    "\n",
    "# Base hyperparameters that will not change during this search\n",
    "base_hyperparams = {\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"mad\",\n",
    "    \"group\": True, # Set to False to test the individual case\n",
    "    \"grad_method\": \"finite-diff\",\n",
    "    \"num_epochs\": 80, # Set high, let early stopping take care of it\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"scheduler_patience\": 7,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# 2. RUN THE GRID SEARCH\n",
    "# ===================================================================\n",
    "best_run_info = {\n",
    "    'logs': None,\n",
    "    'regret': float('inf'),\n",
    "    'params': None\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "total_trials = len(param_combinations)\n",
    "\n",
    "print(f\"Starting Grid Search with {total_trials} trials...\\n\")\n",
    "\n",
    "for i, params_to_tune in enumerate(param_combinations):\n",
    "    # Combine base parameters with the current set from the grid\n",
    "    current_hyperparams = {**base_hyperparams, **params_to_tune}\n",
    "\n",
    "    print(f\"--- [Trial {i+1}/{total_trials}] Running with: {params_to_tune} ---\")\n",
    "\n",
    "    try:\n",
    "        # Call the updated training function\n",
    "        _, results = train_model_regret(\n",
    "            X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "            X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "            model_class=FairRiskPredictor,\n",
    "            input_dim=feats_train.shape[1],\n",
    "            **current_hyperparams\n",
    "        )\n",
    "\n",
    "        # Get the best test regret from this run (returned by the function)\n",
    "        final_test_regret = results[\"best_test_regret\"]\n",
    "        print(f\"--- Trial {i+1} finished. Best Test Regret in this run: {final_test_regret:.4f} ---\\n\")\n",
    "\n",
    "        # Check if this run is the best so far\n",
    "        if final_test_regret < best_run_info['regret']:\n",
    "            best_run_info['regret'] = final_test_regret\n",
    "            best_run_info['logs'] = results[\"logs\"]\n",
    "            best_run_info['params'] = current_hyperparams\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--- Trial {i+1} failed with error: {e} ---\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. REPORT AND PLOT THE BEST RESULT\n",
    "# ===================================================================\n",
    "print(\"=\"*50)\n",
    "print(\"Grid Search Finished!\")\n",
    "if best_run_info['params']:\n",
    "    print(f\"Best Overall Test Regret: {best_run_info['regret']:.4f}\")\n",
    "    print(\"Best Hyperparameters found:\")\n",
    "    # Print only the parameters that were tuned\n",
    "    for key in param_grid.keys():\n",
    "        print(f\"  {key}: {best_run_info['params'].get(key)}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate plots only for the best performing model\n",
    "    best_logs = best_run_info['logs']\n",
    "    print(\"Generating learning curve plots for the best model...\")\n",
    "    # This call matches your original request\n",
    "    visLearningCurve(best_logs[\"train_loss\"],\n",
    "                     best_logs[\"train_regret\"],\n",
    "                     best_logs[\"test_mse\"],\n",
    "                     best_logs[\"test_fairness\"])\n",
    "else:\n",
    "    print(\"No successful runs completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ao-SNztMM4",
   "metadata": {
    "id": "01ao-SNztMM4"
   },
   "source": [
    "# Run Multi-Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab4122e4",
   "metadata": {
    "id": "ab4122e4"
   },
   "outputs": [],
   "source": [
    "def train_many_trials_regret(n_trials=10, base_seed=2025, **train_args):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials`, now aggregating per-group metrics.\n",
    "    \"\"\"\n",
    "    per_trial_metrics = defaultdict(list)\n",
    "    final_model = None\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        final_model, results = train_model_regret(**train_args)\n",
    "        logs = results.get(\"logs\", {})\n",
    "\n",
    "        if logs and logs.get('test_regret'):\n",
    "            # --- Log overall metrics from the final epoch ---\n",
    "            per_trial_metrics['regret'].append(logs['test_regret'][-1])\n",
    "            per_trial_metrics['mse'].append(logs['test_mse'][-1])\n",
    "            per_trial_metrics['fairness'].append(logs['test_fairness'][-1])\n",
    "            per_trial_metrics['training_time'].append(results.get('training_time', 0))\n",
    "\n",
    "            # --- KEY CHANGE: Log per-group metrics from the final epoch with the corrected key ---\n",
    "            if logs['test_per_group_mse']:\n",
    "                final_group_mse = logs['test_per_group_mse'][-1]\n",
    "                for g_id, g_val in final_group_mse.items():\n",
    "                    per_trial_metrics[f'G{int(g_id)}_mse'].append(g_val)\n",
    "\n",
    "            if logs['test_per_group_fairness']:\n",
    "                final_group_fairness = logs['test_per_group_fairness'][-1]\n",
    "                for g_id, g_val in final_group_fairness.items():\n",
    "                    per_trial_metrics[f'G{int(g_id)}_fairness'].append(g_val)\n",
    "\n",
    "            if logs['test_per_group_objective']:\n",
    "                final_group_obj = logs['test_per_group_objective'][-1]\n",
    "                for g_id, g_val in final_group_obj.items():\n",
    "                    per_trial_metrics[f'G{int(g_id)}_objective'].append(g_val)\n",
    "        else:\n",
    "            print(f\"Warning: Trial {t+1} did not produce valid logs.\")\n",
    "\n",
    "    # --- Aggregation logic remains the same ---\n",
    "    avg_results = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      AVERAGED RESULTS ACROSS ALL TRIALS\")\n",
    "    print(\"=\"*60)\n",
    "    for key, values in per_trial_metrics.items():\n",
    "        if values:\n",
    "            μ, sigma = np.mean(values), np.std(values)\n",
    "            avg_results[key] = μ\n",
    "            avg_results[f'{key}_std'] = sigma\n",
    "            print(f\"[{key.upper():>25s}]  μ = {μ:.4f} | σ = {sigma:.4f}\")\n",
    "        else:\n",
    "            avg_results[key] = np.nan\n",
    "            avg_results[f'{key}_std'] = np.nan\n",
    "\n",
    "    return avg_results, final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175a7d",
   "metadata": {
    "id": "8c175a7d"
   },
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6umn7k_0sPOh",
   "metadata": {
    "id": "6umn7k_0sPOh"
   },
   "source": [
    "## Predictor = NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68991548",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "68991548",
    "outputId": "6b9945fc-49c8-471a-90e0-490794129677"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"A dataclass to hold all experiment settings for easy management.\"\"\"\n",
    "    # Parameters to iterate over\n",
    "    alphas: List[float] = field(default_factory=lambda: [0.5, 2.0])\n",
    "    group_settings: List[bool] = field(default_factory=lambda: [True, False])\n",
    "    grad_methods: List[str] = field(default_factory=lambda: ['closed-form', 'finite-diff'])\n",
    "    fairness_types: List[str] = field(default_factory=lambda: ['atkinson', 'mad'])\n",
    "\n",
    "    # Static problem parameters\n",
    "    Q: int = 2500\n",
    "    num_epochs: int = 50\n",
    "    n_trials: int = 5\n",
    "\n",
    "    # Best hyperparameters found from grid search\n",
    "    best_hparams: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {\n",
    "        'finite-diff': {\n",
    "            'init_lr': 0.01, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0, 'clip_grad_norm': None\n",
    "        },\n",
    "        'closed-form': {\n",
    "            'init_lr': 0.01, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0.00, 'clip_grad_norm': None\n",
    "        }\n",
    "    })\n",
    "\n",
    "    def get_fairness_lambdas(self, fairness_type: str) -> List[float]:\n",
    "        \"\"\"Returns the lambdas for a given fairness type.\"\"\"\n",
    "        if fairness_type == 'atkinson':\n",
    "            return [0, 1, 5]\n",
    "        if fairness_type == 'mad':\n",
    "            return [0, 0.05, 0.5]\n",
    "        return [0]\n",
    "\n",
    "# ===================================================================\n",
    "# 2. RESULTS PROCESSING FUNCTION: Separate data wrangling\n",
    "# ===================================================================\n",
    "def process_and_display_results(results_df: pd.DataFrame):\n",
    "    \"\"\"Renames, reorders, and prints the final results DataFrame.\"\"\"\n",
    "    # 1. Rename columns for clarity in the final table\n",
    "    rename_map = {\n",
    "        'regret': 'Decision Regret mean', 'mse': 'Prediction MSE mean',\n",
    "        'fairness': 'Prediction Fairness mean', 'fairness_std': 'Prediction Fairness std',\n",
    "        'mse_std': 'Prediction MSE std', 'regret_std': 'Decision Regret std',\n",
    "        'training_time': 'Training Time mean', 'training_time_std': 'Training Time std',\n",
    "        'G0_mse': 'G0 MSE', 'G0_fairness': 'G0 Fairness', 'G0_objective': 'G0 Objective',\n",
    "        'G1_mse': 'G1 MSE', 'G1_fairness': 'G1 Fairness', 'G1_objective': 'G1 Objective'\n",
    "    }\n",
    "    results_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 2. Define the desired column order\n",
    "    primary_cols = [\n",
    "        'Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness',\n",
    "        'Decision Regret mean', 'Prediction MSE mean', 'Prediction Fairness mean', 'Training Time mean'\n",
    "    ]\n",
    "    # Automatically find all other columns (like standard deviations)\n",
    "    existing_primary_cols = [col for col in primary_cols if col in results_df.columns]\n",
    "    other_cols = sorted([c for c in results_df.columns if c not in existing_primary_cols])\n",
    "\n",
    "    # 3. Apply the new column order\n",
    "    results_df = results_df[existing_primary_cols + other_cols]\n",
    "\n",
    "    # --- Final Printout ---\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"                           EXPERIMENTS COMPLETE\")\n",
    "    print(\"=\"*90)\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "        print(results_df)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. MAIN EXECUTION FUNCTION: Clean and focused harness\n",
    "# ===================================================================\n",
    "def run_experiments(config: ExperimentConfig):\n",
    "    \"\"\"Executes the main experiment loop based on the provided configuration.\"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    # Generate all combinations of the main experiment parameters\n",
    "    experiment_params = list(itertools.product(\n",
    "        config.group_settings, config.grad_methods, config.fairness_types, config.alphas\n",
    "    ))\n",
    "\n",
    "    for group, grad_method, fairness, alpha in experiment_params:\n",
    "        fairness_lambdas = config.get_fairness_lambdas(fairness)\n",
    "        for lam in fairness_lambdas:\n",
    "            run_params = {\n",
    "                'Group': group, 'Grad Method': grad_method, 'Alpha': alpha,\n",
    "                'Lambda': lam, 'Fairness': fairness\n",
    "            }\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            selected_hparams = config.best_hparams[grad_method]\n",
    "            train_args = dict(\n",
    "                X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                cost_train=cost_train, gainF_train=gainF_train,\n",
    "                X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                cost_test=cost_test, gainF_test=gainF_test,\n",
    "                model_class=FairRiskPredictor,\n",
    "                input_dim=feats_train.shape[1],\n",
    "                alpha=alpha, Q=config.Q,\n",
    "                lambda_fair=lam, fairness_type=fairness,\n",
    "                group=group, grad_method=grad_method,\n",
    "                num_epochs=config.num_epochs,\n",
    "                print_results=False, loss_type='mse',\n",
    "                **selected_hparams\n",
    "            )\n",
    "\n",
    "            avg_results, _ = train_many_trials_regret(n_trials=config.n_trials, **train_args)\n",
    "            row = {**run_params, **avg_results}\n",
    "            results_list.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    process_and_display_results(results_df)\n",
    "\n",
    "# # ===================================================================\n",
    "# # 4. SCRIPT ENTRY POINT\n",
    "# # ===================================================================\n",
    "# if __name__ == '__main__':\n",
    "#     # Create a configuration object\n",
    "#     exp_config = ExperimentConfig()\n",
    "\n",
    "#     # You can easily modify the config for a specific run, for example:\n",
    "#     # exp_config.group_settings = [False]\n",
    "#     # exp_config.grad_methods = ['finite-diff']\n",
    "\n",
    "#     # Run the experiments\n",
    "#     run_experiments(exp_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KdAiAIiGsSEP",
   "metadata": {
    "id": "KdAiAIiGsSEP"
   },
   "source": [
    "## Predictor = LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "u5H328tSUsJR",
   "metadata": {
    "id": "u5H328tSUsJR"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15-y1KrIrhcv",
   "metadata": {
    "id": "15-y1KrIrhcv"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"A dataclass to hold all experiment settings for easy management.\"\"\"\n",
    "    # Parameters to iterate over\n",
    "    alphas: List[float] = field(default_factory=lambda:  [0.5, 2.0])\n",
    "    group_settings: List[bool] = field(default_factory=lambda: [True, False])\n",
    "    grad_methods: List[str] = field(default_factory=lambda: ['closed-form', 'finite-diff'])\n",
    "    fairness_types: List[str] = field(default_factory=lambda: ['atkinson', 'mad'])\n",
    "\n",
    "    # Static problem parameters\n",
    "    Q: int = 2500\n",
    "    num_epochs: int = 80\n",
    "    n_trials: int = 5\n",
    "\n",
    "    # Best hyperparameters found from grid search\n",
    "    best_hparams: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {\n",
    "        'finite-diff': {\n",
    "            'init_lr': 0.01, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0, 'clip_grad_norm': None\n",
    "        },\n",
    "        'closed-form': {\n",
    "            'init_lr': 0.03, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0.00, 'clip_grad_norm': None\n",
    "        }\n",
    "    })\n",
    "\n",
    "    def get_fairness_lambdas(self, fairness_type: str) -> List[float]:\n",
    "        \"\"\"Returns the lambdas for a given fairness type.\"\"\"\n",
    "        if fairness_type == 'atkinson':\n",
    "            return [0, 1, 5]\n",
    "        if fairness_type == 'mad':\n",
    "            return [0, 0.05, 0.5]\n",
    "        return [0]\n",
    "\n",
    "# ===================================================================\n",
    "# 2. RESULTS PROCESSING FUNCTION: Separate data wrangling\n",
    "# ===================================================================\n",
    "def process_and_display_results(results_df: pd.DataFrame):\n",
    "    \"\"\"Renames, reorders, and prints the final results DataFrame.\"\"\"\n",
    "    # 1. Rename columns for clarity in the final table\n",
    "    rename_map = {\n",
    "        'regret': 'Decision Regret mean', 'mse': 'Prediction MSE mean',\n",
    "        'fairness': 'Prediction Fairness mean', 'fairness_std': 'Prediction Fairness std',\n",
    "        'mse_std': 'Prediction MSE std', 'regret_std': 'Decision Regret std',\n",
    "        'training_time': 'Training Time mean', 'training_time_std': 'Training Time std',\n",
    "        'G0_mse': 'G0 MSE', 'G0_fairness': 'G0 Fairness', 'G0_objective': 'G0 Objective',\n",
    "        'G1_mse': 'G1 MSE', 'G1_fairness': 'G1 Fairness', 'G1_objective': 'G1 Objective'\n",
    "    }\n",
    "    results_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 2. Define the desired column order\n",
    "    primary_cols = [\n",
    "        'Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness',\n",
    "        'Decision Regret mean', 'Prediction MSE mean', 'Prediction Fairness mean', 'Training Time mean'\n",
    "    ]\n",
    "    # Automatically find all other columns (like standard deviations)\n",
    "    existing_primary_cols = [col for col in primary_cols if col in results_df.columns]\n",
    "    other_cols = sorted([c for c in results_df.columns if c not in existing_primary_cols])\n",
    "\n",
    "    # 3. Apply the new column order\n",
    "    results_df = results_df[existing_primary_cols + other_cols]\n",
    "\n",
    "    # --- Final Printout ---\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"                           EXPERIMENTS COMPLETE\")\n",
    "    print(\"=\"*90)\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "        print(results_df)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. MAIN EXECUTION FUNCTION: Clean and focused harness\n",
    "# ===================================================================\n",
    "def run_experiments(config: ExperimentConfig):\n",
    "    \"\"\"Executes the main experiment loop based on the provided configuration.\"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    # Generate all combinations of the main experiment parameters\n",
    "    experiment_params = list(itertools.product(\n",
    "        config.group_settings, config.grad_methods, config.fairness_types, config.alphas\n",
    "    ))\n",
    "\n",
    "    for group, grad_method, fairness, alpha in experiment_params:\n",
    "        fairness_lambdas = config.get_fairness_lambdas(fairness)\n",
    "        for lam in fairness_lambdas:\n",
    "            run_params = {\n",
    "                'Group': group, 'Grad Method': grad_method, 'Alpha': alpha,\n",
    "                'Lambda': lam, 'Fairness': fairness\n",
    "            }\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            selected_hparams = config.best_hparams[grad_method]\n",
    "            train_args = dict(\n",
    "                X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                cost_train=cost_train, gainF_train=gainF_train,\n",
    "                X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                cost_test=cost_test, gainF_test=gainF_test,\n",
    "                model_class=LinearRegressionModel,\n",
    "                input_dim=feats_train.shape[1],\n",
    "                alpha=alpha, Q=config.Q,\n",
    "                lambda_fair=lam, fairness_type=fairness,\n",
    "                group=group, grad_method=grad_method,\n",
    "                num_epochs=config.num_epochs,\n",
    "                print_results=False,\n",
    "                **selected_hparams\n",
    "            )\n",
    "\n",
    "            avg_results, _ = train_many_trials_regret(n_trials=config.n_trials, **train_args)\n",
    "            row = {**run_params, **avg_results}\n",
    "            results_list.append(row)\n",
    "\n",
    "    results_df_lr = pd.DataFrame(results_list)\n",
    "    process_and_display_results(results_df_lr)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. SCRIPT ENTRY POINT\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    # Create a configuration object\n",
    "    exp_config = ExperimentConfig()\n",
    "\n",
    "    # You can easily modify the config for a specific run, for example:\n",
    "    # exp_config.group_settings = [False]\n",
    "    # exp_config.grad_methods = ['finite-diff']\n",
    "\n",
    "    # Run the experiments\n",
    "    run_experiments(exp_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b7a9c",
   "metadata": {
    "id": "256b7a9c"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "q5D0HZ4j57Uk",
   "metadata": {
    "id": "q5D0HZ4j57Uk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0616\n",
      "Training finished in 0.20s. Best Test Regret: 0.0649\n",
      "Training finished in 0.11s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1354 | σ = 0.0459\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.0649\n",
      "Training finished in 0.11s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1354 | σ = 0.0459\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0616\n",
      "Training finished in 0.11s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0616\n",
      "Training finished in 0.11s. Best Test Regret: 0.0649\n",
      "Training finished in 0.21s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1414 | σ = 0.0514\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8787 | σ = 0.1749\n",
      "[             G1_OBJECTIVE]  μ = 47.9470 | σ = 1.7785\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.21s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1414 | σ = 0.0514\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8787 | σ = 0.1749\n",
      "[             G1_OBJECTIVE]  μ = 47.9470 | σ = 1.7785\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.20s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1313 | σ = 0.0466\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8786 | σ = 0.1752\n",
      "[             G1_OBJECTIVE]  μ = 47.9501 | σ = 1.7776\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.20s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1313 | σ = 0.0466\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8786 | σ = 0.1752\n",
      "[             G1_OBJECTIVE]  μ = 47.9501 | σ = 1.7776\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.22s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1363 | σ = 0.0583\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.22s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1363 | σ = 0.0583\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1024 | σ = 0.0021\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2833\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1024 | σ = 0.0021\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2833\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.2025\n",
      "Training finished in 0.11s. Best Test Regret: 0.2055\n",
      "Training finished in 0.19s. Best Test Regret: 0.2025\n",
      "Training finished in 0.11s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1331 | σ = 0.0385\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0925\n",
      "[             G1_OBJECTIVE]  μ = 9.9304 | σ = 0.2835\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1331 | σ = 0.0385\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0925\n",
      "[             G1_OBJECTIVE]  μ = 9.9304 | σ = 0.2835\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.21s. Best Test Regret: 0.0616\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.08s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1301 | σ = 0.0563\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.21s. Best Test Regret: 0.0616\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.08s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1301 | σ = 0.0563\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.18s. Best Test Regret: 0.0648\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.18s. Best Test Regret: 0.0648\n",
      "Training finished in 0.09s. Best Test Regret: 0.0676\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0694 | σ = 0.0017\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1226 | σ = 0.0434\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 8.8788 | σ = 0.1738\n",
      "[             G1_OBJECTIVE]  μ = 47.9554 | σ = 1.7753\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.12s. Best Test Regret: 0.0618\n",
      "Training finished in 0.09s. Best Test Regret: 0.0676\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0694 | σ = 0.0017\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1226 | σ = 0.0434\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 8.8788 | σ = 0.1738\n",
      "[             G1_OBJECTIVE]  μ = 47.9554 | σ = 1.7753\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.12s. Best Test Regret: 0.0618\n",
      "Training finished in 0.21s. Best Test Regret: 0.0640\n",
      "Training finished in 0.12s. Best Test Regret: 0.0668\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0682 | σ = 0.0016\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1478 | σ = 0.0424\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 8.8431 | σ = 0.1607\n",
      "[             G1_OBJECTIVE]  μ = 48.0040 | σ = 1.7176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.21s. Best Test Regret: 0.0640\n",
      "Training finished in 0.12s. Best Test Regret: 0.0668\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0682 | σ = 0.0016\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1478 | σ = 0.0424\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 8.8431 | σ = 0.1607\n",
      "[             G1_OBJECTIVE]  μ = 48.0040 | σ = 1.7176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.19s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1310 | σ = 0.0409\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.12s. Best Test Regret: 0.2028\n",
      "Training finished in 0.19s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1310 | σ = 0.0409\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.12s. Best Test Regret: 0.2028\n",
      "Training finished in 0.10s. Best Test Regret: 0.2053\n",
      "Training finished in 0.20s. Best Test Regret: 0.2164\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0060\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1427 | σ = 0.0425\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 7.8594 | σ = 0.0917\n",
      "[             G1_OBJECTIVE]  μ = 9.9227 | σ = 0.2786\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2053\n",
      "Training finished in 0.20s. Best Test Regret: 0.2164\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0060\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1427 | σ = 0.0425\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 7.8594 | σ = 0.0917\n",
      "[             G1_OBJECTIVE]  μ = 9.9227 | σ = 0.2786\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2009\n",
      "Training finished in 0.10s. Best Test Regret: 0.2010\n",
      "Training finished in 0.10s. Best Test Regret: 0.2009\n",
      "Training finished in 0.10s. Best Test Regret: 0.2010\n",
      "Training finished in 0.11s. Best Test Regret: 0.2091\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2078 | σ = 0.0051\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1014 | σ = 0.0064\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 7.8722 | σ = 0.0883\n",
      "[             G1_OBJECTIVE]  μ = 9.8670 | σ = 0.2759\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.0616\n",
      "Training finished in 0.11s. Best Test Regret: 0.2091\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2078 | σ = 0.0051\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1014 | σ = 0.0064\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 7.8722 | σ = 0.0883\n",
      "[             G1_OBJECTIVE]  μ = 9.8670 | σ = 0.2759\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.0616\n",
      "Training finished in 0.09s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1291 | σ = 0.0477\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1291 | σ = 0.0477\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.0616\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.19s. Best Test Regret: 0.0616\n",
      "Training finished in 0.10s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1328 | σ = 0.0407\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8787 | σ = 0.1749\n",
      "[             G1_OBJECTIVE]  μ = 47.9470 | σ = 1.7785\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1328 | σ = 0.0407\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8787 | σ = 0.1749\n",
      "[             G1_OBJECTIVE]  μ = 47.9470 | σ = 1.7785\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.0617\n",
      "Training finished in 0.20s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1339 | σ = 0.0472\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8786 | σ = 0.1752\n",
      "[             G1_OBJECTIVE]  μ = 47.9501 | σ = 1.7776\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1339 | σ = 0.0472\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 8.8786 | σ = 0.1752\n",
      "[             G1_OBJECTIVE]  μ = 47.9501 | σ = 1.7776\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.19s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.19s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1280 | σ = 0.0431\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1280 | σ = 0.0431\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.20s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1316 | σ = 0.0469\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2833\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.20s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3058 | σ = 11.2321\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1316 | σ = 0.0469\n",
      "[                   G0_MSE]  μ = 277.5404 | σ = 9.9905\n",
      "[                   G1_MSE]  μ = 393.0214 | σ = 20.5477\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2833\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.12s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2025\n",
      "Training finished in 0.12s. Best Test Regret: 0.2055\n",
      "Training finished in 0.20s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1394 | σ = 0.0458\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0925\n",
      "[             G1_OBJECTIVE]  μ = 9.9304 | σ = 0.2835\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.0616\n",
      "Training finished in 0.20s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3305 | σ = 11.2714\n",
      "[                 FAIRNESS]  μ = 0.0036 | σ = 0.0004\n",
      "[            TRAINING_TIME]  μ = 0.1394 | σ = 0.0458\n",
      "[                   G0_MSE]  μ = 277.5652 | σ = 10.0277\n",
      "[                   G1_MSE]  μ = 393.0456 | σ = 20.6020\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0925\n",
      "[             G1_OBJECTIVE]  μ = 9.9304 | σ = 0.2835\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.0616\n",
      "Training finished in 0.11s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1028 | σ = 0.0086\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.11s. Best Test Regret: 0.0649\n",
      "Training finished in 0.10s. Best Test Regret: 0.0677\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0695 | σ = 0.0018\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1028 | σ = 0.0086\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 8.8785 | σ = 0.1748\n",
      "[             G1_OBJECTIVE]  μ = 47.9480 | σ = 1.7769\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.0617\n",
      "Training finished in 0.10s. Best Test Regret: 0.0648\n",
      "Training finished in 0.09s. Best Test Regret: 0.0676\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0694 | σ = 0.0017\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1256 | σ = 0.0473\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 8.8788 | σ = 0.1738\n",
      "[             G1_OBJECTIVE]  μ = 47.9554 | σ = 1.7753\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.0617\n",
      "Training finished in 0.10s. Best Test Regret: 0.0648\n",
      "Training finished in 0.09s. Best Test Regret: 0.0676\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0694 | σ = 0.0017\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1256 | σ = 0.0473\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 8.8788 | σ = 0.1738\n",
      "[             G1_OBJECTIVE]  μ = 47.9554 | σ = 1.7753\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.0618\n",
      "Training finished in 0.10s. Best Test Regret: 0.0640\n",
      "Training finished in 0.18s. Best Test Regret: 0.0618\n",
      "Training finished in 0.10s. Best Test Regret: 0.0640\n",
      "Training finished in 0.12s. Best Test Regret: 0.0668\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0682 | σ = 0.0016\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1331 | σ = 0.0357\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 8.8431 | σ = 0.1607\n",
      "[             G1_OBJECTIVE]  μ = 48.0040 | σ = 1.7176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.13s. Best Test Regret: 0.2026\n",
      "Training finished in 0.12s. Best Test Regret: 0.0668\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.0682 | σ = 0.0016\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1331 | σ = 0.0357\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 8.8431 | σ = 0.1607\n",
      "[             G1_OBJECTIVE]  μ = 48.0040 | σ = 1.7176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.13s. Best Test Regret: 0.2026\n",
      "Training finished in 0.20s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1430 | σ = 0.0420\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 57.7444 | σ = 5.3725\n",
      "[            TRAINING_TIME]  μ = 0.1430 | σ = 0.0420\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2028\n",
      "Training finished in 0.09s. Best Test Regret: 0.2028\n",
      "Training finished in 0.20s. Best Test Regret: 0.2053\n",
      "Training finished in 0.10s. Best Test Regret: 0.2164\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0060\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1302 | σ = 0.0494\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 7.8594 | σ = 0.0917\n",
      "[             G1_OBJECTIVE]  μ = 9.9227 | σ = 0.2786\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2009\n",
      "Training finished in 0.20s. Best Test Regret: 0.2053\n",
      "Training finished in 0.10s. Best Test Regret: 0.2164\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0060\n",
      "[                      MSE]  μ = 291.4007 | σ = 11.3106\n",
      "[                 FAIRNESS]  μ = 57.6137 | σ = 5.3163\n",
      "[            TRAINING_TIME]  μ = 0.1302 | σ = 0.0494\n",
      "[                   G0_MSE]  μ = 277.6655 | σ = 10.0837\n",
      "[                   G1_MSE]  μ = 392.8929 | σ = 20.5296\n",
      "[              G0_FAIRNESS]  μ = 395.4069 | σ = 12.6125\n",
      "[              G1_FAIRNESS]  μ = 501.9623 | σ = 22.4907\n",
      "[             G0_OBJECTIVE]  μ = 7.8594 | σ = 0.0917\n",
      "[             G1_OBJECTIVE]  μ = 9.9227 | σ = 0.2786\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2009\n",
      "Training finished in 0.10s. Best Test Regret: 0.2010\n",
      "Training finished in 0.10s. Best Test Regret: 0.2010\n",
      "Training finished in 0.21s. Best Test Regret: 0.2091\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2078 | σ = 0.0051\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1369 | σ = 0.0545\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 7.8722 | σ = 0.0883\n",
      "[             G1_OBJECTIVE]  μ = 9.8670 | σ = 0.2759\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1224\n",
      "Training finished in 0.09s. Best Test Regret: 0.1267\n",
      "Training finished in 0.21s. Best Test Regret: 0.2091\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2078 | σ = 0.0051\n",
      "[                      MSE]  μ = 294.1685 | σ = 11.5405\n",
      "[                 FAIRNESS]  μ = 57.0610 | σ = 5.2604\n",
      "[            TRAINING_TIME]  μ = 0.1369 | σ = 0.0545\n",
      "[                   G0_MSE]  μ = 280.5652 | σ = 10.3572\n",
      "[                   G1_MSE]  μ = 394.6871 | σ = 20.5461\n",
      "[              G0_FAIRNESS]  μ = 399.3043 | σ = 12.8694\n",
      "[              G1_FAIRNESS]  μ = 504.5463 | σ = 22.3660\n",
      "[             G0_OBJECTIVE]  μ = 7.8722 | σ = 0.0883\n",
      "[             G1_OBJECTIVE]  μ = 9.8670 | σ = 0.2759\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1224\n",
      "Training finished in 0.09s. Best Test Regret: 0.1267\n",
      "Training finished in 0.09s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.0932 | σ = 0.0010\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.1225\n",
      "Training finished in 0.09s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.0932 | σ = 0.0010\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.1225\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.12s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1283 | σ = 0.0414\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7553 | σ = 0.2177\n",
      "[             G1_OBJECTIVE]  μ = 20.8070 | σ = 1.3899\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.12s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1283 | σ = 0.0414\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7553 | σ = 0.2177\n",
      "[             G1_OBJECTIVE]  μ = 20.8070 | σ = 1.3899\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.1226\n",
      "Training finished in 0.11s. Best Test Regret: 0.1266\n",
      "Training finished in 0.19s. Best Test Regret: 0.1226\n",
      "Training finished in 0.11s. Best Test Regret: 0.1266\n",
      "Training finished in 0.10s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1303 | σ = 0.0398\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7584 | σ = 0.2163\n",
      "[             G1_OBJECTIVE]  μ = 20.8163 | σ = 1.3901\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.10s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1303 | σ = 0.0398\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7584 | σ = 0.2163\n",
      "[             G1_OBJECTIVE]  μ = 20.8163 | σ = 1.3901\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2026\n",
      "Training finished in 0.20s. Best Test Regret: 0.2055\n",
      "Training finished in 0.08s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1245 | σ = 0.0543\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.2026\n",
      "Training finished in 0.20s. Best Test Regret: 0.2055\n",
      "Training finished in 0.08s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1245 | σ = 0.0543\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.2026\n",
      "Training finished in 0.18s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2140 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1234 | σ = 0.0436\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8599 | σ = 0.0927\n",
      "[             G1_OBJECTIVE]  μ = 9.9303 | σ = 0.2825\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2140 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1234 | σ = 0.0436\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8599 | σ = 0.0927\n",
      "[             G1_OBJECTIVE]  μ = 9.9303 | σ = 0.2825\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.2025\n",
      "Training finished in 0.09s. Best Test Regret: 0.2054\n",
      "Training finished in 0.08s. Best Test Regret: 0.2025\n",
      "Training finished in 0.09s. Best Test Regret: 0.2054\n",
      "Training finished in 0.20s. Best Test Regret: 0.2171\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2138 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1236 | σ = 0.0545\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8585 | σ = 0.0926\n",
      "[             G1_OBJECTIVE]  μ = 9.9271 | σ = 0.2851\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1224\n",
      "Training finished in 0.11s. Best Test Regret: 0.1267\n",
      "Training finished in 0.20s. Best Test Regret: 0.2171\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2138 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1236 | σ = 0.0545\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8585 | σ = 0.0926\n",
      "[             G1_OBJECTIVE]  μ = 9.9271 | σ = 0.2851\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1224\n",
      "Training finished in 0.11s. Best Test Regret: 0.1267\n",
      "Training finished in 0.19s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1264 | σ = 0.0482\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1225\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.19s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1264 | σ = 0.0482\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1225\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.09s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.0906 | σ = 0.0024\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 11.7564 | σ = 0.2187\n",
      "[             G1_OBJECTIVE]  μ = 20.8252 | σ = 1.4061\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.1222\n",
      "Training finished in 0.09s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.0906 | σ = 0.0024\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 11.7564 | σ = 0.2187\n",
      "[             G1_OBJECTIVE]  μ = 20.8252 | σ = 1.4061\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.1222\n",
      "Training finished in 0.09s. Best Test Regret: 0.1263\n",
      "Training finished in 0.11s. Best Test Regret: 0.1313\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1326 | σ = 0.0029\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1295 | σ = 0.0428\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 11.7552 | σ = 0.2255\n",
      "[             G1_OBJECTIVE]  μ = 20.9257 | σ = 1.4606\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1263\n",
      "Training finished in 0.11s. Best Test Regret: 0.1313\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1326 | σ = 0.0029\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1295 | σ = 0.0428\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 11.7552 | σ = 0.2255\n",
      "[             G1_OBJECTIVE]  μ = 20.9257 | σ = 1.4606\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.07s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1159 | σ = 0.0506\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.07s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1159 | σ = 0.0506\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.11s. Best Test Regret: 0.2023\n",
      "Training finished in 0.19s. Best Test Regret: 0.2054\n",
      "Training finished in 0.11s. Best Test Regret: 0.2023\n",
      "Training finished in 0.19s. Best Test Regret: 0.2054\n",
      "Training finished in 0.09s. Best Test Regret: 0.2173\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1283 | σ = 0.0408\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0930\n",
      "[             G1_OBJECTIVE]  μ = 9.9268 | σ = 0.2867\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2012\n",
      "Training finished in 0.09s. Best Test Regret: 0.2173\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1283 | σ = 0.0408\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0930\n",
      "[             G1_OBJECTIVE]  μ = 9.9268 | σ = 0.2867\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2012\n",
      "Training finished in 0.20s. Best Test Regret: 0.2046\n",
      "Training finished in 0.09s. Best Test Regret: 0.2160\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2120 | σ = 0.0055\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1274 | σ = 0.0499\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 7.8625 | σ = 0.0950\n",
      "[             G1_OBJECTIVE]  μ = 9.9211 | σ = 0.2906\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1224\n",
      "Training finished in 0.20s. Best Test Regret: 0.2046\n",
      "Training finished in 0.09s. Best Test Regret: 0.2160\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2120 | σ = 0.0055\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1274 | σ = 0.0499\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 7.8625 | σ = 0.0950\n",
      "[             G1_OBJECTIVE]  μ = 9.9211 | σ = 0.2906\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1224\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.21s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1272 | σ = 0.0559\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.21s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1272 | σ = 0.0559\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.1225\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.09s. Best Test Regret: 0.1225\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.20s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1247 | σ = 0.0550\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7553 | σ = 0.2177\n",
      "[             G1_OBJECTIVE]  μ = 20.8070 | σ = 1.3899\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1226\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.20s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1247 | σ = 0.0550\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7553 | σ = 0.2177\n",
      "[             G1_OBJECTIVE]  μ = 20.8070 | σ = 1.3899\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1226\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.09s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.0866 | σ = 0.0044\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7584 | σ = 0.2163\n",
      "[             G1_OBJECTIVE]  μ = 20.8163 | σ = 1.3901\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.0866 | σ = 0.0044\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 11.7584 | σ = 0.2163\n",
      "[             G1_OBJECTIVE]  μ = 20.8163 | σ = 1.3901\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.2026\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.11s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1257 | σ = 0.0388\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2055\n",
      "Training finished in 0.11s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1257 | σ = 0.0388\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 0.6572 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 1, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.2026\n",
      "Training finished in 0.08s. Best Test Regret: 0.2055\n",
      "Training finished in 0.09s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2140 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1206 | σ = 0.0491\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8599 | σ = 0.0927\n",
      "[             G1_OBJECTIVE]  μ = 9.9303 | σ = 0.2825\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.19s. Best Test Regret: 0.2026\n",
      "Training finished in 0.08s. Best Test Regret: 0.2055\n",
      "Training finished in 0.09s. Best Test Regret: 0.2175\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2140 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3960 | σ = 11.2365\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1206 | σ = 0.0491\n",
      "[                   G0_MSE]  μ = 277.6294 | σ = 9.9966\n",
      "[                   G1_MSE]  μ = 393.1212 | σ = 20.5426\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5593 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8599 | σ = 0.0927\n",
      "[             G1_OBJECTIVE]  μ = 9.9303 | σ = 0.2825\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 5, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.2025\n",
      "Training finished in 0.20s. Best Test Regret: 0.2054\n",
      "Training finished in 0.08s. Best Test Regret: 0.2025\n",
      "Training finished in 0.20s. Best Test Regret: 0.2054\n",
      "Training finished in 0.10s. Best Test Regret: 0.2171\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2138 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1253 | σ = 0.0533\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8585 | σ = 0.0926\n",
      "[             G1_OBJECTIVE]  μ = 9.9271 | σ = 0.2851\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1224\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.10s. Best Test Regret: 0.2171\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2138 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3178 | σ = 11.2871\n",
      "[                 FAIRNESS]  μ = 0.6456 | σ = 0.0037\n",
      "[            TRAINING_TIME]  μ = 0.1253 | σ = 0.0533\n",
      "[                   G0_MSE]  μ = 277.5590 | σ = 10.0425\n",
      "[                   G1_MSE]  μ = 392.9848 | σ = 20.6277\n",
      "[              G0_FAIRNESS]  μ = 0.6571 | σ = 0.0036\n",
      "[              G1_FAIRNESS]  μ = 0.5594 | σ = 0.0051\n",
      "[             G0_OBJECTIVE]  μ = 7.8585 | σ = 0.0926\n",
      "[             G1_OBJECTIVE]  μ = 9.9271 | σ = 0.2851\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.08s. Best Test Regret: 0.1224\n",
      "Training finished in 0.08s. Best Test Regret: 0.1267\n",
      "Training finished in 0.21s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1262 | σ = 0.0590\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.1225\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.21s. Best Test Regret: 0.1321\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1333 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1262 | σ = 0.0590\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 11.7551 | σ = 0.2168\n",
      "[             G1_OBJECTIVE]  μ = 20.7994 | σ = 1.3856\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.1225\n",
      "Training finished in 0.09s. Best Test Regret: 0.1266\n",
      "Training finished in 0.18s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1232 | σ = 0.0402\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 11.7564 | σ = 0.2187\n",
      "[             G1_OBJECTIVE]  μ = 20.8252 | σ = 1.4061\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.11s. Best Test Regret: 0.1222\n",
      "Training finished in 0.10s. Best Test Regret: 0.1263\n",
      "Training finished in 0.18s. Best Test Regret: 0.1320\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1332 | σ = 0.0031\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1232 | σ = 0.0402\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 11.7564 | σ = 0.2187\n",
      "[             G1_OBJECTIVE]  μ = 20.8252 | σ = 1.4061\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.11s. Best Test Regret: 0.1222\n",
      "Training finished in 0.10s. Best Test Regret: 0.1263\n",
      "Training finished in 0.07s. Best Test Regret: 0.1313\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1326 | σ = 0.0029\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.0936 | σ = 0.0137\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 11.7552 | σ = 0.2255\n",
      "[             G1_OBJECTIVE]  μ = 20.9257 | σ = 1.4606\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.2026\n",
      "Training finished in 0.07s. Best Test Regret: 0.1313\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.1326 | σ = 0.0029\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.0936 | σ = 0.0137\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 11.7552 | σ = 0.2255\n",
      "[             G1_OBJECTIVE]  μ = 20.9257 | σ = 1.4606\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.20s. Best Test Regret: 0.2026\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1328 | σ = 0.0442\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.10s. Best Test Regret: 0.2055\n",
      "Training finished in 0.10s. Best Test Regret: 0.2174\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2141 | σ = 0.0063\n",
      "[                      MSE]  μ = 291.3369 | σ = 11.2712\n",
      "[                 FAIRNESS]  μ = 409.4840 | σ = 13.9031\n",
      "[            TRAINING_TIME]  μ = 0.1328 | σ = 0.0442\n",
      "[                   G0_MSE]  μ = 277.5706 | σ = 10.0275\n",
      "[                   G1_MSE]  μ = 393.0595 | σ = 20.6027\n",
      "[              G0_FAIRNESS]  μ = 395.2689 | σ = 12.5352\n",
      "[              G1_FAIRNESS]  μ = 502.0937 | σ = 22.5438\n",
      "[             G0_OBJECTIVE]  μ = 7.8595 | σ = 0.0924\n",
      "[             G1_OBJECTIVE]  μ = 9.9306 | σ = 0.2829\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.05, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.18s. Best Test Regret: 0.2023\n",
      "Training finished in 0.11s. Best Test Regret: 0.2054\n",
      "Training finished in 0.18s. Best Test Regret: 0.2023\n",
      "Training finished in 0.11s. Best Test Regret: 0.2054\n",
      "Training finished in 0.10s. Best Test Regret: 0.2173\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1297 | σ = 0.0374\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0930\n",
      "[             G1_OBJECTIVE]  μ = 9.9268 | σ = 0.2867\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2012\n",
      "Training finished in 0.10s. Best Test Regret: 0.2173\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2137 | σ = 0.0062\n",
      "[                      MSE]  μ = 291.3976 | σ = 11.2253\n",
      "[                 FAIRNESS]  μ = 409.5483 | σ = 13.8455\n",
      "[            TRAINING_TIME]  μ = 0.1297 | σ = 0.0374\n",
      "[                   G0_MSE]  μ = 277.6366 | σ = 9.9790\n",
      "[                   G1_MSE]  μ = 393.0813 | σ = 20.5761\n",
      "[              G0_FAIRNESS]  μ = 395.3430 | σ = 12.4734\n",
      "[              G1_FAIRNESS]  μ = 502.0904 | σ = 22.5148\n",
      "[             G0_OBJECTIVE]  μ = 7.8591 | σ = 0.0930\n",
      "[             G1_OBJECTIVE]  μ = 9.9268 | σ = 0.2867\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2.0, 'Lambda': 0.5, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Training finished in 0.09s. Best Test Regret: 0.2012\n",
      "Training finished in 0.19s. Best Test Regret: 0.2046\n",
      "Training finished in 0.10s. Best Test Regret: 0.2160\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2120 | σ = 0.0055\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1246 | σ = 0.0481\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 7.8625 | σ = 0.0950\n",
      "[             G1_OBJECTIVE]  μ = 9.9211 | σ = 0.2906\n",
      "\n",
      "==========================================================================================\n",
      "                           EXPERIMENTS COMPLETE\n",
      "==========================================================================================\n",
      "    Group  Grad Method  Alpha  Lambda  Fairness  Decision Regret mean  Prediction MSE mean  Prediction Fairness mean  Training Time mean  Decision Regret std  G0 Fairness      G0 MSE  G0 Objective  G0_fairness_std  G0_mse_std  G0_objective_std  G1 Fairness      G1 MSE  G1 Objective  G1_fairness_std  G1_mse_std  G1_objective_std  Prediction Fairness std  Prediction MSE std  Training Time std\n",
      "0    True  closed-form    0.5    0.00  atkinson              0.069469           291.336934                  0.003606            0.135417             0.001769     0.657170  277.570648      8.878500         0.003589   10.027530          0.174782     0.559369  393.059540     47.947987         0.005110   20.602665          1.776881                 0.000383           11.271227           0.045923\n",
      "1    True  closed-form    0.5    1.00  atkinson              0.069468           291.305756                  0.003606            0.141393             0.001777     0.657184  277.540415      8.878736         0.003579    9.990538          0.174946     0.559380  393.021403     47.946991         0.005096   20.547705          1.778484                 0.000383           11.232061           0.051398\n",
      "2    True  closed-form    0.5    5.00  atkinson              0.069462           291.330495                  0.003605            0.131335             0.001768     0.657174  277.565247      8.878641         0.003587   10.027729          0.175231     0.559368  393.045583     47.950104         0.005109   20.602018          1.777606                 0.000383           11.271351           0.046570\n",
      "3    True  closed-form    2.0    0.00  atkinson              0.214069           291.336934                  0.003606            0.136338             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.000383           11.271227           0.058327\n",
      "4    True  closed-form    2.0    1.00  atkinson              0.214087           291.305756                  0.003606            0.102384             0.006331     0.657184  277.540415      7.859095         0.003579    9.990538          0.092393     0.559380  393.021403      9.930573         0.005096   20.547705          0.283280                 0.000383           11.232061           0.002064\n",
      "5    True  closed-form    2.0    5.00  atkinson              0.214063           291.330495                  0.003605            0.133061             0.006295     0.657174  277.565247      7.859457         0.003587   10.027729          0.092537     0.559368  393.045583      9.930424         0.005109   20.602018          0.283502                 0.000383           11.271351           0.038488\n",
      "6    True  closed-form    0.5    0.00       mad              0.069469           291.336934                 57.744446            0.130132             0.001769   395.268911  277.570648      8.878500        12.535231   10.027530          0.174782   502.093689  393.059540     47.947987        22.543778   20.602665          1.776881                 5.372533           11.271227           0.056331\n",
      "7    True  closed-form    0.5    0.05       mad              0.069406           291.400655                 57.613688            0.122610             0.001733   395.406850  277.665527      8.878840        12.612547   10.083703          0.173847   501.962311  392.892904     47.955433        22.490699   20.529578          1.775290                 5.316277           11.310605           0.043428\n",
      "8    True  closed-form    0.5    0.50       mad              0.068208           294.168488                 57.060989            0.147750             0.001563   399.304301  280.565155      8.843136        12.869371   10.357241          0.160651   504.546305  394.687134     48.004044        22.365984   20.546080          1.717556                 5.260438           11.540499           0.042400\n",
      "9    True  closed-form    2.0    0.00       mad              0.214069           291.336934                 57.744446            0.130954             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                 5.372533           11.271227           0.040874\n",
      "10   True  closed-form    2.0    0.05       mad              0.213651           291.400655                 57.613688            0.142695             0.005995   395.406850  277.665527      7.859366        12.612547   10.083703          0.091708   501.962311  392.892904      9.922713        22.490699   20.529578          0.278563                 5.316277           11.310605           0.042499\n",
      "11   True  closed-form    2.0    0.50       mad              0.207787           294.168488                 57.060989            0.101386             0.005054   399.304301  280.565155      7.872156        12.869371   10.357241          0.088288   504.546305  394.687134      9.867036        22.365984   20.546080          0.275909                 5.260438           11.540499           0.006421\n",
      "12   True  finite-diff    0.5    0.00  atkinson              0.069469           291.336934                  0.003606            0.129107             0.001769     0.657170  277.570648      8.878500         0.003589   10.027530          0.174782     0.559369  393.059540     47.947987         0.005110   20.602665          1.776881                 0.000383           11.271227           0.047675\n",
      "13   True  finite-diff    0.5    1.00  atkinson              0.069468           291.305756                  0.003606            0.132807             0.001777     0.657184  277.540415      8.878736         0.003579    9.990538          0.174946     0.559380  393.021403     47.946991         0.005096   20.547705          1.778484                 0.000383           11.232061           0.040725\n",
      "14   True  finite-diff    0.5    5.00  atkinson              0.069462           291.330495                  0.003605            0.133868             0.001768     0.657174  277.565247      8.878641         0.003587   10.027729          0.175231     0.559368  393.045583     47.950104         0.005109   20.602018          1.777606                 0.000383           11.271351           0.047216\n",
      "15   True  finite-diff    2.0    0.00  atkinson              0.214069           291.336934                  0.003606            0.128032             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.000383           11.271227           0.043093\n",
      "16   True  finite-diff    2.0    1.00  atkinson              0.214087           291.305756                  0.003606            0.131647             0.006331     0.657184  277.540415      7.859095         0.003579    9.990538          0.092393     0.559380  393.021403      9.930573         0.005096   20.547705          0.283280                 0.000383           11.232061           0.046913\n",
      "17   True  finite-diff    2.0    5.00  atkinson              0.214063           291.330495                  0.003605            0.139370             0.006295     0.657174  277.565247      7.859457         0.003587   10.027729          0.092537     0.559368  393.045583      9.930424         0.005109   20.602018          0.283502                 0.000383           11.271351           0.045772\n",
      "18   True  finite-diff    0.5    0.00       mad              0.069469           291.336934                 57.744446            0.102839             0.001769   395.268911  277.570648      8.878500        12.535231   10.027530          0.174782   502.093689  393.059540     47.947987        22.543778   20.602665          1.776881                 5.372533           11.271227           0.008597\n",
      "19   True  finite-diff    0.5    0.05       mad              0.069406           291.400655                 57.613688            0.125649             0.001733   395.406850  277.665527      8.878840        12.612547   10.083703          0.173847   501.962311  392.892904     47.955433        22.490699   20.529578          1.775290                 5.316277           11.310605           0.047301\n",
      "20   True  finite-diff    0.5    0.50       mad              0.068208           294.168488                 57.060989            0.133065             0.001563   399.304301  280.565155      8.843136        12.869371   10.357241          0.160651   504.546305  394.687134     48.004044        22.365984   20.546080          1.717556                 5.260438           11.540499           0.035686\n",
      "21   True  finite-diff    2.0    0.00       mad              0.214069           291.336934                 57.744446            0.143001             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                 5.372533           11.271227           0.041979\n",
      "22   True  finite-diff    2.0    0.05       mad              0.213651           291.400655                 57.613688            0.130156             0.005995   395.406850  277.665527      7.859366        12.612547   10.083703          0.091708   501.962311  392.892904      9.922713        22.490699   20.529578          0.278563                 5.316277           11.310605           0.049377\n",
      "23   True  finite-diff    2.0    0.50       mad              0.207787           294.168488                 57.060989            0.136875             0.005054   399.304301  280.565155      7.872156        12.869371   10.357241          0.088288   504.546305  394.687134      9.867036        22.365984   20.546080          0.275909                 5.260438           11.540499           0.054509\n",
      "24  False  closed-form    0.5    0.00  atkinson              0.133268           291.336934                  0.645623            0.093190             0.003088     0.657170  277.570648     11.755101         0.003589   10.027530          0.216762     0.559369  393.059540     20.799360         0.005110   20.602665          1.385572                 0.003725           11.271227           0.001034\n",
      "25  False  closed-form    0.5    1.00  atkinson              0.133245           291.396016                  0.645593            0.128300             0.003086     0.657140  277.629405     11.755286         0.003561    9.996622          0.217688     0.559333  393.121155     20.806978         0.005083   20.542602          1.389869                 0.003697           11.236528           0.041403\n",
      "26  False  closed-form    0.5    5.00  atkinson              0.133174           291.317810                  0.645604            0.130342             0.003053     0.657150  277.559041     11.758410         0.003575   10.042479          0.216339     0.559361  392.984812     20.816301         0.005106   20.627695          1.390097                 0.003712           11.287131           0.039824\n",
      "27  False  closed-form    2.0    0.00  atkinson              0.214069           291.336934                  0.645623            0.124495             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.003725           11.271227           0.054278\n",
      "28  False  closed-form    2.0    1.00  atkinson              0.213951           291.396016                  0.645593            0.123414             0.006248     0.657140  277.629405      7.859899         0.003561    9.996622          0.092658     0.559333  393.121155      9.930301         0.005083   20.542602          0.282525                 0.003697           11.236528           0.043626\n",
      "29  False  closed-form    2.0    5.00  atkinson              0.213813           291.317810                  0.645604            0.123556             0.006178     0.657150  277.559041      7.858461         0.003575   10.042479          0.092622     0.559361  392.984812      9.927079         0.005106   20.627695          0.285115                 0.003712           11.287131           0.054462\n",
      "30  False  closed-form    0.5    0.00       mad              0.133268           291.336934                409.483968            0.126404             0.003088   395.268911  277.570648     11.755101        12.535231   10.027530          0.216762   502.093689  393.059540     20.799360        22.543778   20.602665          1.385572                13.903115           11.271227           0.048156\n",
      "31  False  closed-form    0.5    0.05       mad              0.133157           291.397583                409.548340            0.090611             0.003070   395.342997  277.636566     11.756427        12.473405    9.978952          0.218690   502.090413  393.081268     20.825201        22.514806   20.576053          1.406051                13.845541           11.225275           0.002379\n",
      "32  False  closed-form    0.5    0.50       mad              0.132584           292.168488                410.480316            0.129471             0.002907   396.310018  278.418121     11.755196        12.394944    9.925074          0.225461   502.756307  393.773468     20.925682        22.393559   20.472654          1.460587                13.761947           11.163320           0.042840\n",
      "33  False  closed-form    2.0    0.00       mad              0.214069           291.336934                409.483968            0.115894             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                13.903115           11.271227           0.050561\n",
      "34  False  closed-form    2.0    0.05       mad              0.213730           291.397583                409.548340            0.128279             0.006158   395.342997  277.636566      7.859129        12.473405    9.978952          0.092984   502.090413  393.081268      9.926766        22.514806   20.576053          0.286700                13.845541           11.225275           0.040818\n",
      "35  False  closed-form    2.0    0.50       mad              0.211991           292.168488                410.480316            0.127434             0.005492   396.310018  278.418121      7.862534        12.394944    9.925074          0.095022   502.756307  393.773468      9.921089        22.393559   20.472654          0.290607                13.761947           11.163320           0.049880\n",
      "36  False  finite-diff    0.5    0.00  atkinson              0.133268           291.336934                  0.645623            0.127202             0.003088     0.657170  277.570648     11.755101         0.003589   10.027530          0.216762     0.559369  393.059540     20.799360         0.005110   20.602665          1.385572                 0.003725           11.271227           0.055906\n",
      "37  False  finite-diff    0.5    1.00  atkinson              0.133245           291.396016                  0.645593            0.124677             0.003086     0.657140  277.629405     11.755286         0.003561    9.996622          0.217688     0.559333  393.121155     20.806978         0.005083   20.542602          1.389869                 0.003697           11.236528           0.054975\n",
      "38  False  finite-diff    0.5    5.00  atkinson              0.133174           291.317810                  0.645604            0.086554             0.003053     0.657150  277.559041     11.758410         0.003575   10.042479          0.216339     0.559361  392.984812     20.816301         0.005106   20.627695          1.390097                 0.003712           11.287131           0.004363\n",
      "39  False  finite-diff    2.0    0.00  atkinson              0.214069           291.336934                  0.645623            0.125698             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.003725           11.271227           0.038819\n",
      "40  False  finite-diff    2.0    1.00  atkinson              0.213951           291.396016                  0.645593            0.120589             0.006248     0.657140  277.629405      7.859899         0.003561    9.996622          0.092658     0.559333  393.121155      9.930301         0.005083   20.542602          0.282525                 0.003697           11.236528           0.049052\n",
      "41  False  finite-diff    2.0    5.00  atkinson              0.213813           291.317810                  0.645604            0.125281             0.006178     0.657150  277.559041      7.858461         0.003575   10.042479          0.092622     0.559361  392.984812      9.927079         0.005106   20.627695          0.285115                 0.003712           11.287131           0.053254\n",
      "42  False  finite-diff    0.5    0.00       mad              0.133268           291.336934                409.483968            0.126158             0.003088   395.268911  277.570648     11.755101        12.535231   10.027530          0.216762   502.093689  393.059540     20.799360        22.543778   20.602665          1.385572                13.903115           11.271227           0.058977\n",
      "43  False  finite-diff    0.5    0.05       mad              0.133157           291.397583                409.548340            0.123185             0.003070   395.342997  277.636566     11.756427        12.473405    9.978952          0.218690   502.090413  393.081268     20.825201        22.514806   20.576053          1.406051                13.845541           11.225275           0.040197\n",
      "44  False  finite-diff    0.5    0.50       mad              0.132584           292.168488                410.480316            0.093592             0.002907   396.310018  278.418121     11.755196        12.394944    9.925074          0.225461   502.756307  393.773468     20.925682        22.393559   20.472654          1.460587                13.761947           11.163320           0.013697\n",
      "45  False  finite-diff    2.0    0.00       mad              0.214069           291.336934                409.483968            0.132790             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                13.903115           11.271227           0.044186\n",
      "46  False  finite-diff    2.0    0.05       mad              0.213730           291.397583                409.548340            0.129693             0.006158   395.342997  277.636566      7.859129        12.473405    9.978952          0.092984   502.090413  393.081268      9.926766        22.514806   20.576053          0.286700                13.845541           11.225275           0.037415\n",
      "47  False  finite-diff    2.0    0.50       mad              0.211991           292.168488                410.480316            0.124602             0.005492   396.310018  278.418121      7.862534        12.394944    9.925074          0.095022   502.756307  393.773468      9.921089        22.393559   20.472654          0.290607                13.761947           11.163320           0.048095\n",
      "Training finished in 0.19s. Best Test Regret: 0.2046\n",
      "Training finished in 0.10s. Best Test Regret: 0.2160\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[                   REGRET]  μ = 0.2120 | σ = 0.0055\n",
      "[                      MSE]  μ = 292.1685 | σ = 11.1633\n",
      "[                 FAIRNESS]  μ = 410.4803 | σ = 13.7619\n",
      "[            TRAINING_TIME]  μ = 0.1246 | σ = 0.0481\n",
      "[                   G0_MSE]  μ = 278.4181 | σ = 9.9251\n",
      "[                   G1_MSE]  μ = 393.7735 | σ = 20.4727\n",
      "[              G0_FAIRNESS]  μ = 396.3100 | σ = 12.3949\n",
      "[              G1_FAIRNESS]  μ = 502.7563 | σ = 22.3936\n",
      "[             G0_OBJECTIVE]  μ = 7.8625 | σ = 0.0950\n",
      "[             G1_OBJECTIVE]  μ = 9.9211 | σ = 0.2906\n",
      "\n",
      "==========================================================================================\n",
      "                           EXPERIMENTS COMPLETE\n",
      "==========================================================================================\n",
      "    Group  Grad Method  Alpha  Lambda  Fairness  Decision Regret mean  Prediction MSE mean  Prediction Fairness mean  Training Time mean  Decision Regret std  G0 Fairness      G0 MSE  G0 Objective  G0_fairness_std  G0_mse_std  G0_objective_std  G1 Fairness      G1 MSE  G1 Objective  G1_fairness_std  G1_mse_std  G1_objective_std  Prediction Fairness std  Prediction MSE std  Training Time std\n",
      "0    True  closed-form    0.5    0.00  atkinson              0.069469           291.336934                  0.003606            0.135417             0.001769     0.657170  277.570648      8.878500         0.003589   10.027530          0.174782     0.559369  393.059540     47.947987         0.005110   20.602665          1.776881                 0.000383           11.271227           0.045923\n",
      "1    True  closed-form    0.5    1.00  atkinson              0.069468           291.305756                  0.003606            0.141393             0.001777     0.657184  277.540415      8.878736         0.003579    9.990538          0.174946     0.559380  393.021403     47.946991         0.005096   20.547705          1.778484                 0.000383           11.232061           0.051398\n",
      "2    True  closed-form    0.5    5.00  atkinson              0.069462           291.330495                  0.003605            0.131335             0.001768     0.657174  277.565247      8.878641         0.003587   10.027729          0.175231     0.559368  393.045583     47.950104         0.005109   20.602018          1.777606                 0.000383           11.271351           0.046570\n",
      "3    True  closed-form    2.0    0.00  atkinson              0.214069           291.336934                  0.003606            0.136338             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.000383           11.271227           0.058327\n",
      "4    True  closed-form    2.0    1.00  atkinson              0.214087           291.305756                  0.003606            0.102384             0.006331     0.657184  277.540415      7.859095         0.003579    9.990538          0.092393     0.559380  393.021403      9.930573         0.005096   20.547705          0.283280                 0.000383           11.232061           0.002064\n",
      "5    True  closed-form    2.0    5.00  atkinson              0.214063           291.330495                  0.003605            0.133061             0.006295     0.657174  277.565247      7.859457         0.003587   10.027729          0.092537     0.559368  393.045583      9.930424         0.005109   20.602018          0.283502                 0.000383           11.271351           0.038488\n",
      "6    True  closed-form    0.5    0.00       mad              0.069469           291.336934                 57.744446            0.130132             0.001769   395.268911  277.570648      8.878500        12.535231   10.027530          0.174782   502.093689  393.059540     47.947987        22.543778   20.602665          1.776881                 5.372533           11.271227           0.056331\n",
      "7    True  closed-form    0.5    0.05       mad              0.069406           291.400655                 57.613688            0.122610             0.001733   395.406850  277.665527      8.878840        12.612547   10.083703          0.173847   501.962311  392.892904     47.955433        22.490699   20.529578          1.775290                 5.316277           11.310605           0.043428\n",
      "8    True  closed-form    0.5    0.50       mad              0.068208           294.168488                 57.060989            0.147750             0.001563   399.304301  280.565155      8.843136        12.869371   10.357241          0.160651   504.546305  394.687134     48.004044        22.365984   20.546080          1.717556                 5.260438           11.540499           0.042400\n",
      "9    True  closed-form    2.0    0.00       mad              0.214069           291.336934                 57.744446            0.130954             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                 5.372533           11.271227           0.040874\n",
      "10   True  closed-form    2.0    0.05       mad              0.213651           291.400655                 57.613688            0.142695             0.005995   395.406850  277.665527      7.859366        12.612547   10.083703          0.091708   501.962311  392.892904      9.922713        22.490699   20.529578          0.278563                 5.316277           11.310605           0.042499\n",
      "11   True  closed-form    2.0    0.50       mad              0.207787           294.168488                 57.060989            0.101386             0.005054   399.304301  280.565155      7.872156        12.869371   10.357241          0.088288   504.546305  394.687134      9.867036        22.365984   20.546080          0.275909                 5.260438           11.540499           0.006421\n",
      "12   True  finite-diff    0.5    0.00  atkinson              0.069469           291.336934                  0.003606            0.129107             0.001769     0.657170  277.570648      8.878500         0.003589   10.027530          0.174782     0.559369  393.059540     47.947987         0.005110   20.602665          1.776881                 0.000383           11.271227           0.047675\n",
      "13   True  finite-diff    0.5    1.00  atkinson              0.069468           291.305756                  0.003606            0.132807             0.001777     0.657184  277.540415      8.878736         0.003579    9.990538          0.174946     0.559380  393.021403     47.946991         0.005096   20.547705          1.778484                 0.000383           11.232061           0.040725\n",
      "14   True  finite-diff    0.5    5.00  atkinson              0.069462           291.330495                  0.003605            0.133868             0.001768     0.657174  277.565247      8.878641         0.003587   10.027729          0.175231     0.559368  393.045583     47.950104         0.005109   20.602018          1.777606                 0.000383           11.271351           0.047216\n",
      "15   True  finite-diff    2.0    0.00  atkinson              0.214069           291.336934                  0.003606            0.128032             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.000383           11.271227           0.043093\n",
      "16   True  finite-diff    2.0    1.00  atkinson              0.214087           291.305756                  0.003606            0.131647             0.006331     0.657184  277.540415      7.859095         0.003579    9.990538          0.092393     0.559380  393.021403      9.930573         0.005096   20.547705          0.283280                 0.000383           11.232061           0.046913\n",
      "17   True  finite-diff    2.0    5.00  atkinson              0.214063           291.330495                  0.003605            0.139370             0.006295     0.657174  277.565247      7.859457         0.003587   10.027729          0.092537     0.559368  393.045583      9.930424         0.005109   20.602018          0.283502                 0.000383           11.271351           0.045772\n",
      "18   True  finite-diff    0.5    0.00       mad              0.069469           291.336934                 57.744446            0.102839             0.001769   395.268911  277.570648      8.878500        12.535231   10.027530          0.174782   502.093689  393.059540     47.947987        22.543778   20.602665          1.776881                 5.372533           11.271227           0.008597\n",
      "19   True  finite-diff    0.5    0.05       mad              0.069406           291.400655                 57.613688            0.125649             0.001733   395.406850  277.665527      8.878840        12.612547   10.083703          0.173847   501.962311  392.892904     47.955433        22.490699   20.529578          1.775290                 5.316277           11.310605           0.047301\n",
      "20   True  finite-diff    0.5    0.50       mad              0.068208           294.168488                 57.060989            0.133065             0.001563   399.304301  280.565155      8.843136        12.869371   10.357241          0.160651   504.546305  394.687134     48.004044        22.365984   20.546080          1.717556                 5.260438           11.540499           0.035686\n",
      "21   True  finite-diff    2.0    0.00       mad              0.214069           291.336934                 57.744446            0.143001             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                 5.372533           11.271227           0.041979\n",
      "22   True  finite-diff    2.0    0.05       mad              0.213651           291.400655                 57.613688            0.130156             0.005995   395.406850  277.665527      7.859366        12.612547   10.083703          0.091708   501.962311  392.892904      9.922713        22.490699   20.529578          0.278563                 5.316277           11.310605           0.049377\n",
      "23   True  finite-diff    2.0    0.50       mad              0.207787           294.168488                 57.060989            0.136875             0.005054   399.304301  280.565155      7.872156        12.869371   10.357241          0.088288   504.546305  394.687134      9.867036        22.365984   20.546080          0.275909                 5.260438           11.540499           0.054509\n",
      "24  False  closed-form    0.5    0.00  atkinson              0.133268           291.336934                  0.645623            0.093190             0.003088     0.657170  277.570648     11.755101         0.003589   10.027530          0.216762     0.559369  393.059540     20.799360         0.005110   20.602665          1.385572                 0.003725           11.271227           0.001034\n",
      "25  False  closed-form    0.5    1.00  atkinson              0.133245           291.396016                  0.645593            0.128300             0.003086     0.657140  277.629405     11.755286         0.003561    9.996622          0.217688     0.559333  393.121155     20.806978         0.005083   20.542602          1.389869                 0.003697           11.236528           0.041403\n",
      "26  False  closed-form    0.5    5.00  atkinson              0.133174           291.317810                  0.645604            0.130342             0.003053     0.657150  277.559041     11.758410         0.003575   10.042479          0.216339     0.559361  392.984812     20.816301         0.005106   20.627695          1.390097                 0.003712           11.287131           0.039824\n",
      "27  False  closed-form    2.0    0.00  atkinson              0.214069           291.336934                  0.645623            0.124495             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.003725           11.271227           0.054278\n",
      "28  False  closed-form    2.0    1.00  atkinson              0.213951           291.396016                  0.645593            0.123414             0.006248     0.657140  277.629405      7.859899         0.003561    9.996622          0.092658     0.559333  393.121155      9.930301         0.005083   20.542602          0.282525                 0.003697           11.236528           0.043626\n",
      "29  False  closed-form    2.0    5.00  atkinson              0.213813           291.317810                  0.645604            0.123556             0.006178     0.657150  277.559041      7.858461         0.003575   10.042479          0.092622     0.559361  392.984812      9.927079         0.005106   20.627695          0.285115                 0.003712           11.287131           0.054462\n",
      "30  False  closed-form    0.5    0.00       mad              0.133268           291.336934                409.483968            0.126404             0.003088   395.268911  277.570648     11.755101        12.535231   10.027530          0.216762   502.093689  393.059540     20.799360        22.543778   20.602665          1.385572                13.903115           11.271227           0.048156\n",
      "31  False  closed-form    0.5    0.05       mad              0.133157           291.397583                409.548340            0.090611             0.003070   395.342997  277.636566     11.756427        12.473405    9.978952          0.218690   502.090413  393.081268     20.825201        22.514806   20.576053          1.406051                13.845541           11.225275           0.002379\n",
      "32  False  closed-form    0.5    0.50       mad              0.132584           292.168488                410.480316            0.129471             0.002907   396.310018  278.418121     11.755196        12.394944    9.925074          0.225461   502.756307  393.773468     20.925682        22.393559   20.472654          1.460587                13.761947           11.163320           0.042840\n",
      "33  False  closed-form    2.0    0.00       mad              0.214069           291.336934                409.483968            0.115894             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                13.903115           11.271227           0.050561\n",
      "34  False  closed-form    2.0    0.05       mad              0.213730           291.397583                409.548340            0.128279             0.006158   395.342997  277.636566      7.859129        12.473405    9.978952          0.092984   502.090413  393.081268      9.926766        22.514806   20.576053          0.286700                13.845541           11.225275           0.040818\n",
      "35  False  closed-form    2.0    0.50       mad              0.211991           292.168488                410.480316            0.127434             0.005492   396.310018  278.418121      7.862534        12.394944    9.925074          0.095022   502.756307  393.773468      9.921089        22.393559   20.472654          0.290607                13.761947           11.163320           0.049880\n",
      "36  False  finite-diff    0.5    0.00  atkinson              0.133268           291.336934                  0.645623            0.127202             0.003088     0.657170  277.570648     11.755101         0.003589   10.027530          0.216762     0.559369  393.059540     20.799360         0.005110   20.602665          1.385572                 0.003725           11.271227           0.055906\n",
      "37  False  finite-diff    0.5    1.00  atkinson              0.133245           291.396016                  0.645593            0.124677             0.003086     0.657140  277.629405     11.755286         0.003561    9.996622          0.217688     0.559333  393.121155     20.806978         0.005083   20.542602          1.389869                 0.003697           11.236528           0.054975\n",
      "38  False  finite-diff    0.5    5.00  atkinson              0.133174           291.317810                  0.645604            0.086554             0.003053     0.657150  277.559041     11.758410         0.003575   10.042479          0.216339     0.559361  392.984812     20.816301         0.005106   20.627695          1.390097                 0.003712           11.287131           0.004363\n",
      "39  False  finite-diff    2.0    0.00  atkinson              0.214069           291.336934                  0.645623            0.125698             0.006304     0.657170  277.570648      7.859501         0.003589   10.027530          0.092386     0.559369  393.059540      9.930625         0.005110   20.602665          0.282879                 0.003725           11.271227           0.038819\n",
      "40  False  finite-diff    2.0    1.00  atkinson              0.213951           291.396016                  0.645593            0.120589             0.006248     0.657140  277.629405      7.859899         0.003561    9.996622          0.092658     0.559333  393.121155      9.930301         0.005083   20.542602          0.282525                 0.003697           11.236528           0.049052\n",
      "41  False  finite-diff    2.0    5.00  atkinson              0.213813           291.317810                  0.645604            0.125281             0.006178     0.657150  277.559041      7.858461         0.003575   10.042479          0.092622     0.559361  392.984812      9.927079         0.005106   20.627695          0.285115                 0.003712           11.287131           0.053254\n",
      "42  False  finite-diff    0.5    0.00       mad              0.133268           291.336934                409.483968            0.126158             0.003088   395.268911  277.570648     11.755101        12.535231   10.027530          0.216762   502.093689  393.059540     20.799360        22.543778   20.602665          1.385572                13.903115           11.271227           0.058977\n",
      "43  False  finite-diff    0.5    0.05       mad              0.133157           291.397583                409.548340            0.123185             0.003070   395.342997  277.636566     11.756427        12.473405    9.978952          0.218690   502.090413  393.081268     20.825201        22.514806   20.576053          1.406051                13.845541           11.225275           0.040197\n",
      "44  False  finite-diff    0.5    0.50       mad              0.132584           292.168488                410.480316            0.093592             0.002907   396.310018  278.418121     11.755196        12.394944    9.925074          0.225461   502.756307  393.773468     20.925682        22.393559   20.472654          1.460587                13.761947           11.163320           0.013697\n",
      "45  False  finite-diff    2.0    0.00       mad              0.214069           291.336934                409.483968            0.132790             0.006304   395.268911  277.570648      7.859501        12.535231   10.027530          0.092386   502.093689  393.059540      9.930625        22.543778   20.602665          0.282879                13.903115           11.271227           0.044186\n",
      "46  False  finite-diff    2.0    0.05       mad              0.213730           291.397583                409.548340            0.129693             0.006158   395.342997  277.636566      7.859129        12.473405    9.978952          0.092984   502.090413  393.081268      9.926766        22.514806   20.576053          0.286700                13.845541           11.225275           0.037415\n",
      "47  False  finite-diff    2.0    0.50       mad              0.211991           292.168488                410.480316            0.124602             0.005492   396.310018  278.418121      7.862534        12.394944    9.925074          0.095022   502.756307  393.773468      9.921089        22.393559   20.472654          0.290607                13.761947           11.163320           0.048095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"A dataclass to hold all experiment settings for easy management.\"\"\"\n",
    "    # Parameters to iterate over\n",
    "    alphas: List[float] = field(default_factory=lambda: [0.5, 2.0])\n",
    "    group_settings: List[bool] = field(default_factory=lambda: [True, False])\n",
    "    grad_methods: List[str] = field(default_factory=lambda: ['closed-form', 'finite-diff'])\n",
    "    fairness_types: List[str] = field(default_factory=lambda: ['atkinson', 'mad'])\n",
    "\n",
    "    # Static problem parameters\n",
    "    Q: int = 2500\n",
    "    num_epochs: int = 3\n",
    "    n_trials: int = 3\n",
    "\n",
    "    # Best hyperparameters found from grid search\n",
    "    best_hparams: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {\n",
    "        'finite-diff': {\n",
    "            'init_lr': 0.01, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0, 'clip_grad_norm': None\n",
    "        },\n",
    "        'closed-form': {\n",
    "            'init_lr': 0.01, 'weight_decay': 1e-4, 'dropout_rate': 0.4,\n",
    "            'noise_std': 0.00, 'clip_grad_norm': None\n",
    "        }\n",
    "    })\n",
    "\n",
    "    def get_fairness_lambdas(self, fairness_type: str) -> List[float]:\n",
    "        \"\"\"Returns the lambdas for a given fairness type.\"\"\"\n",
    "        if fairness_type == 'atkinson':\n",
    "            return [0, 1, 5]\n",
    "        if fairness_type == 'mad':\n",
    "            return [0, 0.05, 0.5]\n",
    "        return [0]\n",
    "\n",
    "# ===================================================================\n",
    "# 2. RESULTS PROCESSING FUNCTION: Separate data wrangling\n",
    "# ===================================================================\n",
    "def process_and_display_results(results_df: pd.DataFrame):\n",
    "    \"\"\"Renames, reorders, and prints the final results DataFrame.\"\"\"\n",
    "    # 1. Rename columns for clarity in the final table\n",
    "    rename_map = {\n",
    "        'regret': 'Decision Regret mean', 'mse': 'Prediction MSE mean',\n",
    "        'fairness': 'Prediction Fairness mean', 'fairness_std': 'Prediction Fairness std',\n",
    "        'mse_std': 'Prediction MSE std', 'regret_std': 'Decision Regret std',\n",
    "        'training_time': 'Training Time mean', 'training_time_std': 'Training Time std',\n",
    "        'G0_mse': 'G0 MSE', 'G0_fairness': 'G0 Fairness', 'G0_objective': 'G0 Objective',\n",
    "        'G1_mse': 'G1 MSE', 'G1_fairness': 'G1 Fairness', 'G1_objective': 'G1 Objective'\n",
    "    }\n",
    "    results_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 2. Define the desired column order\n",
    "    primary_cols = [\n",
    "        'Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness',\n",
    "        'Decision Regret mean', 'Prediction MSE mean', 'Prediction Fairness mean', 'Training Time mean'\n",
    "    ]\n",
    "    # Automatically find all other columns (like standard deviations)\n",
    "    existing_primary_cols = [col for col in primary_cols if col in results_df.columns]\n",
    "    other_cols = sorted([c for c in results_df.columns if c not in existing_primary_cols])\n",
    "\n",
    "    # 3. Apply the new column order\n",
    "    results_df = results_df[existing_primary_cols + other_cols]\n",
    "\n",
    "    # --- Final Printout ---\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"                           EXPERIMENTS COMPLETE\")\n",
    "    print(\"=\"*90)\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "        print(results_df)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. MAIN EXECUTION FUNCTION: Clean and focused harness\n",
    "# ===================================================================\n",
    "def run_experiments(config: ExperimentConfig):\n",
    "    \"\"\"Executes the main experiment loop based on the provided configuration.\"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    # Generate all combinations of the main experiment parameters\n",
    "    experiment_params = list(itertools.product(\n",
    "        config.group_settings, config.grad_methods, config.fairness_types, config.alphas\n",
    "    ))\n",
    "\n",
    "    for group, grad_method, fairness, alpha in experiment_params:\n",
    "        fairness_lambdas = config.get_fairness_lambdas(fairness)\n",
    "        for lam in fairness_lambdas:\n",
    "            run_params = {\n",
    "                'Group': group, 'Grad Method': grad_method, 'Alpha': alpha,\n",
    "                'Lambda': lam, 'Fairness': fairness\n",
    "            }\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            selected_hparams = config.best_hparams[grad_method]\n",
    "            train_args = dict(\n",
    "                X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                cost_train=cost_train, gainF_train=gainF_train,\n",
    "                X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                cost_test=cost_test, gainF_test=gainF_test,\n",
    "                model_class=FairRiskPredictor,\n",
    "                input_dim=feats_train.shape[1],\n",
    "                alpha=alpha, Q=config.Q,\n",
    "                lambda_fair=lam, fairness_type=fairness,\n",
    "                group=group, grad_method=grad_method,\n",
    "                num_epochs=config.num_epochs,\n",
    "                print_results=False, loss_type='mse',\n",
    "                **selected_hparams\n",
    "            )\n",
    "\n",
    "            avg_results, _ = train_many_trials_regret(n_trials=config.n_trials, **train_args)\n",
    "            row = {**run_params, **avg_results}\n",
    "            results_list.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    process_and_display_results(results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# ===================================================================\n",
    "# 4. SCRIPT ENTRY POINT\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    # Create a configuration object\n",
    "    exp_config = ExperimentConfig()\n",
    "\n",
    "    # You can easily modify the config for a specific run, for example:\n",
    "    # exp_config.group_settings = [False]\n",
    "    # exp_config.grad_methods = ['finite-diff']\n",
    "\n",
    "    # Run the experiments\n",
    "    results = run_experiments(exp_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818b3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
