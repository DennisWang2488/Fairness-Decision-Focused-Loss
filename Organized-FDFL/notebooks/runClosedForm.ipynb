{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e98b16",
   "metadata": {},
   "source": [
    "1. Add autograd Fairness gradient in backward to closed-form\n",
    "\n",
    "2. Verify formula of MAD and Acc Parity on Group\n",
    "\n",
    "3. Let BETA = ALPHA\n",
    "\n",
    "4. Also report training time\n",
    "\n",
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "\n",
    "6. For Fold-OPT Change PGD closed-form to solver.\n",
    "\n",
    "7. Report fairness value when lambda = 0\n",
    "\n",
    "8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59e5f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "    solveGroupProblem, closed_form_group_alpha, AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_individual_gradient_analytical, compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab506ae",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6aba8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"beta\": 2.5,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 100,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "beta     = params[\"beta\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b5919c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 0.1) \n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e5c64",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "845eab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout_rate),\n",
    "            # nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ef940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa098c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f1760a9",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1dfdeb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "    \"\"\"\n",
    "    Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "    without explicitly forming the full Jacobian matrix J.\n",
    "    Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "    n = len(b)\n",
    "    final_grad = np.zeros(n)\n",
    "\n",
    "    # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "    # This part is identical to the start of the original _grad function\n",
    "    if beta > 1:\n",
    "        gamma = beta - 2 + alpha - alpha * beta\n",
    "        psi_s_exp_factor = (2 - alpha) / gamma\n",
    "    else: # beta < 1\n",
    "        gamma = beta + alpha - alpha * beta\n",
    "        psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "    d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "    unique_groups = np.unique(group_idx)\n",
    "    S, H, Psi = {}, {}, {}\n",
    "    for k in unique_groups:\n",
    "        mask = (group_idx == k)\n",
    "        G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "        S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "        H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "        const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "        if beta > 1:\n",
    "            Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "        else:\n",
    "            Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "    Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "    phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "    # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "    v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "    # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "    for j in range(n):\n",
    "        m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "        # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "        dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "        dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "        dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "        dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "        # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "        # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "        # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "        sum_v_dN_db_j = 0\n",
    "        dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "        # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "        # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "        # This makes the sum sparse and efficient to compute\n",
    "        sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "        sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "        # --- 4. Assemble the final gradient component ---\n",
    "        final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "    return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03c830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8363ef",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ec828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha, beta=beta)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha, beta=beta)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2f983e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([0.1000])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1000])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([12.3363])\n",
      "sol_group: tensor([11.4406])\n",
      "obj_group: tensor([-1190.4932])\n",
      "obj_ind: tensor([-1642.7346])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9ba2b",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57dce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, beta, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha, beta)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha, beta)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha, beta)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha, beta)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=beta, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1becd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegretFairnessLoss(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, beta, lambdas, fairness_type, group, grad_method):\n",
    "        # We need the graph for the fairness penalty part for autograd\n",
    "        _, fairness_penalty_for_grad, d_hat_np = _calculate_loss_and_decision(\n",
    "            pred_r, true_r, gainF, cost, race, Q, alpha, beta, lambdas, fairness_type, group\n",
    "        )\n",
    "        # But the regret part of the loss should be detached\n",
    "        regret_loss, _, _ = _calculate_loss_and_decision(\n",
    "            pred_r.detach(), true_r, gainF, cost, race, Q, alpha, beta, 0, 'none', group\n",
    "        )\n",
    "        # The final loss combines the detached regret with the graph-connected fairness penalty\n",
    "        total_loss = regret_loss + lambdas * fairness_penalty_for_grad\n",
    "\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype) if d_hat_np is not None else None\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'beta': beta, 'lambdas': lambdas, 'fairness_type': fairness_type, 'group': group, 'grad_method': grad_method}\n",
    "        return total_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 11\n",
    "\n",
    "        # PyTorch autograd will automatically handle the gradient for the fairness term.\n",
    "        # We only need to compute and return the gradient for the regret term.\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if params['grad_method'] == 'closed-form':\n",
    "            try:\n",
    "                if params['group']:\n",
    "                    # --- EFFICIENT JVP PATH ---\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'], params['beta'])\n",
    "                    grad_obj_wrt_d_hat_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    v_dot_J = solve_coupled_group_jvp(pred_r_np, cost_np, race_np, params['Q'], params['alpha'], params['beta'], v=grad_obj_wrt_d_hat_np)\n",
    "                    grad_regret = -torch.from_numpy(v_dot_J).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                else:\n",
    "                    # --- Individual case (already efficient) ---\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - alpha) * pred_r ** (-alpha)\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "\n",
    "            except (ValueError, TypeError, np.linalg.LinAlgError) as e:\n",
    "                print(f\"Warning: Closed-form gradient failed: {e}. Returning zero grad for regret.\")\n",
    "\n",
    "        elif params['grad_method'] == 'finite-diff':\n",
    "            epsilon = 1e-5\n",
    "            for i in range(pred_r.numel()):\n",
    "                p_plus, p_minus = pred_r.detach().clone(), pred_r.detach().clone()\n",
    "                p_plus[i] += epsilon; p_minus[i] -= epsilon\n",
    "                loss_plus, _, _ = _calculate_loss_and_decision(p_plus, true_r, gainF, cost, race, **params)\n",
    "                loss_minus, _, _ = _calculate_loss_and_decision(p_minus, true_r, gainF, cost, race, **params)\n",
    "                grad_regret[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            # This numerically approximates the gradient of the *entire* loss.\n",
    "            # We return it directly, as autograd won't have a better value.\n",
    "            return (grad_output * grad_regret, None, None, None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "        # The final gradient passed to the optimizer is the sum of the autograd part (fairness)\n",
    "        # and our manually computed part (regret). PyTorch handles this addition automatically.\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ===== 3. Final nn.Module Wrapper\n",
    "# ==============================================================================\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    def __init__(self, Q, alpha, beta, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.beta, self.lambdas = Q, alpha, beta, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        return RegretFairnessLoss.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.beta, self.lambdas, self.fairness_type, self.group, self.grad_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629342b2",
   "metadata": {},
   "source": [
    "# Training Gemini Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c1c3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, beta, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor using the custom RegretFairnessLoss.\n",
    "    \"\"\"\n",
    "    # -------------------------- Tensors and Device ---------------------\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train,\n",
    "               X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, \\\n",
    "    X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device)\n",
    "        for t in tensors\n",
    "    ]\n",
    "\n",
    "    # -------------------------- Dataloaders ------------------------\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # -------------------------- Model, Optimizer, and Loss ---------------\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, beta, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # -------------------------- Logs -------------------------------\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "\n",
    "    # -------------------------- Training Loop ----------------------\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze()\n",
    "            pred_b = torch.clamp(pred_b, min=1e-4) # Ensure predictions are positive\n",
    "\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # ----------------- Evaluation on the held-out set ----------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_test = model(X_test).squeeze().clamp(min=1e-4)\n",
    "            mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "            mse_log.append(mse_val)\n",
    "\n",
    "            # --- Calculate Regret on Test Set ---\n",
    "            _, _, d_pred_np = _calculate_loss_and_decision(\n",
    "                pred_test, y_test, gainF_test, cost_test, race_test,\n",
    "                Q, alpha, beta, 0, 'none', group\n",
    "            )\n",
    "            _, _, d_true_np = _calculate_loss_and_decision(\n",
    "                y_test, y_test, gainF_test, cost_test, race_test,\n",
    "                Q, alpha, beta, 0, 'none', group\n",
    "            )\n",
    "            \n",
    "            if d_pred_np is not None and d_true_np is not None:\n",
    "                if group:\n",
    "                    true_obj = compute_coupled_group_obj(d_true_np, to_numpy_1d(y_test), to_numpy_1d(race_test), alpha, beta)\n",
    "                    pred_obj = compute_coupled_group_obj(d_pred_np, to_numpy_1d(y_test), to_numpy_1d(race_test), alpha, beta)\n",
    "                else:\n",
    "                    true_obj = AlphaFairness(to_numpy_1d(y_test) * d_true_np, alpha)\n",
    "                    pred_obj = AlphaFairness(to_numpy_1d(y_test) * d_pred_np, alpha)\n",
    "                norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "            else:\n",
    "                norm_regret = np.nan # Solver failed\n",
    "            regret_log.append(norm_regret)\n",
    "\n",
    "\n",
    "            # --- Calculate Fairness on Test Set ---\n",
    "            fair_val = 0.0\n",
    "            if fairness_type != 'none':\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=beta, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "            fairness_log.append(fair_val)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:02d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_val:.4f} | Regret {norm_regret:.4f} | Fair {fair_val:.4f}\")\n",
    "\n",
    "    return model, {\"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6d29453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Train-Loss 7332.9580 | Test-MSE 346.3866 | Regret 0.3142 | Fair 0.6579\n",
      "Epoch 10/50 | Train-Loss 6683.3926 | Test-MSE 343.6100 | Regret 0.2863 | Fair 0.6592\n",
      "Epoch 20/50 | Train-Loss 6130.1113 | Test-MSE 340.2187 | Regret 0.2639 | Fair 0.6603\n",
      "Epoch 30/50 | Train-Loss 5745.0234 | Test-MSE 337.0707 | Regret 0.2497 | Fair 0.6608\n",
      "Epoch 40/50 | Train-Loss 5487.3164 | Test-MSE 334.3893 | Regret 0.2410 | Fair 0.6610\n",
      "Epoch 50/50 | Train-Loss 5314.2559 | Test-MSE 332.1207 | Regret 0.2352 | Fair 0.6610\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"alpha\": 0.5,\n",
    "    \"beta\": 0.5,\n",
    "    \"Q\": Q,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"atkinson\",   \n",
    "    \"group\": False,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"closed-form\",\n",
    "    \"num_epochs\": 50,        \n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ee6c5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Train-Loss 6295.5972 | Test-MSE 346.1301 | Regret 5.3562 | Fair 0.0204\n",
      "Epoch 10/50 | Train-Loss 5209.8638 | Test-MSE 343.1317 | Regret 4.4941 | Fair 0.0201\n",
      "Epoch 20/50 | Train-Loss 4566.1587 | Test-MSE 339.7930 | Regret 3.9832 | Fair 0.0198\n",
      "Epoch 30/50 | Train-Loss 4180.6831 | Test-MSE 337.0753 | Regret 3.6535 | Fair 0.0194\n",
      "Epoch 40/50 | Train-Loss 3897.2947 | Test-MSE 334.9794 | Regret 3.4328 | Fair 0.0191\n",
      "Epoch 50/50 | Train-Loss 3672.1292 | Test-MSE 333.2602 | Regret 3.2805 | Fair 0.0187\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"alpha\": alpha,\n",
    "    \"beta\": beta,\n",
    "    \"Q\": Q,\n",
    "    \"lambda_fair\": 1,\n",
    "    \"fairness_type\": \"atkinson\",   \n",
    "    \"group\": True,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"closed-form\",\n",
    "    \"num_epochs\": 50,        \n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1432 | Test-MSE 340.7641 | Regret 0.1190 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.1952 | Test-MSE 316.7935 | Regret 0.0865 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9423 | Test-MSE 311.2867 | Regret 0.0732 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7583 | Test-MSE 307.6521 | Regret 0.0642 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.1837 | Test-MSE 341.7054 | Regret 0.1190 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.2376 | Test-MSE 318.7535 | Regret 0.0915 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9810 | Test-MSE 313.7587 | Regret 0.0756 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7959 | Test-MSE 308.9955 | Regret 0.0655 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.0675 | Test-MSE 340.0304 | Regret 0.1173 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.1914 | Test-MSE 318.1606 | Regret 0.0906 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9286 | Test-MSE 311.3828 | Regret 0.0734 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7630 | Test-MSE 308.2544 | Regret 0.0631 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.7724  |  std = 0.0167\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0265 | Test-MSE 340.7039 | Regret 0.1147 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0151 | Test-MSE 309.8002 | Regret 0.0676 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0110 | Test-MSE 295.9592 | Regret 0.0566 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0088 | Test-MSE 287.7383 | Regret 0.0519 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0273 | Test-MSE 341.7417 | Regret 0.1086 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0155 | Test-MSE 311.8926 | Regret 0.0703 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0114 | Test-MSE 299.9689 | Regret 0.0592 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0091 | Test-MSE 292.0424 | Regret 0.0536 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0257 | Test-MSE 340.0729 | Regret 0.0994 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0150 | Test-MSE 310.6141 | Regret 0.0694 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0110 | Test-MSE 297.8852 | Regret 0.0597 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0089 | Test-MSE 290.3256 | Regret 0.0553 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0089  |  std = 0.0002\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6453 | Test-MSE 340.7068 | Regret 0.1089 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3646 | Test-MSE 316.1238 | Regret 0.0761 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2872 | Test-MSE 310.6775 | Regret 0.0636 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2335 | Test-MSE 307.2025 | Regret 0.0554 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6531 | Test-MSE 341.6537 | Regret 0.1080 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3770 | Test-MSE 318.3612 | Regret 0.0794 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2966 | Test-MSE 313.2743 | Regret 0.0659 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2449 | Test-MSE 308.7423 | Regret 0.0567 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6196 | Test-MSE 340.0304 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3607 | Test-MSE 317.7393 | Regret 0.0781 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2823 | Test-MSE 310.9457 | Regret 0.0640 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2343 | Test-MSE 308.2675 | Regret 0.0551 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.2376  |  std = 0.0052\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4521 | Test-MSE 340.6559 | Regret 0.0335 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2723 | Test-MSE 309.7896 | Regret 0.0211 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2108 | Test-MSE 295.1635 | Regret 0.0175 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1750 | Test-MSE 287.8890 | Regret 0.0149 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4579 | Test-MSE 341.6232 | Regret 0.0319 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2804 | Test-MSE 311.7780 | Regret 0.0218 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2179 | Test-MSE 299.2923 | Regret 0.0181 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1817 | Test-MSE 291.6497 | Regret 0.0152 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4371 | Test-MSE 339.9308 | Regret 0.0300 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2717 | Test-MSE 310.7772 | Regret 0.0214 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2117 | Test-MSE 297.6441 | Regret 0.0178 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1753 | Test-MSE 291.0777 | Regret 0.0150 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.1773  |  std = 0.0031\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2133 | Test-MSE 340.6805 | Regret 0.1571 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1185 | Test-MSE 315.7098 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0926 | Test-MSE 310.2985 | Regret 0.0874 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0755 | Test-MSE 306.9650 | Regret 0.0758 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2150 | Test-MSE 341.6537 | Regret 0.1550 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1224 | Test-MSE 318.1295 | Regret 0.1093 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0952 | Test-MSE 313.0290 | Regret 0.0907 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0790 | Test-MSE 308.7429 | Regret 0.0781 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2036 | Test-MSE 340.0305 | Regret 0.1499 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1165 | Test-MSE 317.5373 | Regret 0.1071 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0907 | Test-MSE 310.9568 | Regret 0.0878 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0754 | Test-MSE 308.3284 | Regret 0.0755 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0766  |  std = 0.0017\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.4945 | Test-MSE 340.6135 | Regret 0.1079 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.0240 | Test-MSE 309.7021 | Regret 0.0699 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 7.8070 | Test-MSE 293.2859 | Regret 0.0573 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.5563 | Test-MSE 286.7802 | Regret 0.0493 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 16.4774 | Test-MSE 341.5659 | Regret 0.1046 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.3664 | Test-MSE 311.7730 | Regret 0.0722 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 8.1187 | Test-MSE 297.5720 | Regret 0.0590 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.7457 | Test-MSE 290.6397 | Regret 0.0504 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 15.8833 | Test-MSE 339.8916 | Regret 0.1000 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.0402 | Test-MSE 310.9011 | Regret 0.0708 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 7.8035 | Test-MSE 296.3110 | Regret 0.0579 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.5140 | Test-MSE 290.2194 | Regret 0.0499 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 6.6053  |  std = 0.1007\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0736 | Test-MSE 340.6805 | Regret 0.3530 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0397 | Test-MSE 315.4717 | Regret 0.2281 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0307 | Test-MSE 310.0607 | Regret 0.1877 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0250 | Test-MSE 306.8338 | Regret 0.1621 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0740 | Test-MSE 341.6537 | Regret 0.3471 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0409 | Test-MSE 318.0392 | Regret 0.2353 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0315 | Test-MSE 312.8951 | Regret 0.1951 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0261 | Test-MSE 308.6572 | Regret 0.1674 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0698 | Test-MSE 340.0305 | Regret 0.3328 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0388 | Test-MSE 317.4532 | Regret 0.2299 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0300 | Test-MSE 311.1502 | Regret 0.1877 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0249 | Test-MSE 308.4918 | Regret 0.1618 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0253  |  std = 0.0005\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6004 | Test-MSE 340.6135 | Regret 0.2221 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 491.6048 | Test-MSE 309.8607 | Regret 0.1438 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 376.4490 | Test-MSE 292.6831 | Regret 0.1175 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 319.5690 | Test-MSE 285.6093 | Regret 0.1031 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 817.3315 | Test-MSE 341.5659 | Regret 0.2167 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 509.3578 | Test-MSE 311.7619 | Regret 0.1480 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 392.2265 | Test-MSE 296.8894 | Regret 0.1205 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 328.6343 | Test-MSE 289.4665 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 789.9996 | Test-MSE 339.8174 | Regret 0.2079 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 492.4255 | Test-MSE 310.9540 | Regret 0.1448 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 376.1085 | Test-MSE 295.6365 | Regret 0.1187 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 317.9721 | Test-MSE 288.8583 | Regret 0.1043 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 322.0585  |  std = 4.6953\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 97.8727 | Test-MSE 340.7641 | Regret 0.1190 | Fair 80.0155\n",
      "Epoch 10/30 | Train-Loss 76.4691 | Test-MSE 316.7935 | Regret 0.0865 | Fair 68.1967\n",
      "Epoch 20/30 | Train-Loss 72.2987 | Test-MSE 311.2867 | Regret 0.0732 | Fair 67.7545\n",
      "Epoch 30/30 | Train-Loss 74.3380 | Test-MSE 307.6521 | Regret 0.0642 | Fair 69.0484\n",
      "Epoch 01/30 | Train-Loss 99.5045 | Test-MSE 341.7054 | Regret 0.1190 | Fair 80.8831\n",
      "Epoch 10/30 | Train-Loss 79.0971 | Test-MSE 318.7535 | Regret 0.0915 | Fair 68.4977\n",
      "Epoch 20/30 | Train-Loss 74.5401 | Test-MSE 313.7587 | Regret 0.0756 | Fair 68.3501\n",
      "Epoch 30/30 | Train-Loss 75.5218 | Test-MSE 308.9955 | Regret 0.0655 | Fair 69.3055\n",
      "Epoch 01/30 | Train-Loss 99.0658 | Test-MSE 340.0304 | Regret 0.1173 | Fair 78.5901\n",
      "Epoch 10/30 | Train-Loss 78.8628 | Test-MSE 318.1606 | Regret 0.0906 | Fair 67.3305\n",
      "Epoch 20/30 | Train-Loss 73.5733 | Test-MSE 311.3828 | Regret 0.0734 | Fair 66.1859\n",
      "Epoch 30/30 | Train-Loss 75.1926 | Test-MSE 308.2544 | Regret 0.0631 | Fair 67.7604\n",
      "[    LOSS] final-epoch mean = 75.0175  |  std = 0.4989\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 68.7048  |  std = 0.6760\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.7559 | Test-MSE 340.7039 | Regret 0.1147 | Fair 80.1184\n",
      "Epoch 10/30 | Train-Loss 70.2425 | Test-MSE 309.8002 | Regret 0.0676 | Fair 65.3703\n",
      "Epoch 20/30 | Train-Loss 58.4445 | Test-MSE 295.9592 | Regret 0.0566 | Fair 57.6398\n",
      "Epoch 30/30 | Train-Loss 53.7673 | Test-MSE 287.7383 | Regret 0.0519 | Fair 56.1501\n",
      "Epoch 01/30 | Train-Loss 97.3481 | Test-MSE 341.7417 | Regret 0.1086 | Fair 80.9350\n",
      "Epoch 10/30 | Train-Loss 73.0384 | Test-MSE 311.8926 | Regret 0.0703 | Fair 66.4532\n",
      "Epoch 20/30 | Train-Loss 61.8746 | Test-MSE 299.9689 | Regret 0.0592 | Fair 59.5035\n",
      "Epoch 30/30 | Train-Loss 56.7909 | Test-MSE 292.0424 | Regret 0.0536 | Fair 58.4140\n",
      "Epoch 01/30 | Train-Loss 97.0240 | Test-MSE 340.0729 | Regret 0.0994 | Fair 78.8321\n",
      "Epoch 10/30 | Train-Loss 71.8026 | Test-MSE 310.6141 | Regret 0.0694 | Fair 65.6564\n",
      "Epoch 20/30 | Train-Loss 60.6693 | Test-MSE 297.8852 | Regret 0.0597 | Fair 59.5198\n",
      "Epoch 30/30 | Train-Loss 56.0607 | Test-MSE 290.3256 | Regret 0.0553 | Fair 58.6787\n",
      "[    LOSS] final-epoch mean = 55.5397  |  std = 1.2882\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 57.7476  |  std = 1.1348\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.3747 | Test-MSE 340.7068 | Regret 0.1089 | Fair 80.0739\n",
      "Epoch 10/30 | Train-Loss 75.8548 | Test-MSE 316.1238 | Regret 0.0761 | Fair 67.5744\n",
      "Epoch 20/30 | Train-Loss 73.2627 | Test-MSE 310.6775 | Regret 0.0636 | Fair 67.3145\n",
      "Epoch 30/30 | Train-Loss 76.4344 | Test-MSE 307.2025 | Regret 0.0554 | Fair 68.9379\n",
      "Epoch 01/30 | Train-Loss 97.9739 | Test-MSE 341.6537 | Regret 0.1080 | Fair 80.8845\n",
      "Epoch 10/30 | Train-Loss 78.7169 | Test-MSE 318.3612 | Regret 0.0794 | Fair 68.3347\n",
      "Epoch 20/30 | Train-Loss 75.6597 | Test-MSE 313.2743 | Regret 0.0659 | Fair 68.2978\n",
      "Epoch 30/30 | Train-Loss 77.3636 | Test-MSE 308.7423 | Regret 0.0567 | Fair 69.4794\n",
      "Epoch 01/30 | Train-Loss 97.6179 | Test-MSE 340.0304 | Regret 0.1054 | Fair 78.5901\n",
      "Epoch 10/30 | Train-Loss 78.5472 | Test-MSE 317.7393 | Regret 0.0781 | Fair 66.9545\n",
      "Epoch 20/30 | Train-Loss 74.7743 | Test-MSE 310.9457 | Regret 0.0640 | Fair 65.9726\n",
      "Epoch 30/30 | Train-Loss 77.1026 | Test-MSE 308.2675 | Regret 0.0551 | Fair 67.7093\n",
      "[    LOSS] final-epoch mean = 76.9669  |  std = 0.3913\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 68.7088  |  std = 0.7405\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.1815 | Test-MSE 340.6559 | Regret 0.0335 | Fair 80.1142\n",
      "Epoch 10/30 | Train-Loss 71.4456 | Test-MSE 309.7896 | Regret 0.0211 | Fair 64.2056\n",
      "Epoch 20/30 | Train-Loss 59.4198 | Test-MSE 295.1635 | Regret 0.0175 | Fair 55.4350\n",
      "Epoch 30/30 | Train-Loss 55.4148 | Test-MSE 287.8890 | Regret 0.0149 | Fair 55.0195\n",
      "Epoch 01/30 | Train-Loss 97.7787 | Test-MSE 341.6232 | Regret 0.0319 | Fair 80.9260\n",
      "Epoch 10/30 | Train-Loss 73.8578 | Test-MSE 311.7780 | Regret 0.0218 | Fair 65.1869\n",
      "Epoch 20/30 | Train-Loss 62.9055 | Test-MSE 299.2923 | Regret 0.0181 | Fair 57.1669\n",
      "Epoch 30/30 | Train-Loss 59.0025 | Test-MSE 291.6497 | Regret 0.0152 | Fair 56.8663\n",
      "Epoch 01/30 | Train-Loss 97.4354 | Test-MSE 339.9308 | Regret 0.0300 | Fair 78.5669\n",
      "Epoch 10/30 | Train-Loss 72.7088 | Test-MSE 310.7772 | Regret 0.0214 | Fair 64.0588\n",
      "Epoch 20/30 | Train-Loss 61.4417 | Test-MSE 297.6441 | Regret 0.0178 | Fair 57.9549\n",
      "Epoch 30/30 | Train-Loss 58.1763 | Test-MSE 291.0777 | Regret 0.0150 | Fair 57.9426\n",
      "[    LOSS] final-epoch mean = 57.5312  |  std = 1.5341\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 56.6095  |  std = 1.2071\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.9427 | Test-MSE 340.6805 | Regret 0.1571 | Fair 80.0949\n",
      "Epoch 10/30 | Train-Loss 75.6314 | Test-MSE 315.7098 | Regret 0.1054 | Fair 66.9394\n",
      "Epoch 20/30 | Train-Loss 73.7400 | Test-MSE 310.2985 | Regret 0.0874 | Fair 66.5511\n",
      "Epoch 30/30 | Train-Loss 77.1148 | Test-MSE 306.9650 | Regret 0.0758 | Fair 68.5329\n",
      "Epoch 01/30 | Train-Loss 97.5358 | Test-MSE 341.6537 | Regret 0.1550 | Fair 80.8845\n",
      "Epoch 10/30 | Train-Loss 78.6093 | Test-MSE 318.1295 | Regret 0.1093 | Fair 68.0290\n",
      "Epoch 20/30 | Train-Loss 76.2550 | Test-MSE 313.0290 | Regret 0.0907 | Fair 67.7599\n",
      "Epoch 30/30 | Train-Loss 77.9645 | Test-MSE 308.7429 | Regret 0.0781 | Fair 69.2088\n",
      "Epoch 01/30 | Train-Loss 97.2019 | Test-MSE 340.0305 | Regret 0.1499 | Fair 78.5902\n",
      "Epoch 10/30 | Train-Loss 78.5651 | Test-MSE 317.5373 | Regret 0.1071 | Fair 66.6270\n",
      "Epoch 20/30 | Train-Loss 75.5554 | Test-MSE 310.9568 | Regret 0.0878 | Fair 65.6754\n",
      "Epoch 30/30 | Train-Loss 77.8731 | Test-MSE 308.3284 | Regret 0.0755 | Fair 67.7562\n",
      "[    LOSS] final-epoch mean = 77.6508  |  std = 0.3808\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 68.4993  |  std = 0.5935\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 112.2239 | Test-MSE 340.6135 | Regret 0.1079 | Fair 80.1107\n",
      "Epoch 10/30 | Train-Loss 81.9478 | Test-MSE 309.7021 | Regret 0.0699 | Fair 63.4320\n",
      "Epoch 20/30 | Train-Loss 67.4527 | Test-MSE 293.2859 | Regret 0.0573 | Fair 53.7726\n",
      "Epoch 30/30 | Train-Loss 62.6560 | Test-MSE 286.7802 | Regret 0.0493 | Fair 52.7241\n",
      "Epoch 01/30 | Train-Loss 113.7982 | Test-MSE 341.5659 | Regret 0.1046 | Fair 80.9170\n",
      "Epoch 10/30 | Train-Loss 84.2585 | Test-MSE 311.7730 | Regret 0.0722 | Fair 64.4054\n",
      "Epoch 20/30 | Train-Loss 71.2187 | Test-MSE 297.5720 | Regret 0.0590 | Fair 55.2862\n",
      "Epoch 30/30 | Train-Loss 66.6287 | Test-MSE 290.6397 | Regret 0.0504 | Fair 54.7893\n",
      "Epoch 01/30 | Train-Loss 112.8817 | Test-MSE 339.8916 | Regret 0.1000 | Fair 78.4000\n",
      "Epoch 10/30 | Train-Loss 83.3007 | Test-MSE 310.9011 | Regret 0.0708 | Fair 62.8102\n",
      "Epoch 20/30 | Train-Loss 69.8314 | Test-MSE 296.3110 | Regret 0.0579 | Fair 56.1629\n",
      "Epoch 30/30 | Train-Loss 65.5695 | Test-MSE 290.2194 | Regret 0.0499 | Fair 56.5277\n",
      "[    LOSS] final-epoch mean = 64.9514  |  std = 1.6797\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 54.6804  |  std = 1.5548\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.8030 | Test-MSE 340.6805 | Regret 0.3530 | Fair 80.0948\n",
      "Epoch 10/30 | Train-Loss 75.5273 | Test-MSE 315.4717 | Regret 0.2281 | Fair 66.4464\n",
      "Epoch 20/30 | Train-Loss 73.9158 | Test-MSE 310.0607 | Regret 0.1877 | Fair 65.8236\n",
      "Epoch 30/30 | Train-Loss 77.2415 | Test-MSE 306.8338 | Regret 0.1621 | Fair 68.0927\n",
      "Epoch 01/30 | Train-Loss 97.3948 | Test-MSE 341.6537 | Regret 0.3471 | Fair 80.8846\n",
      "Epoch 10/30 | Train-Loss 78.5151 | Test-MSE 318.0392 | Regret 0.2353 | Fair 67.7644\n",
      "Epoch 20/30 | Train-Loss 76.5820 | Test-MSE 312.8951 | Regret 0.1951 | Fair 67.1384\n",
      "Epoch 30/30 | Train-Loss 77.8694 | Test-MSE 308.6572 | Regret 0.1674 | Fair 68.6945\n",
      "Epoch 01/30 | Train-Loss 97.0681 | Test-MSE 340.0305 | Regret 0.3328 | Fair 78.5904\n",
      "Epoch 10/30 | Train-Loss 78.5843 | Test-MSE 317.4532 | Regret 0.2299 | Fair 66.5184\n",
      "Epoch 20/30 | Train-Loss 75.9874 | Test-MSE 311.1502 | Regret 0.1877 | Fair 65.8747\n",
      "Epoch 30/30 | Train-Loss 78.1206 | Test-MSE 308.4918 | Regret 0.1618 | Fair 68.4593\n",
      "[    LOSS] final-epoch mean = 77.7438  |  std = 0.3697\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 68.4155  |  std = 0.2476\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 918.3298 | Test-MSE 340.6135 | Regret 0.2221 | Fair 80.1107\n",
      "Epoch 10/30 | Train-Loss 563.9587 | Test-MSE 309.8607 | Regret 0.1438 | Fair 63.2583\n",
      "Epoch 20/30 | Train-Loss 436.3583 | Test-MSE 292.6831 | Regret 0.1175 | Fair 53.2400\n",
      "Epoch 30/30 | Train-Loss 375.5227 | Test-MSE 285.6093 | Regret 0.1031 | Fair 51.4129\n",
      "Epoch 01/30 | Train-Loss 914.6523 | Test-MSE 341.5659 | Regret 0.2167 | Fair 80.9170\n",
      "Epoch 10/30 | Train-Loss 583.5933 | Test-MSE 311.7619 | Regret 0.1480 | Fair 64.1849\n",
      "Epoch 20/30 | Train-Loss 455.5044 | Test-MSE 296.8894 | Regret 0.1205 | Fair 54.6447\n",
      "Epoch 30/30 | Train-Loss 388.1050 | Test-MSE 289.4665 | Regret 0.1054 | Fair 53.5461\n",
      "Epoch 01/30 | Train-Loss 886.9979 | Test-MSE 339.8174 | Regret 0.2079 | Fair 78.4081\n",
      "Epoch 10/30 | Train-Loss 566.4116 | Test-MSE 310.9540 | Regret 0.1448 | Fair 62.5484\n",
      "Epoch 20/30 | Train-Loss 438.4414 | Test-MSE 295.6365 | Regret 0.1187 | Fair 55.1882\n",
      "Epoch 30/30 | Train-Loss 376.4626 | Test-MSE 288.8583 | Regret 0.1043 | Fair 55.2914\n",
      "[    LOSS] final-epoch mean = 380.0301  |  std = 5.7227\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 53.4168  |  std = 1.5860\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 193.6021 | Test-MSE 340.7641 | Regret 0.1190 | Fair 160.0309\n",
      "Epoch 10/30 | Train-Loss 151.7429 | Test-MSE 316.7935 | Regret 0.0865 | Fair 136.3935\n",
      "Epoch 20/30 | Train-Loss 143.6551 | Test-MSE 311.2867 | Regret 0.0732 | Fair 135.5090\n",
      "Epoch 30/30 | Train-Loss 147.9177 | Test-MSE 307.6521 | Regret 0.0642 | Fair 138.0969\n",
      "Epoch 01/30 | Train-Loss 196.8253 | Test-MSE 341.7054 | Regret 0.1190 | Fair 161.7662\n",
      "Epoch 10/30 | Train-Loss 156.9566 | Test-MSE 318.7535 | Regret 0.0915 | Fair 136.9955\n",
      "Epoch 20/30 | Train-Loss 148.0992 | Test-MSE 313.7587 | Regret 0.0756 | Fair 136.7002\n",
      "Epoch 30/30 | Train-Loss 150.2477 | Test-MSE 308.9955 | Regret 0.0655 | Fair 138.6110\n",
      "Epoch 01/30 | Train-Loss 196.0641 | Test-MSE 340.0304 | Regret 0.1173 | Fair 157.1802\n",
      "Epoch 10/30 | Train-Loss 156.5342 | Test-MSE 318.1606 | Regret 0.0906 | Fair 134.6610\n",
      "Epoch 20/30 | Train-Loss 146.2180 | Test-MSE 311.3828 | Regret 0.0734 | Fair 132.3718\n",
      "Epoch 30/30 | Train-Loss 149.6222 | Test-MSE 308.2544 | Regret 0.0631 | Fair 135.5207\n",
      "[    LOSS] final-epoch mean = 149.2625  |  std = 0.9846\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 137.4095  |  std = 1.3520\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.4853 | Test-MSE 340.7039 | Regret 0.1147 | Fair 160.2368\n",
      "Epoch 10/30 | Train-Loss 140.4699 | Test-MSE 309.8002 | Regret 0.0676 | Fair 130.7407\n",
      "Epoch 20/30 | Train-Loss 116.8780 | Test-MSE 295.9592 | Regret 0.0566 | Fair 115.2795\n",
      "Epoch 30/30 | Train-Loss 107.5259 | Test-MSE 287.7383 | Regret 0.0519 | Fair 112.3002\n",
      "Epoch 01/30 | Train-Loss 194.6689 | Test-MSE 341.7417 | Regret 0.1086 | Fair 161.8699\n",
      "Epoch 10/30 | Train-Loss 146.0613 | Test-MSE 311.8926 | Regret 0.0703 | Fair 132.9063\n",
      "Epoch 20/30 | Train-Loss 123.7379 | Test-MSE 299.9689 | Regret 0.0592 | Fair 119.0070\n",
      "Epoch 30/30 | Train-Loss 113.5727 | Test-MSE 292.0424 | Regret 0.0536 | Fair 116.8280\n",
      "Epoch 01/30 | Train-Loss 194.0223 | Test-MSE 340.0729 | Regret 0.0994 | Fair 157.6642\n",
      "Epoch 10/30 | Train-Loss 143.5903 | Test-MSE 310.6141 | Regret 0.0694 | Fair 131.3128\n",
      "Epoch 20/30 | Train-Loss 121.3277 | Test-MSE 297.8852 | Regret 0.0597 | Fair 119.0397\n",
      "Epoch 30/30 | Train-Loss 112.1125 | Test-MSE 290.3256 | Regret 0.0553 | Fair 117.3575\n",
      "[    LOSS] final-epoch mean = 111.0704  |  std = 2.5762\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 115.4952  |  std = 2.2695\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 192.1041 | Test-MSE 340.7068 | Regret 0.1089 | Fair 160.1478\n",
      "Epoch 10/30 | Train-Loss 151.3451 | Test-MSE 316.1238 | Regret 0.0761 | Fair 135.1488\n",
      "Epoch 20/30 | Train-Loss 146.2381 | Test-MSE 310.6775 | Regret 0.0636 | Fair 134.6291\n",
      "Epoch 30/30 | Train-Loss 152.6353 | Test-MSE 307.2025 | Regret 0.0554 | Fair 137.8757\n",
      "Epoch 01/30 | Train-Loss 195.2947 | Test-MSE 341.6537 | Regret 0.1080 | Fair 161.7690\n",
      "Epoch 10/30 | Train-Loss 157.0568 | Test-MSE 318.3612 | Regret 0.0794 | Fair 136.6693\n",
      "Epoch 20/30 | Train-Loss 151.0229 | Test-MSE 313.2743 | Regret 0.0659 | Fair 136.5955\n",
      "Epoch 30/30 | Train-Loss 154.4823 | Test-MSE 308.7423 | Regret 0.0567 | Fair 138.9587\n",
      "Epoch 01/30 | Train-Loss 194.6162 | Test-MSE 340.0304 | Regret 0.1054 | Fair 157.1802\n",
      "Epoch 10/30 | Train-Loss 156.7337 | Test-MSE 317.7393 | Regret 0.0781 | Fair 133.9089\n",
      "Epoch 20/30 | Train-Loss 149.2663 | Test-MSE 310.9457 | Regret 0.0640 | Fair 131.9452\n",
      "Epoch 30/30 | Train-Loss 153.9709 | Test-MSE 308.2675 | Regret 0.0551 | Fair 135.4186\n",
      "[    LOSS] final-epoch mean = 153.6962  |  std = 0.7787\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 137.4177  |  std = 1.4811\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.9109 | Test-MSE 340.6559 | Regret 0.0335 | Fair 160.2284\n",
      "Epoch 10/30 | Train-Loss 142.6190 | Test-MSE 309.7896 | Regret 0.0211 | Fair 128.4113\n",
      "Epoch 20/30 | Train-Loss 118.6289 | Test-MSE 295.1635 | Regret 0.0175 | Fair 110.8701\n",
      "Epoch 30/30 | Train-Loss 110.6546 | Test-MSE 287.8890 | Regret 0.0149 | Fair 110.0389\n",
      "Epoch 01/30 | Train-Loss 195.0995 | Test-MSE 341.6232 | Regret 0.0319 | Fair 161.8520\n",
      "Epoch 10/30 | Train-Loss 147.4353 | Test-MSE 311.7780 | Regret 0.0218 | Fair 130.3737\n",
      "Epoch 20/30 | Train-Loss 125.5932 | Test-MSE 299.2923 | Regret 0.0181 | Fair 114.3337\n",
      "Epoch 30/30 | Train-Loss 117.8233 | Test-MSE 291.6497 | Regret 0.0152 | Fair 113.7327\n",
      "Epoch 01/30 | Train-Loss 194.4337 | Test-MSE 339.9308 | Regret 0.0300 | Fair 157.1338\n",
      "Epoch 10/30 | Train-Loss 145.1460 | Test-MSE 310.7772 | Regret 0.0214 | Fair 128.1176\n",
      "Epoch 20/30 | Train-Loss 122.6717 | Test-MSE 297.6441 | Regret 0.0178 | Fair 115.9098\n",
      "Epoch 30/30 | Train-Loss 116.1773 | Test-MSE 291.0777 | Regret 0.0150 | Fair 115.8851\n",
      "[    LOSS] final-epoch mean = 114.8851  |  std = 3.0660\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 113.2189  |  std = 2.4142\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.6721 | Test-MSE 340.6805 | Regret 0.1571 | Fair 160.1897\n",
      "Epoch 10/30 | Train-Loss 151.1443 | Test-MSE 315.7098 | Regret 0.1054 | Fair 133.8788\n",
      "Epoch 20/30 | Train-Loss 147.3874 | Test-MSE 310.2985 | Regret 0.0874 | Fair 133.1022\n",
      "Epoch 30/30 | Train-Loss 154.1541 | Test-MSE 306.9650 | Regret 0.0758 | Fair 137.0657\n",
      "Epoch 01/30 | Train-Loss 194.8566 | Test-MSE 341.6537 | Regret 0.1550 | Fair 161.7690\n",
      "Epoch 10/30 | Train-Loss 157.0962 | Test-MSE 318.1295 | Regret 0.1093 | Fair 136.0581\n",
      "Epoch 20/30 | Train-Loss 152.4149 | Test-MSE 313.0290 | Regret 0.0907 | Fair 135.5197\n",
      "Epoch 30/30 | Train-Loss 155.8500 | Test-MSE 308.7429 | Regret 0.0781 | Fair 138.4175\n",
      "Epoch 01/30 | Train-Loss 194.2002 | Test-MSE 340.0305 | Regret 0.1499 | Fair 157.1803\n",
      "Epoch 10/30 | Train-Loss 157.0137 | Test-MSE 317.5373 | Regret 0.1071 | Fair 133.2540\n",
      "Epoch 20/30 | Train-Loss 151.0202 | Test-MSE 310.9568 | Regret 0.0878 | Fair 131.3508\n",
      "Epoch 30/30 | Train-Loss 155.6709 | Test-MSE 308.3284 | Regret 0.0755 | Fair 135.5124\n",
      "[    LOSS] final-epoch mean = 155.2250  |  std = 0.7608\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 136.9985  |  std = 1.1870\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 207.9533 | Test-MSE 340.6135 | Regret 0.1079 | Fair 160.2215\n",
      "Epoch 10/30 | Train-Loss 153.8717 | Test-MSE 309.7021 | Regret 0.0699 | Fair 126.8639\n",
      "Epoch 20/30 | Train-Loss 127.0984 | Test-MSE 293.2859 | Regret 0.0573 | Fair 107.5453\n",
      "Epoch 30/30 | Train-Loss 118.7558 | Test-MSE 286.7802 | Regret 0.0493 | Fair 105.4481\n",
      "Epoch 01/30 | Train-Loss 211.1190 | Test-MSE 341.5659 | Regret 0.1046 | Fair 161.8339\n",
      "Epoch 10/30 | Train-Loss 158.1506 | Test-MSE 311.7730 | Regret 0.0722 | Fair 128.8109\n",
      "Epoch 20/30 | Train-Loss 134.3188 | Test-MSE 297.5720 | Regret 0.0590 | Fair 110.5725\n",
      "Epoch 30/30 | Train-Loss 126.5118 | Test-MSE 290.6397 | Regret 0.0504 | Fair 109.5786\n",
      "Epoch 01/30 | Train-Loss 209.8800 | Test-MSE 339.8916 | Regret 0.1000 | Fair 156.8000\n",
      "Epoch 10/30 | Train-Loss 156.5612 | Test-MSE 310.9011 | Regret 0.0708 | Fair 125.6205\n",
      "Epoch 20/30 | Train-Loss 131.8593 | Test-MSE 296.3110 | Regret 0.0579 | Fair 112.3259\n",
      "Epoch 30/30 | Train-Loss 124.6250 | Test-MSE 290.2194 | Regret 0.0499 | Fair 113.0555\n",
      "[    LOSS] final-epoch mean = 123.2975  |  std = 3.3026\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 109.3607  |  std = 3.1095\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.5324 | Test-MSE 340.6805 | Regret 0.3530 | Fair 160.1897\n",
      "Epoch 10/30 | Train-Loss 151.0149 | Test-MSE 315.4717 | Regret 0.2281 | Fair 132.8928\n",
      "Epoch 20/30 | Train-Loss 147.8009 | Test-MSE 310.0607 | Regret 0.1877 | Fair 131.6472\n",
      "Epoch 30/30 | Train-Loss 154.4579 | Test-MSE 306.8338 | Regret 0.1621 | Fair 136.1854\n",
      "Epoch 01/30 | Train-Loss 194.7156 | Test-MSE 341.6537 | Regret 0.3471 | Fair 161.7692\n",
      "Epoch 10/30 | Train-Loss 156.9893 | Test-MSE 318.0392 | Regret 0.2353 | Fair 135.5287\n",
      "Epoch 20/30 | Train-Loss 153.1324 | Test-MSE 312.8951 | Regret 0.1951 | Fair 134.2769\n",
      "Epoch 30/30 | Train-Loss 155.7127 | Test-MSE 308.6572 | Regret 0.1674 | Fair 137.3889\n",
      "Epoch 01/30 | Train-Loss 194.0665 | Test-MSE 340.0305 | Regret 0.3328 | Fair 157.1807\n",
      "Epoch 10/30 | Train-Loss 157.1298 | Test-MSE 317.4532 | Regret 0.2299 | Fair 133.0367\n",
      "Epoch 20/30 | Train-Loss 151.9448 | Test-MSE 311.1502 | Regret 0.1877 | Fair 131.7494\n",
      "Epoch 30/30 | Train-Loss 156.2163 | Test-MSE 308.4918 | Regret 0.1618 | Fair 136.9185\n",
      "[    LOSS] final-epoch mean = 155.4623  |  std = 0.7394\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 136.8310  |  std = 0.4952\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 1014.0592 | Test-MSE 340.6135 | Regret 0.2221 | Fair 160.2215\n",
      "Epoch 10/30 | Train-Loss 636.3126 | Test-MSE 309.8607 | Regret 0.1438 | Fair 126.5167\n",
      "Epoch 20/30 | Train-Loss 496.2677 | Test-MSE 292.6831 | Regret 0.1175 | Fair 106.4800\n",
      "Epoch 30/30 | Train-Loss 431.4764 | Test-MSE 285.6093 | Regret 0.1031 | Fair 102.8258\n",
      "Epoch 01/30 | Train-Loss 1011.9731 | Test-MSE 341.5659 | Regret 0.2167 | Fair 161.8339\n",
      "Epoch 10/30 | Train-Loss 657.8289 | Test-MSE 311.7619 | Regret 0.1480 | Fair 128.3698\n",
      "Epoch 20/30 | Train-Loss 518.7823 | Test-MSE 296.8894 | Regret 0.1205 | Fair 109.2895\n",
      "Epoch 30/30 | Train-Loss 447.5757 | Test-MSE 289.4665 | Regret 0.1054 | Fair 107.0922\n",
      "Epoch 01/30 | Train-Loss 983.9962 | Test-MSE 339.8174 | Regret 0.2079 | Fair 156.8162\n",
      "Epoch 10/30 | Train-Loss 640.3977 | Test-MSE 310.9540 | Regret 0.1448 | Fair 125.0969\n",
      "Epoch 20/30 | Train-Loss 500.7744 | Test-MSE 295.6365 | Regret 0.1187 | Fair 110.3764\n",
      "Epoch 30/30 | Train-Loss 434.9532 | Test-MSE 288.8583 | Regret 0.1043 | Fair 110.5829\n",
      "[    LOSS] final-epoch mean = 438.0017  |  std = 6.9170\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 106.8336  |  std = 3.1721\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1490 | Test-MSE 340.7641 | Regret 0.1190 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.1995 | Test-MSE 316.7935 | Regret 0.0865 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.9463 | Test-MSE 311.2867 | Regret 0.0732 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.7627 | Test-MSE 307.6521 | Regret 0.0642 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 2.1896 | Test-MSE 341.7054 | Regret 0.1190 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.2422 | Test-MSE 318.7535 | Regret 0.0915 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.9852 | Test-MSE 313.7587 | Regret 0.0756 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.8004 | Test-MSE 308.9955 | Regret 0.0655 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 2.0734 | Test-MSE 340.0304 | Regret 0.1173 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 1.1959 | Test-MSE 318.1606 | Regret 0.0906 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.9328 | Test-MSE 311.3828 | Regret 0.0734 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.7675 | Test-MSE 308.2544 | Regret 0.0631 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.7768  |  std = 0.0168\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0419 | Test-MSE 340.7039 | Regret 0.1147 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 0.0258 | Test-MSE 309.8002 | Regret 0.0676 | Fair 0.0111\n",
      "Epoch 20/30 | Train-Loss 0.0194 | Test-MSE 295.9592 | Regret 0.0566 | Fair 0.0096\n",
      "Epoch 30/30 | Train-Loss 0.0165 | Test-MSE 287.7383 | Regret 0.0519 | Fair 0.0097\n",
      "Epoch 01/30 | Train-Loss 0.0431 | Test-MSE 341.7417 | Regret 0.1086 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 0.0268 | Test-MSE 311.8926 | Regret 0.0703 | Fair 0.0113\n",
      "Epoch 20/30 | Train-Loss 0.0204 | Test-MSE 299.9689 | Regret 0.0592 | Fair 0.0099\n",
      "Epoch 30/30 | Train-Loss 0.0174 | Test-MSE 292.0424 | Regret 0.0536 | Fair 0.0101\n",
      "Epoch 01/30 | Train-Loss 0.0415 | Test-MSE 340.0729 | Regret 0.0994 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 0.0260 | Test-MSE 310.6141 | Regret 0.0694 | Fair 0.0111\n",
      "Epoch 20/30 | Train-Loss 0.0198 | Test-MSE 297.8852 | Regret 0.0597 | Fair 0.0101\n",
      "Epoch 30/30 | Train-Loss 0.0170 | Test-MSE 290.3256 | Regret 0.0553 | Fair 0.0103\n",
      "[    LOSS] final-epoch mean = 0.0170  |  std = 0.0004\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 0.0100  |  std = 0.0003\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6511 | Test-MSE 340.7068 | Regret 0.1089 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.3690 | Test-MSE 316.1238 | Regret 0.0761 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.2915 | Test-MSE 310.6775 | Regret 0.0636 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.2382 | Test-MSE 307.2025 | Regret 0.0554 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 0.6590 | Test-MSE 341.6537 | Regret 0.1080 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.3817 | Test-MSE 318.3612 | Regret 0.0794 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.3010 | Test-MSE 313.2743 | Regret 0.0659 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.2496 | Test-MSE 308.7423 | Regret 0.0567 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 0.6255 | Test-MSE 340.0304 | Regret 0.1054 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.3653 | Test-MSE 317.7393 | Regret 0.0781 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.2866 | Test-MSE 310.9457 | Regret 0.0640 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.2390 | Test-MSE 308.2675 | Regret 0.0551 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.2423  |  std = 0.0052\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4675 | Test-MSE 340.6559 | Regret 0.0335 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 0.2833 | Test-MSE 309.7896 | Regret 0.0211 | Fair 0.0108\n",
      "Epoch 20/30 | Train-Loss 0.2196 | Test-MSE 295.1635 | Regret 0.0175 | Fair 0.0090\n",
      "Epoch 30/30 | Train-Loss 0.1831 | Test-MSE 287.8890 | Regret 0.0149 | Fair 0.0093\n",
      "Epoch 01/30 | Train-Loss 0.4737 | Test-MSE 341.6232 | Regret 0.0319 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 0.2920 | Test-MSE 311.7780 | Regret 0.0218 | Fair 0.0109\n",
      "Epoch 20/30 | Train-Loss 0.2273 | Test-MSE 299.2923 | Regret 0.0181 | Fair 0.0093\n",
      "Epoch 30/30 | Train-Loss 0.1906 | Test-MSE 291.6497 | Regret 0.0152 | Fair 0.0096\n",
      "Epoch 01/30 | Train-Loss 0.4529 | Test-MSE 339.9308 | Regret 0.0300 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 0.2829 | Test-MSE 310.7772 | Regret 0.0214 | Fair 0.0107\n",
      "Epoch 20/30 | Train-Loss 0.2208 | Test-MSE 297.6441 | Regret 0.0178 | Fair 0.0096\n",
      "Epoch 30/30 | Train-Loss 0.1839 | Test-MSE 291.0777 | Regret 0.0150 | Fair 0.0100\n",
      "[    LOSS] final-epoch mean = 0.1859  |  std = 0.0034\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 0.0096  |  std = 0.0003\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2190 | Test-MSE 340.6805 | Regret 0.1571 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.1229 | Test-MSE 315.7098 | Regret 0.1054 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.0969 | Test-MSE 310.2985 | Regret 0.0874 | Fair 0.0042\n",
      "Epoch 30/30 | Train-Loss 0.0803 | Test-MSE 306.9650 | Regret 0.0758 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.2209 | Test-MSE 341.6537 | Regret 0.1550 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.1270 | Test-MSE 318.1295 | Regret 0.1093 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.0997 | Test-MSE 313.0290 | Regret 0.0907 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.0838 | Test-MSE 308.7429 | Regret 0.0781 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.2095 | Test-MSE 340.0305 | Regret 0.1499 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.1212 | Test-MSE 317.5373 | Regret 0.1071 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0952 | Test-MSE 310.9568 | Regret 0.0878 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0802 | Test-MSE 308.3284 | Regret 0.0755 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.0815  |  std = 0.0017\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.5099 | Test-MSE 340.6135 | Regret 0.1079 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 10.0353 | Test-MSE 309.7021 | Regret 0.0699 | Fair 0.0105\n",
      "Epoch 20/30 | Train-Loss 7.8160 | Test-MSE 293.2859 | Regret 0.0573 | Fair 0.0086\n",
      "Epoch 30/30 | Train-Loss 6.5648 | Test-MSE 286.7802 | Regret 0.0493 | Fair 0.0087\n",
      "Epoch 01/30 | Train-Loss 16.4932 | Test-MSE 341.5659 | Regret 0.1046 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 10.3781 | Test-MSE 311.7730 | Regret 0.0722 | Fair 0.0107\n",
      "Epoch 20/30 | Train-Loss 8.1284 | Test-MSE 297.5720 | Regret 0.0590 | Fair 0.0088\n",
      "Epoch 30/30 | Train-Loss 6.7550 | Test-MSE 290.6397 | Regret 0.0504 | Fair 0.0091\n",
      "Epoch 01/30 | Train-Loss 15.8991 | Test-MSE 339.8916 | Regret 0.1000 | Fair 0.0130\n",
      "Epoch 10/30 | Train-Loss 10.0517 | Test-MSE 310.9011 | Regret 0.0708 | Fair 0.0103\n",
      "Epoch 20/30 | Train-Loss 7.8130 | Test-MSE 296.3110 | Regret 0.0579 | Fair 0.0092\n",
      "Epoch 30/30 | Train-Loss 6.5230 | Test-MSE 290.2194 | Regret 0.0499 | Fair 0.0096\n",
      "[    LOSS] final-epoch mean = 6.6143  |  std = 0.1010\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 0.0091  |  std = 0.0004\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0794 | Test-MSE 340.6805 | Regret 0.3530 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.0441 | Test-MSE 315.4717 | Regret 0.2281 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0350 | Test-MSE 310.0607 | Regret 0.1877 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0298 | Test-MSE 306.8338 | Regret 0.1621 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.0799 | Test-MSE 341.6537 | Regret 0.3471 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.0455 | Test-MSE 318.0392 | Regret 0.2353 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.0361 | Test-MSE 312.8951 | Regret 0.1951 | Fair 0.0042\n",
      "Epoch 30/30 | Train-Loss 0.0310 | Test-MSE 308.6572 | Regret 0.1674 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.0757 | Test-MSE 340.0305 | Regret 0.3328 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.0435 | Test-MSE 317.4532 | Regret 0.2299 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0345 | Test-MSE 311.1502 | Regret 0.1877 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0298 | Test-MSE 308.4918 | Regret 0.1618 | Fair 0.0045\n",
      "[    LOSS] final-epoch mean = 0.0302  |  std = 0.0005\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6158 | Test-MSE 340.6135 | Regret 0.2221 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 491.6162 | Test-MSE 309.8607 | Regret 0.1438 | Fair 0.0105\n",
      "Epoch 20/30 | Train-Loss 376.4582 | Test-MSE 292.6831 | Regret 0.1175 | Fair 0.0085\n",
      "Epoch 30/30 | Train-Loss 319.5775 | Test-MSE 285.6093 | Regret 0.1031 | Fair 0.0083\n",
      "Epoch 01/30 | Train-Loss 817.3474 | Test-MSE 341.5659 | Regret 0.2167 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 509.3696 | Test-MSE 311.7619 | Regret 0.1480 | Fair 0.0106\n",
      "Epoch 20/30 | Train-Loss 392.2364 | Test-MSE 296.8894 | Regret 0.1205 | Fair 0.0087\n",
      "Epoch 30/30 | Train-Loss 328.6436 | Test-MSE 289.4665 | Regret 0.1054 | Fair 0.0088\n",
      "Epoch 01/30 | Train-Loss 790.0154 | Test-MSE 339.8174 | Regret 0.2079 | Fair 0.0130\n",
      "Epoch 10/30 | Train-Loss 492.4372 | Test-MSE 310.9540 | Regret 0.1448 | Fair 0.0102\n",
      "Epoch 20/30 | Train-Loss 376.1180 | Test-MSE 295.6365 | Regret 0.1187 | Fair 0.0089\n",
      "Epoch 30/30 | Train-Loss 317.9811 | Test-MSE 288.8583 | Regret 0.1043 | Fair 0.0093\n",
      "[    LOSS] final-epoch mean = 322.0674  |  std = 4.6955\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 0.0088  |  std = 0.0004\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1432 | Test-MSE 341.2104 | Regret 0.1196 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.2324 | Test-MSE 325.6695 | Regret 0.0850 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1.0181 | Test-MSE 319.6523 | Regret 0.0775 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.8378 | Test-MSE 312.1354 | Regret 0.0713 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.1837 | Test-MSE 342.7979 | Regret 0.1216 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.3451 | Test-MSE 318.6104 | Regret 0.0993 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1.0867 | Test-MSE 319.1606 | Regret 0.0811 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.8927 | Test-MSE 315.5461 | Regret 0.0682 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.0675 | Test-MSE 340.2924 | Regret 0.1186 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.3101 | Test-MSE 320.6841 | Regret 0.0976 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1.0298 | Test-MSE 315.5294 | Regret 0.0809 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.8873 | Test-MSE 312.9357 | Regret 0.0715 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.8726  |  std = 0.0247\n",
      "[     MSE] final-epoch mean = 313.5391  |  std = 1.4563\n",
      "[  REGRET] final-epoch mean = 0.0703  |  std = 0.0015\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0265 | Test-MSE 340.8665 | Regret 0.1155 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0157 | Test-MSE 316.0061 | Regret 0.0841 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0118 | Test-MSE 303.1128 | Regret 0.0726 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0096 | Test-MSE 289.5672 | Regret 0.0660 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0273 | Test-MSE 341.9448 | Regret 0.1119 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0160 | Test-MSE 316.5491 | Regret 0.0738 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0125 | Test-MSE 306.3176 | Regret 0.0621 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0101 | Test-MSE 294.9112 | Regret 0.0527 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0257 | Test-MSE 340.8514 | Regret 0.1016 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0159 | Test-MSE 312.7367 | Regret 0.0713 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0125 | Test-MSE 298.9539 | Regret 0.0621 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0104 | Test-MSE 290.1864 | Regret 0.0556 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0100  |  std = 0.0003\n",
      "[     MSE] final-epoch mean = 291.5549  |  std = 2.3866\n",
      "[  REGRET] final-epoch mean = 0.0581  |  std = 0.0057\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6453 | Test-MSE 341.4452 | Regret 0.1099 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3745 | Test-MSE 320.0526 | Regret 0.0781 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.3169 | Test-MSE 312.6628 | Regret 0.0683 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2763 | Test-MSE 315.7146 | Regret 0.0597 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6531 | Test-MSE 342.2649 | Regret 0.1093 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3874 | Test-MSE 323.9145 | Regret 0.0809 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.3189 | Test-MSE 322.9502 | Regret 0.0680 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2839 | Test-MSE 321.7028 | Regret 0.0606 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6196 | Test-MSE 340.3372 | Regret 0.1062 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.4230 | Test-MSE 314.1689 | Regret 0.0909 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.3765 | Test-MSE 304.4076 | Regret 0.0834 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.3077 | Test-MSE 305.6761 | Regret 0.0686 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.2893  |  std = 0.0134\n",
      "[     MSE] final-epoch mean = 314.3645  |  std = 6.6122\n",
      "[  REGRET] final-epoch mean = 0.0630  |  std = 0.0040\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4521 | Test-MSE 340.9657 | Regret 0.0342 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2931 | Test-MSE 313.2625 | Regret 0.0254 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2416 | Test-MSE 297.8157 | Regret 0.0216 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1959 | Test-MSE 287.6530 | Regret 0.0174 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4579 | Test-MSE 341.7667 | Regret 0.0326 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3012 | Test-MSE 310.0309 | Regret 0.0234 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2453 | Test-MSE 297.4942 | Regret 0.0192 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2192 | Test-MSE 299.5959 | Regret 0.0177 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4371 | Test-MSE 340.1520 | Regret 0.0302 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2852 | Test-MSE 317.4822 | Regret 0.0222 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2350 | Test-MSE 307.8093 | Regret 0.0186 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1841 | Test-MSE 288.2469 | Regret 0.0150 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.1997  |  std = 0.0146\n",
      "[     MSE] final-epoch mean = 291.8319  |  std = 5.4953\n",
      "[  REGRET] final-epoch mean = 0.0167  |  std = 0.0012\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2133 | Test-MSE 341.4120 | Regret 0.1614 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1216 | Test-MSE 323.7880 | Regret 0.1099 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0984 | Test-MSE 314.2408 | Regret 0.0925 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0846 | Test-MSE 313.5186 | Regret 0.0816 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2150 | Test-MSE 342.1200 | Regret 0.1566 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1295 | Test-MSE 324.0017 | Regret 0.1119 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.1048 | Test-MSE 321.0536 | Regret 0.0956 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0849 | Test-MSE 314.2905 | Regret 0.0813 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2036 | Test-MSE 340.0954 | Regret 0.1506 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1228 | Test-MSE 325.4027 | Regret 0.1102 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0964 | Test-MSE 314.7726 | Regret 0.0928 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0925 | Test-MSE 302.8673 | Regret 0.0918 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0874  |  std = 0.0037\n",
      "[     MSE] final-epoch mean = 310.2255  |  std = 5.2125\n",
      "[  REGRET] final-epoch mean = 0.0849  |  std = 0.0049\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.4945 | Test-MSE 343.7913 | Regret 0.1173 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 12.6401 | Test-MSE 333.2577 | Regret 0.0931 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 10.2446 | Test-MSE 318.7173 | Regret 0.0749 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 8.7576 | Test-MSE 299.0812 | Regret 0.0658 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 16.4774 | Test-MSE 342.2797 | Regret 0.1081 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 11.4152 | Test-MSE 318.9473 | Regret 0.0828 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 11.1998 | Test-MSE 296.5595 | Regret 0.0834 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 9.0768 | Test-MSE 289.5814 | Regret 0.0664 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 15.8833 | Test-MSE 341.4805 | Regret 0.1047 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.9666 | Test-MSE 320.5132 | Regret 0.0786 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 9.9115 | Test-MSE 316.7787 | Regret 0.0711 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 8.0931 | Test-MSE 302.8413 | Regret 0.0587 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 8.6425  |  std = 0.4098\n",
      "[     MSE] final-epoch mean = 297.1679  |  std = 5.5798\n",
      "[  REGRET] final-epoch mean = 0.0636  |  std = 0.0035\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0736 | Test-MSE 341.4491 | Regret 0.3634 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0415 | Test-MSE 325.9968 | Regret 0.2356 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0337 | Test-MSE 313.2637 | Regret 0.2026 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0323 | Test-MSE 303.3290 | Regret 0.2027 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0740 | Test-MSE 342.1490 | Regret 0.3508 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0458 | Test-MSE 317.3567 | Regret 0.2642 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0380 | Test-MSE 310.0402 | Regret 0.2290 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0339 | Test-MSE 305.0835 | Regret 0.2078 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0698 | Test-MSE 342.4200 | Regret 0.3507 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0429 | Test-MSE 322.1830 | Regret 0.2462 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0355 | Test-MSE 321.0208 | Regret 0.2114 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0297 | Test-MSE 319.6953 | Regret 0.1836 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0320  |  std = 0.0017\n",
      "[     MSE] final-epoch mean = 309.3693  |  std = 7.3367\n",
      "[  REGRET] final-epoch mean = 0.1980  |  std = 0.0104\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6004 | Test-MSE 340.6135 | Regret 0.2221 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 585.1654 | Test-MSE 327.2297 | Regret 0.1732 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 456.8709 | Test-MSE 307.4642 | Regret 0.1378 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 395.7496 | Test-MSE 300.0164 | Regret 0.1239 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 817.3315 | Test-MSE 341.5536 | Regret 0.2167 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 558.5045 | Test-MSE 321.0413 | Regret 0.1627 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 483.3439 | Test-MSE 307.1490 | Regret 0.1446 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 400.5425 | Test-MSE 298.2517 | Regret 0.1225 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 789.9996 | Test-MSE 339.9859 | Regret 0.2085 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 573.6550 | Test-MSE 317.6758 | Regret 0.1678 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 501.3513 | Test-MSE 301.6686 | Regret 0.1475 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 395.0335 | Test-MSE 296.1650 | Regret 0.1217 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 397.1085  |  std = 2.4457\n",
      "[     MSE] final-epoch mean = 298.1444  |  std = 1.5741\n",
      "[  REGRET] final-epoch mean = 0.1227  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 97.8727 | Test-MSE 342.1938 | Regret 0.1277 | Fair 80.4566\n",
      "Epoch 10/30 | Train-Loss 77.6047 | Test-MSE 323.9360 | Regret 0.1018 | Fair 70.4656\n",
      "Epoch 20/30 | Train-Loss 57.0067 | Test-MSE 303.6687 | Regret 0.1165 | Fair 59.4711\n",
      "Epoch 30/30 | Train-Loss 40.6863 | Test-MSE 286.7998 | Regret 0.1369 | Fair 52.2464\n",
      "Epoch 01/30 | Train-Loss 99.5045 | Test-MSE 346.9723 | Regret 0.1464 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 95.4505 | Test-MSE 345.6913 | Regret 0.1379 | Fair 79.1612\n",
      "Epoch 20/30 | Train-Loss 74.9431 | Test-MSE 333.0984 | Regret 0.1220 | Fair 67.8691\n",
      "Epoch 30/30 | Train-Loss 62.5278 | Test-MSE 326.0279 | Regret 0.1241 | Fair 61.6952\n",
      "Epoch 01/30 | Train-Loss 99.0658 | Test-MSE 342.3260 | Regret 0.1228 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 90.1063 | Test-MSE 336.3700 | Regret 0.1121 | Fair 76.0092\n",
      "Epoch 20/30 | Train-Loss 68.4999 | Test-MSE 318.6961 | Regret 0.1146 | Fair 66.0583\n",
      "Epoch 30/30 | Train-Loss 54.6502 | Test-MSE 311.1558 | Regret 0.1184 | Fair 59.5934\n",
      "[    LOSS] final-epoch mean = 52.6214  |  std = 9.0314\n",
      "[     MSE] final-epoch mean = 307.9945  |  std = 16.1701\n",
      "[  REGRET] final-epoch mean = 0.1265  |  std = 0.0077\n",
      "[FAIRNESS] final-epoch mean = 57.8450  |  std = 4.0507\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.7559 | Test-MSE 342.1938 | Regret 0.1224 | Fair 80.4566\n",
      "Epoch 10/30 | Train-Loss 76.2213 | Test-MSE 324.0952 | Regret 0.0972 | Fair 70.6048\n",
      "Epoch 20/30 | Train-Loss 57.7061 | Test-MSE 311.0562 | Regret 0.0915 | Fair 62.1613\n",
      "Epoch 30/30 | Train-Loss 46.3405 | Test-MSE 307.8468 | Regret 0.0974 | Fair 57.3401\n",
      "Epoch 01/30 | Train-Loss 97.3481 | Test-MSE 346.9723 | Regret 0.1280 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 94.2464 | Test-MSE 347.1475 | Regret 0.1264 | Fair 80.5811\n",
      "Epoch 20/30 | Train-Loss 84.3748 | Test-MSE 340.7111 | Regret 0.1131 | Fair 73.7911\n",
      "Epoch 30/30 | Train-Loss 63.9915 | Test-MSE 326.9634 | Regret 0.0952 | Fair 63.8783\n",
      "Epoch 01/30 | Train-Loss 97.0240 | Test-MSE 342.3260 | Regret 0.1052 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 89.1318 | Test-MSE 339.6898 | Regret 0.1018 | Fair 76.6763\n",
      "Epoch 20/30 | Train-Loss 74.7589 | Test-MSE 332.7543 | Regret 0.0987 | Fair 68.8825\n",
      "Epoch 30/30 | Train-Loss 58.4154 | Test-MSE 317.9472 | Regret 0.0940 | Fair 60.0513\n",
      "[    LOSS] final-epoch mean = 56.2491  |  std = 7.3670\n",
      "[     MSE] final-epoch mean = 317.5858  |  std = 7.8085\n",
      "[  REGRET] final-epoch mean = 0.0955  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 60.4232  |  std = 2.6821\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.3747 | Test-MSE 342.0361 | Regret 0.1158 | Fair 80.3722\n",
      "Epoch 10/30 | Train-Loss 76.9935 | Test-MSE 326.5292 | Regret 0.0954 | Fair 71.9351\n",
      "Epoch 20/30 | Train-Loss 57.7497 | Test-MSE 309.1036 | Regret 0.1063 | Fair 63.0662\n",
      "Epoch 30/30 | Train-Loss 43.6661 | Test-MSE 297.7182 | Regret 0.1131 | Fair 55.6557\n",
      "Epoch 01/30 | Train-Loss 97.9739 | Test-MSE 346.9723 | Regret 0.1338 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 94.9467 | Test-MSE 347.1475 | Regret 0.1380 | Fair 80.5811\n",
      "Epoch 20/30 | Train-Loss 84.9937 | Test-MSE 340.4268 | Regret 0.1221 | Fair 73.5729\n",
      "Epoch 30/30 | Train-Loss 66.6126 | Test-MSE 326.9081 | Regret 0.1149 | Fair 64.4669\n",
      "Epoch 01/30 | Train-Loss 97.6179 | Test-MSE 342.3260 | Regret 0.1118 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 89.6999 | Test-MSE 341.2495 | Regret 0.1132 | Fair 77.4524\n",
      "Epoch 20/30 | Train-Loss 79.0752 | Test-MSE 336.6000 | Regret 0.1156 | Fair 71.7688\n",
      "Epoch 30/30 | Train-Loss 60.1345 | Test-MSE 323.3209 | Regret 0.1180 | Fair 62.6428\n",
      "[    LOSS] final-epoch mean = 56.8044  |  std = 9.6593\n",
      "[     MSE] final-epoch mean = 315.9824  |  std = 12.9975\n",
      "[  REGRET] final-epoch mean = 0.1153  |  std = 0.0020\n",
      "[FAIRNESS] final-epoch mean = 60.9218  |  std = 3.7974\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.1815 | Test-MSE 342.1938 | Regret 0.0357 | Fair 80.4566\n",
      "Epoch 10/30 | Train-Loss 76.6844 | Test-MSE 323.8638 | Regret 0.0285 | Fair 70.5428\n",
      "Epoch 20/30 | Train-Loss 53.1325 | Test-MSE 299.8319 | Regret 0.0269 | Fair 58.1932\n",
      "Epoch 30/30 | Train-Loss 35.2970 | Test-MSE 279.0035 | Regret 0.0315 | Fair 49.3231\n",
      "Epoch 01/30 | Train-Loss 97.7787 | Test-MSE 346.9723 | Regret 0.0379 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 91.2887 | Test-MSE 345.3162 | Regret 0.0371 | Fair 79.7132\n",
      "Epoch 20/30 | Train-Loss 78.8466 | Test-MSE 337.8495 | Regret 0.0338 | Fair 73.3891\n",
      "Epoch 30/30 | Train-Loss 55.8388 | Test-MSE 317.4718 | Regret 0.0302 | Fair 62.0295\n",
      "Epoch 01/30 | Train-Loss 97.4354 | Test-MSE 342.3260 | Regret 0.0319 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 89.5300 | Test-MSE 339.9963 | Regret 0.0317 | Fair 76.8432\n",
      "Epoch 20/30 | Train-Loss 69.6506 | Test-MSE 324.8433 | Regret 0.0293 | Fair 66.7494\n",
      "Epoch 30/30 | Train-Loss 53.9496 | Test-MSE 311.2581 | Regret 0.0304 | Fair 60.2343\n",
      "[    LOSS] final-epoch mean = 48.3618  |  std = 9.2704\n",
      "[     MSE] final-epoch mean = 302.5778  |  std = 16.8615\n",
      "[  REGRET] final-epoch mean = 0.0307  |  std = 0.0006\n",
      "[FAIRNESS] final-epoch mean = 57.1956  |  std = 5.6148\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.9427 | Test-MSE 342.1938 | Regret 0.1706 | Fair 80.4566\n",
      "Epoch 10/30 | Train-Loss 76.2542 | Test-MSE 323.9360 | Regret 0.1281 | Fair 70.4656\n",
      "Epoch 20/30 | Train-Loss 55.6018 | Test-MSE 303.3782 | Regret 0.1420 | Fair 59.3962\n",
      "Epoch 30/30 | Train-Loss 43.9448 | Test-MSE 299.7060 | Regret 0.1535 | Fair 54.8130\n",
      "Epoch 01/30 | Train-Loss 97.5358 | Test-MSE 346.9723 | Regret 0.1947 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 91.8673 | Test-MSE 344.1884 | Regret 0.1760 | Fair 78.0636\n",
      "Epoch 20/30 | Train-Loss 77.6681 | Test-MSE 334.5933 | Regret 0.1592 | Fair 70.0446\n",
      "Epoch 30/30 | Train-Loss 56.5972 | Test-MSE 317.1718 | Regret 0.1611 | Fair 59.3510\n",
      "Epoch 01/30 | Train-Loss 97.2019 | Test-MSE 342.3260 | Regret 0.1608 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 89.3041 | Test-MSE 339.6898 | Regret 0.1591 | Fair 76.6763\n",
      "Epoch 20/30 | Train-Loss 74.9361 | Test-MSE 332.7543 | Regret 0.1604 | Fair 68.8825\n",
      "Epoch 30/30 | Train-Loss 58.6051 | Test-MSE 317.9472 | Regret 0.1693 | Fair 60.0513\n",
      "[    LOSS] final-epoch mean = 53.0490  |  std = 6.4896\n",
      "[     MSE] final-epoch mean = 311.6083  |  std = 8.4222\n",
      "[  REGRET] final-epoch mean = 0.1613  |  std = 0.0065\n",
      "[FAIRNESS] final-epoch mean = 58.0718  |  std = 2.3219\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 112.2239 | Test-MSE 341.9536 | Regret 0.1136 | Fair 80.2537\n",
      "Epoch 10/30 | Train-Loss 95.2112 | Test-MSE 330.7295 | Regret 0.1028 | Fair 72.7478\n",
      "Epoch 20/30 | Train-Loss 78.9375 | Test-MSE 322.2581 | Regret 0.0995 | Fair 65.3923\n",
      "Epoch 30/30 | Train-Loss 63.5444 | Test-MSE 311.0320 | Regret 0.0946 | Fair 57.9748\n",
      "Epoch 01/30 | Train-Loss 113.7982 | Test-MSE 346.7743 | Regret 0.1219 | Fair 82.4505\n",
      "Epoch 10/30 | Train-Loss 109.3546 | Test-MSE 344.3565 | Regret 0.1200 | Fair 78.5566\n",
      "Epoch 20/30 | Train-Loss 84.5297 | Test-MSE 324.5427 | Regret 0.1022 | Fair 67.9594\n",
      "Epoch 30/30 | Train-Loss 65.7620 | Test-MSE 303.4876 | Regret 0.1021 | Fair 56.8311\n",
      "Epoch 01/30 | Train-Loss 112.8817 | Test-MSE 342.2285 | Regret 0.1060 | Fair 79.4680\n",
      "Epoch 10/30 | Train-Loss 95.6124 | Test-MSE 330.1188 | Regret 0.0882 | Fair 73.5981\n",
      "Epoch 20/30 | Train-Loss 72.6493 | Test-MSE 311.1914 | Regret 0.0840 | Fair 62.4951\n",
      "Epoch 30/30 | Train-Loss 58.2456 | Test-MSE 305.6740 | Regret 0.0888 | Fair 56.4512\n",
      "[    LOSS] final-epoch mean = 62.5173  |  std = 3.1533\n",
      "[     MSE] final-epoch mean = 306.7312  |  std = 3.1694\n",
      "[  REGRET] final-epoch mean = 0.0952  |  std = 0.0054\n",
      "[FAIRNESS] final-epoch mean = 57.0857  |  std = 0.6475\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.8030 | Test-MSE 342.1938 | Regret 0.3865 | Fair 80.4566\n",
      "Epoch 10/30 | Train-Loss 76.1568 | Test-MSE 323.9360 | Regret 0.2806 | Fair 70.4656\n",
      "Epoch 20/30 | Train-Loss 55.4988 | Test-MSE 303.3782 | Regret 0.3095 | Fair 59.3962\n",
      "Epoch 30/30 | Train-Loss 43.8307 | Test-MSE 299.7060 | Regret 0.3366 | Fair 54.8130\n",
      "Epoch 01/30 | Train-Loss 97.3948 | Test-MSE 346.9723 | Regret 0.4437 | Fair 82.7242\n",
      "Epoch 10/30 | Train-Loss 94.3031 | Test-MSE 347.1475 | Regret 0.4652 | Fair 80.5811\n",
      "Epoch 20/30 | Train-Loss 84.4256 | Test-MSE 340.7111 | Regret 0.4000 | Fair 73.7911\n",
      "Epoch 30/30 | Train-Loss 64.0358 | Test-MSE 326.9634 | Regret 0.3462 | Fair 63.8783\n",
      "Epoch 01/30 | Train-Loss 97.0681 | Test-MSE 342.3260 | Regret 0.3609 | Fair 79.4393\n",
      "Epoch 10/30 | Train-Loss 89.1754 | Test-MSE 339.6898 | Regret 0.3553 | Fair 76.6763\n",
      "Epoch 20/30 | Train-Loss 74.8055 | Test-MSE 332.7543 | Regret 0.3557 | Fair 68.8825\n",
      "Epoch 30/30 | Train-Loss 58.4673 | Test-MSE 317.9472 | Regret 0.3748 | Fair 60.0513\n",
      "[    LOSS] final-epoch mean = 55.4446  |  std = 8.5211\n",
      "[     MSE] final-epoch mean = 314.8722  |  std = 11.3383\n",
      "[  REGRET] final-epoch mean = 0.3525  |  std = 0.0162\n",
      "[FAIRNESS] final-epoch mean = 59.5809  |  std = 3.7158\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 918.3298 | Test-MSE 340.6135 | Regret 0.2221 | Fair 80.1107\n",
      "Epoch 10/30 | Train-Loss 647.8063 | Test-MSE 323.6375 | Regret 0.1671 | Fair 73.3526\n",
      "Epoch 20/30 | Train-Loss 563.3688 | Test-MSE 300.7481 | Regret 0.1532 | Fair 64.8442\n",
      "Epoch 30/30 | Train-Loss 526.9257 | Test-MSE 289.9565 | Regret 0.1405 | Fair 59.6811\n",
      "Epoch 01/30 | Train-Loss 914.6523 | Test-MSE 341.5536 | Regret 0.2167 | Fair 80.9181\n",
      "Epoch 10/30 | Train-Loss 678.0668 | Test-MSE 319.3686 | Regret 0.1776 | Fair 71.9981\n",
      "Epoch 20/30 | Train-Loss 572.8255 | Test-MSE 305.1249 | Regret 0.1472 | Fair 63.6442\n",
      "Epoch 30/30 | Train-Loss 460.0329 | Test-MSE 293.1924 | Regret 0.1236 | Fair 56.1859\n",
      "Epoch 01/30 | Train-Loss 886.9979 | Test-MSE 339.9859 | Regret 0.2085 | Fair 78.4051\n",
      "Epoch 10/30 | Train-Loss 684.9287 | Test-MSE 315.8297 | Regret 0.1786 | Fair 70.7481\n",
      "Epoch 20/30 | Train-Loss 583.8705 | Test-MSE 308.3086 | Regret 0.1487 | Fair 66.5674\n",
      "Epoch 30/30 | Train-Loss 479.5260 | Test-MSE 305.8909 | Regret 0.1255 | Fair 63.6599\n",
      "[    LOSS] final-epoch mean = 488.8282  |  std = 28.0898\n",
      "[     MSE] final-epoch mean = 296.3466  |  std = 6.8769\n",
      "[  REGRET] final-epoch mean = 0.1298  |  std = 0.0076\n",
      "[FAIRNESS] final-epoch mean = 59.8423  |  std = 3.0534\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 193.6021 | Test-MSE 342.1938 | Regret 0.1277 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 157.5699 | Test-MSE 324.6577 | Regret 0.1002 | Fair 143.2457\n",
      "Epoch 20/30 | Train-Loss 118.8008 | Test-MSE 305.9222 | Regret 0.1157 | Fair 124.7793\n",
      "Epoch 30/30 | Train-Loss 84.3320 | Test-MSE 287.7448 | Regret 0.1363 | Fair 107.9240\n",
      "Epoch 01/30 | Train-Loss 196.8253 | Test-MSE 346.9723 | Regret 0.1464 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 181.8038 | Test-MSE 343.9326 | Regret 0.1343 | Fair 154.3951\n",
      "Epoch 20/30 | Train-Loss 154.3698 | Test-MSE 336.8820 | Regret 0.1261 | Fair 137.1817\n",
      "Epoch 30/30 | Train-Loss 119.7558 | Test-MSE 323.9970 | Regret 0.1265 | Fair 118.7169\n",
      "Epoch 01/30 | Train-Loss 196.0641 | Test-MSE 342.3260 | Regret 0.1228 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 185.4709 | Test-MSE 344.8549 | Regret 0.1317 | Fair 157.2449\n",
      "Epoch 20/30 | Train-Loss 178.3125 | Test-MSE 346.0081 | Regret 0.1473 | Fair 151.7508\n",
      "Epoch 30/30 | Train-Loss 142.5608 | Test-MSE 335.5183 | Regret 0.1339 | Fair 130.7809\n",
      "[    LOSS] final-epoch mean = 115.5496  |  std = 23.9571\n",
      "[     MSE] final-epoch mean = 315.7534  |  std = 20.3559\n",
      "[  REGRET] final-epoch mean = 0.1322  |  std = 0.0042\n",
      "[FAIRNESS] final-epoch mean = 119.1406  |  std = 9.3361\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.4853 | Test-MSE 342.1938 | Regret 0.1224 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 158.5334 | Test-MSE 328.2291 | Regret 0.1014 | Fair 145.8366\n",
      "Epoch 20/30 | Train-Loss 137.2581 | Test-MSE 324.2192 | Regret 0.1012 | Fair 137.6695\n",
      "Epoch 30/30 | Train-Loss 111.4424 | Test-MSE 317.0695 | Regret 0.1003 | Fair 128.8418\n",
      "Epoch 01/30 | Train-Loss 194.6689 | Test-MSE 346.9723 | Regret 0.1280 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 179.6723 | Test-MSE 343.5295 | Regret 0.1141 | Fair 153.7173\n",
      "Epoch 20/30 | Train-Loss 136.7379 | Test-MSE 328.5978 | Regret 0.0955 | Fair 131.3364\n",
      "Epoch 30/30 | Train-Loss 113.3363 | Test-MSE 321.9525 | Regret 0.0934 | Fair 119.7557\n",
      "Epoch 01/30 | Train-Loss 194.0223 | Test-MSE 342.3260 | Regret 0.1052 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 183.4297 | Test-MSE 344.8549 | Regret 0.1147 | Fair 157.2449\n",
      "Epoch 20/30 | Train-Loss 175.9902 | Test-MSE 346.0081 | Regret 0.1268 | Fair 151.7508\n",
      "Epoch 30/30 | Train-Loss 140.5620 | Test-MSE 336.1569 | Regret 0.1107 | Fair 131.5963\n",
      "[    LOSS] final-epoch mean = 121.7802  |  std = 13.3032\n",
      "[     MSE] final-epoch mean = 325.0596  |  std = 8.0962\n",
      "[  REGRET] final-epoch mean = 0.1015  |  std = 0.0071\n",
      "[FAIRNESS] final-epoch mean = 126.7313  |  std = 5.0590\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 192.1041 | Test-MSE 342.1938 | Regret 0.1175 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 155.8015 | Test-MSE 325.0432 | Regret 0.0919 | Fair 142.3526\n",
      "Epoch 20/30 | Train-Loss 118.7570 | Test-MSE 307.0225 | Regret 0.1017 | Fair 123.9791\n",
      "Epoch 30/30 | Train-Loss 81.7825 | Test-MSE 287.4794 | Regret 0.1223 | Fair 109.1000\n",
      "Epoch 01/30 | Train-Loss 195.2947 | Test-MSE 346.9723 | Regret 0.1338 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 180.8726 | Test-MSE 344.5628 | Regret 0.1264 | Fair 155.1886\n",
      "Epoch 20/30 | Train-Loss 140.6799 | Test-MSE 331.6188 | Regret 0.1126 | Fair 133.0109\n",
      "Epoch 30/30 | Train-Loss 116.0521 | Test-MSE 325.5388 | Regret 0.1165 | Fair 121.8376\n",
      "Epoch 01/30 | Train-Loss 194.6162 | Test-MSE 342.3260 | Regret 0.1118 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 184.0330 | Test-MSE 344.8549 | Regret 0.1213 | Fair 157.2449\n",
      "Epoch 20/30 | Train-Loss 176.6905 | Test-MSE 346.0081 | Regret 0.1377 | Fair 151.7508\n",
      "Epoch 30/30 | Train-Loss 141.1741 | Test-MSE 335.5183 | Regret 0.1232 | Fair 130.7809\n",
      "[    LOSS] final-epoch mean = 113.0029  |  std = 24.3422\n",
      "[     MSE] final-epoch mean = 316.1789  |  std = 20.6985\n",
      "[  REGRET] final-epoch mean = 0.1207  |  std = 0.0030\n",
      "[FAIRNESS] final-epoch mean = 120.5728  |  std = 8.8963\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.9109 | Test-MSE 342.1938 | Regret 0.0357 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 158.6829 | Test-MSE 327.0399 | Regret 0.0292 | Fair 145.5075\n",
      "Epoch 20/30 | Train-Loss 129.3166 | Test-MSE 314.6246 | Regret 0.0273 | Fair 130.9494\n",
      "Epoch 30/30 | Train-Loss 96.1236 | Test-MSE 301.1956 | Regret 0.0278 | Fair 115.5798\n",
      "Epoch 01/30 | Train-Loss 195.0995 | Test-MSE 346.9723 | Regret 0.0379 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 180.1148 | Test-MSE 343.5294 | Regret 0.0355 | Fair 153.7173\n",
      "Epoch 20/30 | Train-Loss 136.4682 | Test-MSE 327.3726 | Regret 0.0305 | Fair 129.1102\n",
      "Epoch 30/30 | Train-Loss 107.0200 | Test-MSE 315.3932 | Regret 0.0315 | Fair 113.9997\n",
      "Epoch 01/30 | Train-Loss 194.4337 | Test-MSE 342.3260 | Regret 0.0319 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 183.1189 | Test-MSE 344.3011 | Regret 0.0350 | Fair 156.4076\n",
      "Epoch 20/30 | Train-Loss 167.6333 | Test-MSE 341.5497 | Regret 0.0366 | Fair 146.8293\n",
      "Epoch 30/30 | Train-Loss 133.3972 | Test-MSE 329.1352 | Regret 0.0322 | Fair 127.2547\n",
      "[    LOSS] final-epoch mean = 112.1803  |  std = 15.6482\n",
      "[     MSE] final-epoch mean = 315.2413  |  std = 11.4068\n",
      "[  REGRET] final-epoch mean = 0.0305  |  std = 0.0019\n",
      "[FAIRNESS] final-epoch mean = 118.9447  |  std = 5.9113\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.6721 | Test-MSE 342.1938 | Regret 0.1706 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 158.6644 | Test-MSE 328.2291 | Regret 0.1312 | Fair 145.8366\n",
      "Epoch 20/30 | Train-Loss 137.3946 | Test-MSE 324.2192 | Regret 0.1349 | Fair 137.6695\n",
      "Epoch 30/30 | Train-Loss 111.5917 | Test-MSE 317.0695 | Regret 0.1456 | Fair 128.8418\n",
      "Epoch 01/30 | Train-Loss 194.8566 | Test-MSE 346.9723 | Regret 0.1947 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 179.8689 | Test-MSE 343.5295 | Regret 0.1786 | Fair 153.7173\n",
      "Epoch 20/30 | Train-Loss 137.6492 | Test-MSE 328.8304 | Regret 0.1486 | Fair 130.1892\n",
      "Epoch 30/30 | Train-Loss 104.9829 | Test-MSE 314.2044 | Regret 0.1507 | Fair 111.2161\n",
      "Epoch 01/30 | Train-Loss 194.2002 | Test-MSE 342.3260 | Regret 0.1608 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 183.6125 | Test-MSE 344.8549 | Regret 0.1761 | Fair 157.2449\n",
      "Epoch 20/30 | Train-Loss 176.2074 | Test-MSE 346.0081 | Regret 0.2029 | Fair 151.7508\n",
      "Epoch 30/30 | Train-Loss 140.7531 | Test-MSE 335.5183 | Regret 0.1783 | Fair 130.7809\n",
      "[    LOSS] final-epoch mean = 119.1093  |  std = 15.5405\n",
      "[     MSE] final-epoch mean = 322.2641  |  std = 9.4449\n",
      "[  REGRET] final-epoch mean = 0.1582  |  std = 0.0144\n",
      "[FAIRNESS] final-epoch mean = 123.6129  |  std = 8.8015\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 207.9533 | Test-MSE 342.1843 | Regret 0.1146 | Fair 160.9052\n",
      "Epoch 10/30 | Train-Loss 157.7593 | Test-MSE 317.9874 | Regret 0.0839 | Fair 136.8162\n",
      "Epoch 20/30 | Train-Loss 114.6981 | Test-MSE 293.6855 | Regret 0.0913 | Fair 116.4412\n",
      "Epoch 30/30 | Train-Loss 86.2932 | Test-MSE 279.5781 | Regret 0.1028 | Fair 102.5334\n",
      "Epoch 01/30 | Train-Loss 211.1190 | Test-MSE 346.9723 | Regret 0.1240 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 192.7710 | Test-MSE 338.9558 | Regret 0.0991 | Fair 152.6970\n",
      "Epoch 20/30 | Train-Loss 162.2307 | Test-MSE 326.9047 | Regret 0.0872 | Fair 133.6218\n",
      "Epoch 30/30 | Train-Loss 123.6511 | Test-MSE 309.2511 | Regret 0.0833 | Fair 114.7910\n",
      "Epoch 01/30 | Train-Loss 209.8800 | Test-MSE 342.2633 | Regret 0.1060 | Fair 158.9144\n",
      "Epoch 10/30 | Train-Loss 170.2131 | Test-MSE 326.1925 | Regret 0.0850 | Fair 140.3380\n",
      "Epoch 20/30 | Train-Loss 153.9984 | Test-MSE 321.9644 | Regret 0.0863 | Fair 134.3637\n",
      "Epoch 30/30 | Train-Loss 130.7842 | Test-MSE 317.0045 | Regret 0.0902 | Fair 125.8784\n",
      "[    LOSS] final-epoch mean = 113.5761  |  std = 19.5105\n",
      "[     MSE] final-epoch mean = 301.9446  |  std = 16.1291\n",
      "[  REGRET] final-epoch mean = 0.0921  |  std = 0.0081\n",
      "[FAIRNESS] final-epoch mean = 114.4009  |  std = 9.5345\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.5324 | Test-MSE 342.1938 | Regret 0.3865 | Fair 160.9133\n",
      "Epoch 10/30 | Train-Loss 158.5642 | Test-MSE 328.2291 | Regret 0.2889 | Fair 145.8366\n",
      "Epoch 20/30 | Train-Loss 137.2913 | Test-MSE 324.2192 | Regret 0.2962 | Fair 137.6695\n",
      "Epoch 30/30 | Train-Loss 111.4810 | Test-MSE 317.0695 | Regret 0.3186 | Fair 128.8418\n",
      "Epoch 01/30 | Train-Loss 194.7156 | Test-MSE 346.9723 | Regret 0.4437 | Fair 165.4483\n",
      "Epoch 10/30 | Train-Loss 179.7238 | Test-MSE 343.5295 | Regret 0.4041 | Fair 153.7173\n",
      "Epoch 20/30 | Train-Loss 136.7814 | Test-MSE 328.5978 | Regret 0.3412 | Fair 131.3364\n",
      "Epoch 30/30 | Train-Loss 113.3832 | Test-MSE 321.9525 | Regret 0.3568 | Fair 119.7557\n",
      "Epoch 01/30 | Train-Loss 194.0665 | Test-MSE 342.3260 | Regret 0.3609 | Fair 158.8786\n",
      "Epoch 10/30 | Train-Loss 183.4756 | Test-MSE 344.8549 | Regret 0.3989 | Fair 157.2449\n",
      "Epoch 20/30 | Train-Loss 176.0472 | Test-MSE 346.0081 | Regret 0.4660 | Fair 151.7508\n",
      "Epoch 30/30 | Train-Loss 140.6131 | Test-MSE 335.5183 | Regret 0.4016 | Fair 130.7809\n",
      "[    LOSS] final-epoch mean = 121.8258  |  std = 13.3073\n",
      "[     MSE] final-epoch mean = 324.8468  |  std = 7.8048\n",
      "[  REGRET] final-epoch mean = 0.3590  |  std = 0.0339\n",
      "[FAIRNESS] final-epoch mean = 126.4594  |  std = 4.8059\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 1014.0592 | Test-MSE 341.8483 | Regret 0.2330 | Fair 160.3478\n",
      "Epoch 10/30 | Train-Loss 832.4614 | Test-MSE 331.5956 | Regret 0.1922 | Fair 155.8177\n",
      "Epoch 20/30 | Train-Loss 650.7828 | Test-MSE 310.0931 | Regret 0.1527 | Fair 137.6340\n",
      "Epoch 30/30 | Train-Loss 573.8337 | Test-MSE 300.5345 | Regret 0.1357 | Fair 128.3493\n",
      "Epoch 01/30 | Train-Loss 1011.9731 | Test-MSE 341.5536 | Regret 0.2167 | Fair 161.8363\n",
      "Epoch 10/30 | Train-Loss 748.0312 | Test-MSE 311.1880 | Regret 0.1756 | Fair 134.9116\n",
      "Epoch 20/30 | Train-Loss 590.4864 | Test-MSE 302.9000 | Regret 0.1365 | Fair 121.1916\n",
      "Epoch 30/30 | Train-Loss 552.7471 | Test-MSE 301.3903 | Regret 0.1303 | Fair 115.8614\n",
      "Epoch 01/30 | Train-Loss 983.9962 | Test-MSE 339.9859 | Regret 0.2085 | Fair 156.8102\n",
      "Epoch 10/30 | Train-Loss 791.9928 | Test-MSE 330.6432 | Regret 0.1789 | Fair 147.3067\n",
      "Epoch 20/30 | Train-Loss 651.4912 | Test-MSE 316.3365 | Regret 0.1469 | Fair 133.9116\n",
      "Epoch 30/30 | Train-Loss 570.0984 | Test-MSE 305.4500 | Regret 0.1308 | Fair 123.6720\n",
      "[    LOSS] final-epoch mean = 565.5597  |  std = 9.1874\n",
      "[     MSE] final-epoch mean = 302.4583  |  std = 2.1441\n",
      "[  REGRET] final-epoch mean = 0.1323  |  std = 0.0024\n",
      "[FAIRNESS] final-epoch mean = 122.6276  |  std = 5.1514\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1490 | Test-MSE 341.5483 | Regret 0.1201 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.3340 | Test-MSE 320.3091 | Regret 0.0946 | Fair 0.0046\n",
      "Epoch 20/30 | Train-Loss 1.0408 | Test-MSE 318.2567 | Regret 0.0765 | Fair 0.0046\n",
      "Epoch 30/30 | Train-Loss 0.9584 | Test-MSE 317.2480 | Regret 0.0699 | Fair 0.0047\n",
      "Epoch 01/30 | Train-Loss 2.1896 | Test-MSE 342.0192 | Regret 0.1206 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.3056 | Test-MSE 321.2454 | Regret 0.0953 | Fair 0.0045\n",
      "Epoch 20/30 | Train-Loss 1.0985 | Test-MSE 314.7072 | Regret 0.0857 | Fair 0.0045\n",
      "Epoch 30/30 | Train-Loss 0.9482 | Test-MSE 310.7719 | Regret 0.0761 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 2.0734 | Test-MSE 340.2481 | Regret 0.1185 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 1.3385 | Test-MSE 322.9897 | Regret 0.0964 | Fair 0.0044\n",
      "Epoch 20/30 | Train-Loss 1.0599 | Test-MSE 319.1497 | Regret 0.0805 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.9557 | Test-MSE 317.7629 | Regret 0.0731 | Fair 0.0042\n",
      "[    LOSS] final-epoch mean = 0.9541  |  std = 0.0043\n",
      "[     MSE] final-epoch mean = 315.2609  |  std = 3.1812\n",
      "[  REGRET] final-epoch mean = 0.0730  |  std = 0.0025\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0002\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0419 | Test-MSE 342.6043 | Regret 0.1219 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 0.0352 | Test-MSE 337.5031 | Regret 0.1092 | Fair 0.0130\n",
      "Epoch 20/30 | Train-Loss 0.0448 | Test-MSE 345.1829 | Regret 0.1297 | Fair 0.0133\n",
      "Epoch 30/30 | Train-Loss 0.0453 | Test-MSE 344.3852 | Regret 0.1259 | Fair 0.0131\n",
      "Epoch 01/30 | Train-Loss 0.0431 | Test-MSE 346.9257 | Regret 0.1253 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 0.0442 | Test-MSE 347.9048 | Regret 0.1266 | Fair 0.0128\n",
      "Epoch 20/30 | Train-Loss 0.0452 | Test-MSE 348.4095 | Regret 0.1323 | Fair 0.0122\n",
      "Epoch 30/30 | Train-Loss 0.0390 | Test-MSE 343.4208 | Regret 0.1152 | Fair 0.0107\n",
      "Epoch 01/30 | Train-Loss 0.0415 | Test-MSE 340.9282 | Regret 0.1015 | Fair 0.0134\n",
      "Epoch 10/30 | Train-Loss 0.0360 | Test-MSE 337.7292 | Regret 0.0957 | Fair 0.0129\n",
      "Epoch 20/30 | Train-Loss 0.0343 | Test-MSE 333.6408 | Regret 0.0909 | Fair 0.0123\n",
      "Epoch 30/30 | Train-Loss 0.0320 | Test-MSE 323.1346 | Regret 0.0833 | Fair 0.0120\n",
      "[    LOSS] final-epoch mean = 0.0388  |  std = 0.0054\n",
      "[     MSE] final-epoch mean = 336.9802  |  std = 9.7982\n",
      "[  REGRET] final-epoch mean = 0.1081  |  std = 0.0181\n",
      "[FAIRNESS] final-epoch mean = 0.0119  |  std = 0.0010\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6511 | Test-MSE 341.1345 | Regret 0.1108 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.4224 | Test-MSE 331.6849 | Regret 0.0814 | Fair 0.0047\n",
      "Epoch 20/30 | Train-Loss 0.3250 | Test-MSE 319.1698 | Regret 0.0699 | Fair 0.0042\n",
      "Epoch 30/30 | Train-Loss 0.3055 | Test-MSE 321.6709 | Regret 0.0670 | Fair 0.0044\n",
      "Epoch 01/30 | Train-Loss 0.6590 | Test-MSE 344.9332 | Regret 0.1170 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.4176 | Test-MSE 328.6857 | Regret 0.0832 | Fair 0.0045\n",
      "Epoch 20/30 | Train-Loss 0.3529 | Test-MSE 319.8570 | Regret 0.0729 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.3154 | Test-MSE 318.2782 | Regret 0.0658 | Fair 0.0044\n",
      "Epoch 01/30 | Train-Loss 0.6255 | Test-MSE 340.0888 | Regret 0.1059 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.3978 | Test-MSE 328.4244 | Regret 0.0807 | Fair 0.0045\n",
      "Epoch 20/30 | Train-Loss 0.3493 | Test-MSE 325.8105 | Regret 0.0723 | Fair 0.0045\n",
      "Epoch 30/30 | Train-Loss 0.2973 | Test-MSE 315.9486 | Regret 0.0658 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.3061  |  std = 0.0074\n",
      "[     MSE] final-epoch mean = 318.6326  |  std = 2.3495\n",
      "[  REGRET] final-epoch mean = 0.0662  |  std = 0.0006\n",
      "[FAIRNESS] final-epoch mean = 0.0044  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4675 | Test-MSE 341.0433 | Regret 0.0342 | Fair 0.0134\n",
      "Epoch 10/30 | Train-Loss 0.3815 | Test-MSE 336.9629 | Regret 0.0310 | Fair 0.0138\n",
      "Epoch 20/30 | Train-Loss 0.3382 | Test-MSE 329.2426 | Regret 0.0273 | Fair 0.0139\n",
      "Epoch 30/30 | Train-Loss 0.2856 | Test-MSE 320.0410 | Regret 0.0243 | Fair 0.0131\n",
      "Epoch 01/30 | Train-Loss 0.4737 | Test-MSE 346.2901 | Regret 0.0360 | Fair 0.0136\n",
      "Epoch 10/30 | Train-Loss 0.3510 | Test-MSE 328.7457 | Regret 0.0262 | Fair 0.0115\n",
      "Epoch 20/30 | Train-Loss 0.2876 | Test-MSE 305.9023 | Regret 0.0225 | Fair 0.0098\n",
      "Epoch 30/30 | Train-Loss 0.2490 | Test-MSE 297.0576 | Regret 0.0202 | Fair 0.0085\n",
      "Epoch 01/30 | Train-Loss 0.4529 | Test-MSE 340.0545 | Regret 0.0301 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 0.3362 | Test-MSE 324.2964 | Regret 0.0248 | Fair 0.0120\n",
      "Epoch 20/30 | Train-Loss 0.2821 | Test-MSE 316.9411 | Regret 0.0214 | Fair 0.0112\n",
      "Epoch 30/30 | Train-Loss 0.2311 | Test-MSE 294.8515 | Regret 0.0184 | Fair 0.0096\n",
      "[    LOSS] final-epoch mean = 0.2553  |  std = 0.0227\n",
      "[     MSE] final-epoch mean = 303.9833  |  std = 11.3901\n",
      "[  REGRET] final-epoch mean = 0.0210  |  std = 0.0024\n",
      "[FAIRNESS] final-epoch mean = 0.0104  |  std = 0.0020\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2190 | Test-MSE 341.6012 | Regret 0.1616 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.1459 | Test-MSE 332.0343 | Regret 0.1203 | Fair 0.0049\n",
      "Epoch 20/30 | Train-Loss 0.1161 | Test-MSE 317.5394 | Regret 0.1009 | Fair 0.0045\n",
      "Epoch 30/30 | Train-Loss 0.1122 | Test-MSE 314.4807 | Regret 0.0987 | Fair 0.0044\n",
      "Epoch 01/30 | Train-Loss 0.2209 | Test-MSE 346.3847 | Regret 0.1829 | Fair 0.0051\n",
      "Epoch 10/30 | Train-Loss 0.1562 | Test-MSE 332.3893 | Regret 0.1280 | Fair 0.0044\n",
      "Epoch 20/30 | Train-Loss 0.1336 | Test-MSE 326.8937 | Regret 0.1145 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.1226 | Test-MSE 325.8077 | Regret 0.1060 | Fair 0.0040\n",
      "Epoch 01/30 | Train-Loss 0.2095 | Test-MSE 340.0619 | Regret 0.1507 | Fair 0.0049\n",
      "Epoch 10/30 | Train-Loss 0.1541 | Test-MSE 329.6182 | Regret 0.1279 | Fair 0.0046\n",
      "Epoch 20/30 | Train-Loss 0.1478 | Test-MSE 333.2031 | Regret 0.1256 | Fair 0.0044\n",
      "Epoch 30/30 | Train-Loss 0.1453 | Test-MSE 335.3665 | Regret 0.1206 | Fair 0.0046\n",
      "[    LOSS] final-epoch mean = 0.1267  |  std = 0.0138\n",
      "[     MSE] final-epoch mean = 325.2183  |  std = 8.5368\n",
      "[  REGRET] final-epoch mean = 0.1084  |  std = 0.0091\n",
      "[FAIRNESS] final-epoch mean = 0.0043  |  std = 0.0002\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.5099 | Test-MSE 343.6884 | Regret 0.1176 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 11.8099 | Test-MSE 319.0396 | Regret 0.0811 | Fair 0.0120\n",
      "Epoch 20/30 | Train-Loss 9.8907 | Test-MSE 305.5667 | Regret 0.0703 | Fair 0.0107\n",
      "Epoch 30/30 | Train-Loss 8.5123 | Test-MSE 296.1639 | Regret 0.0614 | Fair 0.0096\n",
      "Epoch 01/30 | Train-Loss 16.4932 | Test-MSE 342.2797 | Regret 0.1081 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 11.3046 | Test-MSE 323.4111 | Regret 0.0808 | Fair 0.0124\n",
      "Epoch 20/30 | Train-Loss 9.6307 | Test-MSE 306.1755 | Regret 0.0681 | Fair 0.0116\n",
      "Epoch 30/30 | Train-Loss 8.6031 | Test-MSE 289.1438 | Regret 0.0610 | Fair 0.0101\n",
      "Epoch 01/30 | Train-Loss 15.8991 | Test-MSE 341.4805 | Regret 0.1047 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 11.8250 | Test-MSE 326.9696 | Regret 0.0831 | Fair 0.0126\n",
      "Epoch 20/30 | Train-Loss 9.9554 | Test-MSE 304.8197 | Regret 0.0720 | Fair 0.0119\n",
      "Epoch 30/30 | Train-Loss 8.3560 | Test-MSE 302.1306 | Regret 0.0611 | Fair 0.0115\n",
      "[    LOSS] final-epoch mean = 8.4905  |  std = 0.1021\n",
      "[     MSE] final-epoch mean = 295.8127  |  std = 5.3076\n",
      "[  REGRET] final-epoch mean = 0.0612  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 0.0104  |  std = 0.0008\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0794 | Test-MSE 341.7306 | Regret 0.3651 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.0525 | Test-MSE 330.2115 | Regret 0.2657 | Fair 0.0047\n",
      "Epoch 20/30 | Train-Loss 0.0430 | Test-MSE 318.2271 | Regret 0.2294 | Fair 0.0044\n",
      "Epoch 30/30 | Train-Loss 0.0428 | Test-MSE 305.0643 | Regret 0.2394 | Fair 0.0041\n",
      "Epoch 01/30 | Train-Loss 0.0799 | Test-MSE 346.3267 | Regret 0.4133 | Fair 0.0051\n",
      "Epoch 10/30 | Train-Loss 0.0711 | Test-MSE 342.7817 | Regret 0.3532 | Fair 0.0047\n",
      "Epoch 20/30 | Train-Loss 0.0604 | Test-MSE 335.1848 | Regret 0.3035 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.0530 | Test-MSE 326.2399 | Regret 0.2804 | Fair 0.0039\n",
      "Epoch 01/30 | Train-Loss 0.0757 | Test-MSE 341.0517 | Regret 0.3414 | Fair 0.0049\n",
      "Epoch 10/30 | Train-Loss 0.0681 | Test-MSE 339.5652 | Regret 0.3391 | Fair 0.0048\n",
      "Epoch 20/30 | Train-Loss 0.0768 | Test-MSE 343.8937 | Regret 0.3877 | Fair 0.0048\n",
      "Epoch 30/30 | Train-Loss 0.0759 | Test-MSE 343.1665 | Regret 0.3756 | Fair 0.0047\n",
      "[    LOSS] final-epoch mean = 0.0572  |  std = 0.0138\n",
      "[     MSE] final-epoch mean = 324.8236  |  std = 15.5874\n",
      "[  REGRET] final-epoch mean = 0.2985  |  std = 0.0570\n",
      "[FAIRNESS] final-epoch mean = 0.0042  |  std = 0.0004\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6158 | Test-MSE 340.6135 | Regret 0.2221 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 585.1802 | Test-MSE 327.2297 | Regret 0.1732 | Fair 0.0130\n",
      "Epoch 20/30 | Train-Loss 456.8838 | Test-MSE 307.4642 | Regret 0.1378 | Fair 0.0119\n",
      "Epoch 30/30 | Train-Loss 395.7620 | Test-MSE 300.0164 | Regret 0.1239 | Fair 0.0116\n",
      "Epoch 01/30 | Train-Loss 817.3474 | Test-MSE 341.5536 | Regret 0.2167 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 558.5179 | Test-MSE 321.0413 | Regret 0.1627 | Fair 0.0125\n",
      "Epoch 20/30 | Train-Loss 483.3560 | Test-MSE 307.1490 | Regret 0.1446 | Fair 0.0117\n",
      "Epoch 30/30 | Train-Loss 400.5536 | Test-MSE 298.2517 | Regret 0.1225 | Fair 0.0106\n",
      "Epoch 01/30 | Train-Loss 790.0154 | Test-MSE 339.9859 | Regret 0.2085 | Fair 0.0130\n",
      "Epoch 10/30 | Train-Loss 573.6677 | Test-MSE 317.6758 | Regret 0.1678 | Fair 0.0116\n",
      "Epoch 20/30 | Train-Loss 501.3618 | Test-MSE 301.6686 | Regret 0.1475 | Fair 0.0105\n",
      "Epoch 30/30 | Train-Loss 395.0435 | Test-MSE 296.1650 | Regret 0.1217 | Fair 0.0097\n",
      "[    LOSS] final-epoch mean = 397.1197  |  std = 2.4458\n",
      "[     MSE] final-epoch mean = 298.1444  |  std = 1.5741\n",
      "[  REGRET] final-epoch mean = 0.1227  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 0.0107  |  std = 0.0008\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7011.7090 | Test-MSE 340.7570 | Regret 0.2708 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 4742.1230 | Test-MSE 317.1879 | Regret 0.2145 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 4780.1914 | Test-MSE 308.8528 | Regret 0.2204 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 4787.9082 | Test-MSE 306.1238 | Regret 0.2204 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 7018.1680 | Test-MSE 341.8628 | Regret 0.2676 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 4923.2305 | Test-MSE 319.8243 | Regret 0.2214 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 4955.7129 | Test-MSE 312.2747 | Regret 0.2286 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 4931.2852 | Test-MSE 308.7267 | Regret 0.2275 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 6745.2812 | Test-MSE 340.0435 | Regret 0.2573 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 4869.6855 | Test-MSE 318.9343 | Regret 0.2203 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 4816.1914 | Test-MSE 310.2683 | Regret 0.2210 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 4792.6152 | Test-MSE 307.4250 | Regret 0.2228 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 4837.2695  |  std = 66.5069\n",
      "[     MSE] final-epoch mean = 307.4252  |  std = 1.0626\n",
      "[  REGRET] final-epoch mean = 0.2235  |  std = 0.0030\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 345.2168 | Regret 0.1757 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 347.7406 | Regret -3.0423 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.3568 | Regret -3.1231 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.2707 | Regret -3.1312 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 345.7332 | Regret -2.4435 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 348.1538 | Regret -2.9061 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.0910 | Regret -3.1907 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.4602 | Regret -3.0825 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 344.9396 | Regret -3.2143 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 347.6756 | Regret -3.0053 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.4876 | Regret -3.1229 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.2873 | Regret -3.1241 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "[     MSE] final-epoch mean = 347.3394  |  std = 0.0857\n",
      "[  REGRET] final-epoch mean = -3.1126  |  std = 0.0215\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6006 | Test-MSE 345.9343 | Regret 0.2607 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1040.6389 | Test-MSE 347.8834 | Regret 0.3314 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1173.5344 | Test-MSE 347.5380 | Regret 0.3805 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 1244.5125 | Test-MSE 347.2708 | Regret 0.4138 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 817.3315 | Test-MSE 347.2298 | Regret 0.2600 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1044.7007 | Test-MSE 348.5083 | Regret 0.3277 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1192.7051 | Test-MSE 348.6878 | Regret 0.3833 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 1233.4893 | Test-MSE 348.2719 | Regret 0.4137 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 789.9995 | Test-MSE 346.2032 | Regret 0.2458 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1008.4849 | Test-MSE 348.1180 | Regret 0.3075 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1145.0854 | Test-MSE 348.2359 | Regret 0.3662 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 1219.1458 | Test-MSE 347.9045 | Regret 0.4045 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 1232.3825  |  std = 10.3854\n",
      "[     MSE] final-epoch mean = 347.8157  |  std = 0.4135\n",
      "[  REGRET] final-epoch mean = 0.4107  |  std = 0.0044\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2409.9163 | Test-MSE 344.2051 | Regret 1.5221 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 2429.3821 | Test-MSE 343.5187 | Regret 1.7679 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 2374.5608 | Test-MSE 340.5577 | Regret 2.0580 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 2334.5842 | Test-MSE 338.3372 | Regret 1.9593 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2376.7942 | Test-MSE 345.6972 | Regret 1.5026 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 2374.2656 | Test-MSE 344.0042 | Regret 1.9085 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 2277.3750 | Test-MSE 340.9825 | Regret 2.1346 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 2321.5349 | Test-MSE 339.5481 | Regret 2.1077 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2261.8022 | Test-MSE 344.7876 | Regret 1.3667 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 2364.3315 | Test-MSE 343.8294 | Regret 1.5928 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 2300.6873 | Test-MSE 341.1698 | Regret 2.0394 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 2317.8555 | Test-MSE 339.7560 | Regret 2.1882 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 2324.6582  |  std = 7.1777\n",
      "[     MSE] final-epoch mean = 339.2137  |  std = 0.6256\n",
      "[  REGRET] final-epoch mean = 2.0851  |  std = 0.0948\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7518.6187 | Test-MSE 340.7570 | Regret 0.2708 | Fair 475.6294\n",
      "Epoch 10/30 | Train-Loss 5207.8970 | Test-MSE 317.1879 | Regret 0.2145 | Fair 443.1749\n",
      "Epoch 20/30 | Train-Loss 5230.4429 | Test-MSE 308.8528 | Regret 0.2204 | Fair 430.9527\n",
      "Epoch 30/30 | Train-Loss 5232.3506 | Test-MSE 306.1238 | Regret 0.2204 | Fair 425.8210\n",
      "Epoch 01/30 | Train-Loss 7527.3960 | Test-MSE 341.8628 | Regret 0.2676 | Fair 477.3989\n",
      "Epoch 10/30 | Train-Loss 5393.0513 | Test-MSE 319.8243 | Regret 0.2214 | Fair 446.8394\n",
      "Epoch 20/30 | Train-Loss 5411.0537 | Test-MSE 312.2747 | Regret 0.2286 | Fair 435.5049\n",
      "Epoch 30/30 | Train-Loss 5379.1685 | Test-MSE 308.7267 | Regret 0.2275 | Fair 429.4091\n",
      "Epoch 01/30 | Train-Loss 7253.8892 | Test-MSE 340.0435 | Regret 0.2573 | Fair 475.0849\n",
      "Epoch 10/30 | Train-Loss 5340.0342 | Test-MSE 318.9343 | Regret 0.2203 | Fair 445.6313\n",
      "Epoch 20/30 | Train-Loss 5269.7983 | Test-MSE 310.2683 | Regret 0.2210 | Fair 432.5707\n",
      "Epoch 30/30 | Train-Loss 5238.9326 | Test-MSE 307.4250 | Regret 0.2228 | Fair 427.5624\n",
      "[    LOSS] final-epoch mean = 5283.4839  |  std = 67.7125\n",
      "[     MSE] final-epoch mean = 307.4252  |  std = 1.0626\n",
      "[  REGRET] final-epoch mean = 0.2235  |  std = 0.0030\n",
      "[FAIRNESS] final-epoch mean = 427.5975  |  std = 1.4651\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 506.9095 | Test-MSE 345.2168 | Regret 0.1757 | Fair 481.4425\n",
      "Epoch 10/30 | Train-Loss 514.0344 | Test-MSE 347.7406 | Regret -3.0423 | Fair 484.8758\n",
      "Epoch 20/30 | Train-Loss 513.2349 | Test-MSE 347.3568 | Regret -3.1231 | Fair 484.3970\n",
      "Epoch 30/30 | Train-Loss 512.9160 | Test-MSE 347.2707 | Regret -3.1312 | Fair 484.2633\n",
      "Epoch 01/30 | Train-Loss 509.2281 | Test-MSE 345.7332 | Regret -2.4435 | Fair 482.4005\n",
      "Epoch 10/30 | Train-Loss 514.4103 | Test-MSE 348.1538 | Regret -2.9061 | Fair 485.4096\n",
      "Epoch 20/30 | Train-Loss 512.9197 | Test-MSE 347.0910 | Regret -3.1907 | Fair 484.0065\n",
      "Epoch 30/30 | Train-Loss 513.0735 | Test-MSE 347.4602 | Regret -3.0825 | Fair 484.5656\n",
      "Epoch 01/30 | Train-Loss 508.6079 | Test-MSE 344.9396 | Regret -3.2143 | Fair 481.2773\n",
      "Epoch 10/30 | Train-Loss 513.5325 | Test-MSE 347.6756 | Regret -3.0053 | Fair 484.8084\n",
      "Epoch 20/30 | Train-Loss 513.2054 | Test-MSE 347.4876 | Regret -3.1229 | Fair 484.5465\n",
      "Epoch 30/30 | Train-Loss 513.0938 | Test-MSE 347.2873 | Regret -3.1241 | Fair 484.2779\n",
      "[    LOSS] final-epoch mean = 513.0277  |  std = 0.0795\n",
      "[     MSE] final-epoch mean = 347.3394  |  std = 0.0857\n",
      "[  REGRET] final-epoch mean = -3.1126  |  std = 0.0215\n",
      "[FAIRNESS] final-epoch mean = 484.3689  |  std = 0.1392\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 1329.5100 | Test-MSE 345.9343 | Regret 0.2607 | Fair 482.4054\n",
      "Epoch 10/30 | Train-Loss 1555.1558 | Test-MSE 347.8834 | Regret 0.3314 | Fair 486.4582\n",
      "Epoch 20/30 | Train-Loss 1688.9067 | Test-MSE 347.5380 | Regret 0.3805 | Fair 487.5122\n",
      "Epoch 30/30 | Train-Loss 1759.8171 | Test-MSE 347.2708 | Regret 0.4138 | Fair 487.9358\n",
      "Epoch 01/30 | Train-Loss 1326.5596 | Test-MSE 347.2298 | Regret 0.2600 | Fair 484.3865\n",
      "Epoch 10/30 | Train-Loss 1560.8005 | Test-MSE 348.5083 | Regret 0.3277 | Fair 487.6193\n",
      "Epoch 20/30 | Train-Loss 1709.5471 | Test-MSE 348.6878 | Regret 0.3833 | Fair 489.1482\n",
      "Epoch 30/30 | Train-Loss 1750.0297 | Test-MSE 348.2719 | Regret 0.4137 | Fair 489.2268\n",
      "Epoch 01/30 | Train-Loss 1298.6074 | Test-MSE 346.2032 | Regret 0.2458 | Fair 483.0429\n",
      "Epoch 10/30 | Train-Loss 1524.0493 | Test-MSE 348.1180 | Regret 0.3075 | Fair 486.9980\n",
      "Epoch 20/30 | Train-Loss 1661.2291 | Test-MSE 348.2359 | Regret 0.3662 | Fair 488.4225\n",
      "Epoch 30/30 | Train-Loss 1735.0624 | Test-MSE 347.9045 | Regret 0.4045 | Fair 488.8075\n",
      "[    LOSS] final-epoch mean = 1748.3031  |  std = 10.1796\n",
      "[     MSE] final-epoch mean = 347.8157  |  std = 0.4135\n",
      "[  REGRET] final-epoch mean = 0.4107  |  std = 0.0044\n",
      "[FAIRNESS] final-epoch mean = 488.6567  |  std = 0.5377\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2916.8257 | Test-MSE 344.2051 | Regret 1.5221 | Fair 480.4319\n",
      "Epoch 10/30 | Train-Loss 2938.2278 | Test-MSE 343.5187 | Regret 1.7679 | Fair 480.9047\n",
      "Epoch 20/30 | Train-Loss 2878.7952 | Test-MSE 340.5577 | Regret 2.0580 | Fair 477.1612\n",
      "Epoch 30/30 | Train-Loss 2835.1733 | Test-MSE 338.3372 | Regret 1.9593 | Fair 474.4012\n",
      "Epoch 01/30 | Train-Loss 2886.0222 | Test-MSE 345.6972 | Regret 1.5026 | Fair 482.6898\n",
      "Epoch 10/30 | Train-Loss 2882.9788 | Test-MSE 344.0042 | Regret 1.9085 | Fair 481.4531\n",
      "Epoch 20/30 | Train-Loss 2781.6145 | Test-MSE 340.9825 | Regret 2.1346 | Fair 477.6182\n",
      "Epoch 30/30 | Train-Loss 2823.6958 | Test-MSE 339.5481 | Regret 2.1077 | Fair 476.0284\n",
      "Epoch 01/30 | Train-Loss 2770.4102 | Test-MSE 344.7876 | Regret 1.3667 | Fair 481.4730\n",
      "Epoch 10/30 | Train-Loss 2873.9136 | Test-MSE 343.8294 | Regret 1.5928 | Fair 481.2302\n",
      "Epoch 20/30 | Train-Loss 2804.9978 | Test-MSE 341.1698 | Regret 2.0394 | Fair 478.0592\n",
      "Epoch 30/30 | Train-Loss 2819.8425 | Test-MSE 339.7560 | Regret 2.1882 | Fair 476.4319\n",
      "[    LOSS] final-epoch mean = 2826.2372  |  std = 6.5117\n",
      "[     MSE] final-epoch mean = 339.2137  |  std = 0.6256\n",
      "[  REGRET] final-epoch mean = 2.0851  |  std = 0.0948\n",
      "[FAIRNESS] final-epoch mean = 475.6205  |  std = 0.8778\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7012.3638 | Test-MSE 340.7570 | Regret 0.2708 | Fair 0.6599\n",
      "Epoch 10/30 | Train-Loss 4742.7803 | Test-MSE 317.1879 | Regret 0.2145 | Fair 0.6614\n",
      "Epoch 20/30 | Train-Loss 4780.8462 | Test-MSE 308.8528 | Regret 0.2204 | Fair 0.6599\n",
      "Epoch 30/30 | Train-Loss 4788.5610 | Test-MSE 306.1238 | Regret 0.2204 | Fair 0.6576\n",
      "Epoch 01/30 | Train-Loss 7018.8242 | Test-MSE 341.8628 | Regret 0.2676 | Fair 0.6609\n",
      "Epoch 10/30 | Train-Loss 4923.8882 | Test-MSE 319.8243 | Regret 0.2214 | Fair 0.6619\n",
      "Epoch 20/30 | Train-Loss 4956.3672 | Test-MSE 312.2747 | Regret 0.2286 | Fair 0.6598\n",
      "Epoch 30/30 | Train-Loss 4931.9380 | Test-MSE 308.7267 | Regret 0.2275 | Fair 0.6576\n",
      "Epoch 01/30 | Train-Loss 6745.9385 | Test-MSE 340.0435 | Regret 0.2573 | Fair 0.6617\n",
      "Epoch 10/30 | Train-Loss 4870.3433 | Test-MSE 318.9343 | Regret 0.2203 | Fair 0.6615\n",
      "Epoch 20/30 | Train-Loss 4816.8462 | Test-MSE 310.2683 | Regret 0.2210 | Fair 0.6596\n",
      "Epoch 30/30 | Train-Loss 4793.2681 | Test-MSE 307.4250 | Regret 0.2228 | Fair 0.6575\n",
      "[    LOSS] final-epoch mean = 4837.9224  |  std = 66.5069\n",
      "[     MSE] final-epoch mean = 307.4252  |  std = 1.0626\n",
      "[  REGRET] final-epoch mean = 0.2235  |  std = 0.0030\n",
      "[FAIRNESS] final-epoch mean = 0.6576  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6549 | Test-MSE 345.2168 | Regret 0.1757 | Fair 0.6582\n",
      "Epoch 10/30 | Train-Loss 0.6562 | Test-MSE 347.7406 | Regret -3.0423 | Fair 0.6589\n",
      "Epoch 20/30 | Train-Loss 0.6559 | Test-MSE 347.3568 | Regret -3.1231 | Fair 0.6589\n",
      "Epoch 30/30 | Train-Loss 0.6558 | Test-MSE 347.2707 | Regret -3.1312 | Fair 0.6587\n",
      "Epoch 01/30 | Train-Loss 0.6563 | Test-MSE 345.7332 | Regret -2.4435 | Fair 0.6592\n",
      "Epoch 10/30 | Train-Loss 0.6556 | Test-MSE 348.1538 | Regret -2.9061 | Fair 0.6585\n",
      "Epoch 20/30 | Train-Loss 0.6560 | Test-MSE 347.0910 | Regret -3.1907 | Fair 0.6588\n",
      "Epoch 30/30 | Train-Loss 0.6559 | Test-MSE 347.4602 | Regret -3.0825 | Fair 0.6589\n",
      "Epoch 01/30 | Train-Loss 0.6571 | Test-MSE 344.9396 | Regret -3.2143 | Fair 0.6596\n",
      "Epoch 10/30 | Train-Loss 0.6557 | Test-MSE 347.6756 | Regret -3.0053 | Fair 0.6586\n",
      "Epoch 20/30 | Train-Loss 0.6558 | Test-MSE 347.4876 | Regret -3.1229 | Fair 0.6587\n",
      "Epoch 30/30 | Train-Loss 0.6559 | Test-MSE 347.2873 | Regret -3.1241 | Fair 0.6589\n",
      "[    LOSS] final-epoch mean = 0.6559  |  std = 0.0000\n",
      "[     MSE] final-epoch mean = 347.3394  |  std = 0.0857\n",
      "[  REGRET] final-epoch mean = -3.1126  |  std = 0.0215\n",
      "[FAIRNESS] final-epoch mean = 0.6588  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 823.2555 | Test-MSE 345.9343 | Regret 0.2607 | Fair 0.6577\n",
      "Epoch 10/30 | Train-Loss 1041.2958 | Test-MSE 347.8834 | Regret 0.3314 | Fair 0.6596\n",
      "Epoch 20/30 | Train-Loss 1174.1943 | Test-MSE 347.5380 | Regret 0.3805 | Fair 0.6620\n",
      "Epoch 30/30 | Train-Loss 1245.1733 | Test-MSE 347.2708 | Regret 0.4138 | Fair 0.6630\n",
      "Epoch 01/30 | Train-Loss 817.9879 | Test-MSE 347.2298 | Regret 0.2600 | Fair 0.6587\n",
      "Epoch 10/30 | Train-Loss 1045.3590 | Test-MSE 348.5083 | Regret 0.3277 | Fair 0.6610\n",
      "Epoch 20/30 | Train-Loss 1193.3654 | Test-MSE 348.6878 | Regret 0.3833 | Fair 0.6626\n",
      "Epoch 30/30 | Train-Loss 1234.1501 | Test-MSE 348.2719 | Regret 0.4137 | Fair 0.6631\n",
      "Epoch 01/30 | Train-Loss 790.6566 | Test-MSE 346.2032 | Regret 0.2458 | Fair 0.6592\n",
      "Epoch 10/30 | Train-Loss 1009.1432 | Test-MSE 348.1180 | Regret 0.3075 | Fair 0.6609\n",
      "Epoch 20/30 | Train-Loss 1145.7455 | Test-MSE 348.2359 | Regret 0.3662 | Fair 0.6623\n",
      "Epoch 30/30 | Train-Loss 1219.8066 | Test-MSE 347.9045 | Regret 0.4045 | Fair 0.6633\n",
      "[    LOSS] final-epoch mean = 1233.0434  |  std = 10.3854\n",
      "[     MSE] final-epoch mean = 347.8157  |  std = 0.4135\n",
      "[  REGRET] final-epoch mean = 0.4107  |  std = 0.0044\n",
      "[FAIRNESS] final-epoch mean = 0.6631  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2410.5710 | Test-MSE 344.2051 | Regret 1.5221 | Fair 0.6592\n",
      "Epoch 10/30 | Train-Loss 2430.0422 | Test-MSE 343.5187 | Regret 1.7679 | Fair 0.6628\n",
      "Epoch 20/30 | Train-Loss 2375.2229 | Test-MSE 340.5577 | Regret 2.0580 | Fair 0.6640\n",
      "Epoch 30/30 | Train-Loss 2335.2468 | Test-MSE 338.3372 | Regret 1.9593 | Fair 0.6647\n",
      "Epoch 01/30 | Train-Loss 2377.4504 | Test-MSE 345.6972 | Regret 1.5026 | Fair 0.6602\n",
      "Epoch 10/30 | Train-Loss 2374.9260 | Test-MSE 344.0042 | Regret 1.9085 | Fair 0.6631\n",
      "Epoch 20/30 | Train-Loss 2278.0371 | Test-MSE 340.9825 | Regret 2.1346 | Fair 0.6645\n",
      "Epoch 30/30 | Train-Loss 2322.1978 | Test-MSE 339.5481 | Regret 2.1077 | Fair 0.6651\n",
      "Epoch 01/30 | Train-Loss 2262.4595 | Test-MSE 344.7876 | Regret 1.3667 | Fair 0.6606\n",
      "Epoch 10/30 | Train-Loss 2364.9919 | Test-MSE 343.8294 | Regret 1.5928 | Fair 0.6631\n",
      "Epoch 20/30 | Train-Loss 2301.3494 | Test-MSE 341.1698 | Regret 2.0394 | Fair 0.6645\n",
      "Epoch 30/30 | Train-Loss 2318.5183 | Test-MSE 339.7560 | Regret 2.1882 | Fair 0.6651\n",
      "[    LOSS] final-epoch mean = 2325.3210  |  std = 7.1776\n",
      "[     MSE] final-epoch mean = 339.2137  |  std = 0.6256\n",
      "[  REGRET] final-epoch mean = 2.0851  |  std = 0.0948\n",
      "[FAIRNESS] final-epoch mean = 0.6650  |  std = 0.0002\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7011.7090 | Test-MSE 344.6229 | Regret 0.3029 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 5094.6055 | Test-MSE 331.5996 | Regret 0.2157 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 4274.9180 | Test-MSE 322.9350 | Regret 0.1915 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 3904.4434 | Test-MSE 313.8449 | Regret 0.1794 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 7018.1680 | Test-MSE 343.5056 | Regret 0.2770 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 5636.9492 | Test-MSE 329.1368 | Regret 0.2317 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 5313.1465 | Test-MSE 318.2343 | Regret 0.2244 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 5028.1836 | Test-MSE 309.3742 | Regret 0.2121 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 6745.2812 | Test-MSE 345.0539 | Regret 0.3010 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 5050.0020 | Test-MSE 328.5370 | Regret 0.2105 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 4347.6719 | Test-MSE 312.2397 | Regret 0.1880 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 3721.6211 | Test-MSE 306.9804 | Regret 0.1663 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 4218.0827  |  std = 577.6698\n",
      "[     MSE] final-epoch mean = 310.0665  |  std = 2.8449\n",
      "[  REGRET] final-epoch mean = 0.1859  |  std = 0.0192\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 345.2168 | Regret 0.1757 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 347.7406 | Regret -3.0423 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.3568 | Regret -3.1231 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.2707 | Regret -3.1312 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 345.7332 | Regret -2.4435 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 348.1538 | Regret -2.9061 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.0910 | Regret -3.1907 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.4602 | Regret -3.0825 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0000 | Test-MSE 344.9396 | Regret -3.2143 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0000 | Test-MSE 347.6756 | Regret -3.0053 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0000 | Test-MSE 347.4876 | Regret -3.1229 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0000 | Test-MSE 347.2873 | Regret -3.1241 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "[     MSE] final-epoch mean = 347.3394  |  std = 0.0857\n",
      "[  REGRET] final-epoch mean = -3.1126  |  std = 0.0215\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6006 | Test-MSE 341.0338 | Regret 0.2254 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 559.1721 | Test-MSE 322.6396 | Regret 0.1677 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 485.5813 | Test-MSE 311.3460 | Regret 0.1503 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 394.3115 | Test-MSE 301.3771 | Regret 0.1224 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 817.3315 | Test-MSE 343.9326 | Regret 0.2323 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 602.0317 | Test-MSE 330.4513 | Regret 0.1793 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 553.7559 | Test-MSE 323.0929 | Regret 0.1683 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 484.3469 | Test-MSE 316.9229 | Regret 0.1479 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 789.9995 | Test-MSE 342.4008 | Regret 0.2189 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 554.2944 | Test-MSE 317.2791 | Regret 0.1613 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 483.6951 | Test-MSE 303.3550 | Regret 0.1434 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 423.9983 | Test-MSE 302.6139 | Regret 0.1288 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 434.2189  |  std = 37.4605\n",
      "[     MSE] final-epoch mean = 306.9713  |  std = 7.0549\n",
      "[  REGRET] final-epoch mean = 0.1330  |  std = 0.0109\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2409.9163 | Test-MSE 341.7085 | Regret 1.4296 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1457.2969 | Test-MSE 313.8187 | Regret 0.9173 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1113.5515 | Test-MSE 299.6648 | Regret 0.7480 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 987.2170 | Test-MSE 299.2202 | Regret 0.6646 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2376.7942 | Test-MSE 342.0696 | Regret 1.3105 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1574.2509 | Test-MSE 324.6541 | Regret 1.0101 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1322.7067 | Test-MSE 318.7815 | Regret 0.8891 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 1123.1985 | Test-MSE 308.6546 | Regret 0.7769 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2261.8022 | Test-MSE 340.2644 | Regret 1.2106 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1514.0295 | Test-MSE 320.5653 | Regret 0.9293 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 1200.2949 | Test-MSE 307.0276 | Regret 0.8060 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 1013.7279 | Test-MSE 295.7412 | Regret 0.7174 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 1041.3811  |  std = 58.8573\n",
      "[     MSE] final-epoch mean = 301.2053  |  std = 5.4555\n",
      "[  REGRET] final-epoch mean = 0.7197  |  std = 0.0458\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7518.6187 | Test-MSE 344.6229 | Regret 0.3029 | Fair 480.6009\n",
      "Epoch 10/30 | Train-Loss 5707.1104 | Test-MSE 331.2920 | Regret 0.2199 | Fair 464.0054\n",
      "Epoch 20/30 | Train-Loss 4684.9702 | Test-MSE 320.1616 | Regret 0.1832 | Fair 448.7888\n",
      "Epoch 30/30 | Train-Loss 4189.3125 | Test-MSE 314.8909 | Regret 0.1661 | Fair 441.2772\n",
      "Epoch 01/30 | Train-Loss 7527.3960 | Test-MSE 343.5056 | Regret 0.2770 | Fair 479.7007\n",
      "Epoch 10/30 | Train-Loss 6123.3169 | Test-MSE 329.1368 | Regret 0.2317 | Fair 462.1509\n",
      "Epoch 20/30 | Train-Loss 5360.1982 | Test-MSE 323.3826 | Regret 0.2031 | Fair 454.0843\n",
      "Epoch 30/30 | Train-Loss 4564.9121 | Test-MSE 319.7076 | Regret 0.1776 | Fair 448.3780\n",
      "Epoch 01/30 | Train-Loss 7253.8892 | Test-MSE 345.0539 | Regret 0.3010 | Fair 481.3374\n",
      "Epoch 10/30 | Train-Loss 5539.4902 | Test-MSE 328.5370 | Regret 0.2105 | Fair 460.7823\n",
      "Epoch 20/30 | Train-Loss 4810.2002 | Test-MSE 312.1598 | Regret 0.1881 | Fair 438.8458\n",
      "Epoch 30/30 | Train-Loss 4291.4648 | Test-MSE 307.9490 | Regret 0.1702 | Fair 432.4387\n",
      "[    LOSS] final-epoch mean = 4348.5632  |  std = 158.5642\n",
      "[     MSE] final-epoch mean = 314.1825  |  std = 4.8265\n",
      "[  REGRET] final-epoch mean = 0.1713  |  std = 0.0048\n",
      "[FAIRNESS] final-epoch mean = 440.6979  |  std = 6.5201\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 506.9095 | Test-MSE 341.9255 | Regret 0.1455 | Fair 476.9757\n",
      "Epoch 10/30 | Train-Loss 472.2202 | Test-MSE 320.6619 | Regret 0.3169 | Fair 448.8922\n",
      "Epoch 20/30 | Train-Loss 453.5806 | Test-MSE 311.6902 | Regret 0.8952 | Fair 436.2864\n",
      "Epoch 30/30 | Train-Loss 453.8127 | Test-MSE 311.9020 | Regret 0.4076 | Fair 435.4698\n",
      "Epoch 01/30 | Train-Loss 509.2281 | Test-MSE 344.4306 | Regret -1.1759 | Fair 480.7750\n",
      "Epoch 10/30 | Train-Loss 495.5717 | Test-MSE 335.3803 | Regret 0.6288 | Fair 468.8979\n",
      "Epoch 20/30 | Train-Loss 486.6559 | Test-MSE 330.8542 | Regret 0.1591 | Fair 462.2186\n",
      "Epoch 30/30 | Train-Loss 478.2477 | Test-MSE 325.8634 | Regret 0.0712 | Fair 455.5966\n",
      "Epoch 01/30 | Train-Loss 508.6079 | Test-MSE 344.3060 | Regret -3.1233 | Fair 480.3596\n",
      "Epoch 10/30 | Train-Loss 505.6979 | Test-MSE 341.6024 | Regret -2.5051 | Fair 475.8386\n",
      "Epoch 20/30 | Train-Loss 500.8445 | Test-MSE 339.1577 | Regret -1.5388 | Fair 472.1911\n",
      "Epoch 30/30 | Train-Loss 498.1936 | Test-MSE 337.6714 | Regret -0.3274 | Fair 470.0280\n",
      "[    LOSS] final-epoch mean = 476.7513  |  std = 18.1493\n",
      "[     MSE] final-epoch mean = 325.1456  |  std = 10.5326\n",
      "[  REGRET] final-epoch mean = 0.0505  |  std = 0.3004\n",
      "[FAIRNESS] final-epoch mean = 453.6981  |  std = 14.1721\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 1329.5100 | Test-MSE 341.0338 | Regret 0.2254 | Fair 475.8935\n",
      "Epoch 10/30 | Train-Loss 1037.8257 | Test-MSE 323.2972 | Regret 0.1681 | Fair 452.6165\n",
      "Epoch 20/30 | Train-Loss 951.0729 | Test-MSE 315.3396 | Regret 0.1535 | Fair 441.1169\n",
      "Epoch 30/30 | Train-Loss 922.6243 | Test-MSE 314.0533 | Regret 0.1466 | Fair 438.7693\n",
      "Epoch 01/30 | Train-Loss 1326.5596 | Test-MSE 343.9326 | Regret 0.2323 | Fair 480.0383\n",
      "Epoch 10/30 | Train-Loss 1103.2053 | Test-MSE 330.9281 | Regret 0.1834 | Fair 463.0370\n",
      "Epoch 20/30 | Train-Loss 1031.0712 | Test-MSE 317.9813 | Regret 0.1704 | Fair 445.6626\n",
      "Epoch 30/30 | Train-Loss 961.3085 | Test-MSE 302.4396 | Regret 0.1544 | Fair 424.7871\n",
      "Epoch 01/30 | Train-Loss 1298.6074 | Test-MSE 342.4008 | Regret 0.2189 | Fair 478.1498\n",
      "Epoch 10/30 | Train-Loss 1036.4674 | Test-MSE 316.7461 | Regret 0.1638 | Fair 444.9847\n",
      "Epoch 20/30 | Train-Loss 934.2410 | Test-MSE 309.3370 | Regret 0.1426 | Fair 434.3994\n",
      "Epoch 30/30 | Train-Loss 877.5018 | Test-MSE 302.1053 | Regret 0.1318 | Fair 424.2853\n",
      "[    LOSS] final-epoch mean = 920.4782  |  std = 34.2476\n",
      "[     MSE] final-epoch mean = 306.1994  |  std = 5.5552\n",
      "[  REGRET] final-epoch mean = 0.1443  |  std = 0.0094\n",
      "[FAIRNESS] final-epoch mean = 429.2806  |  std = 6.7126\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2916.8257 | Test-MSE 341.7085 | Regret 1.4296 | Fair 476.6563\n",
      "Epoch 10/30 | Train-Loss 1921.4758 | Test-MSE 313.8187 | Regret 0.9173 | Fair 440.1127\n",
      "Epoch 20/30 | Train-Loss 1594.3063 | Test-MSE 290.3928 | Regret 0.8149 | Fair 408.8921\n",
      "Epoch 30/30 | Train-Loss 1450.9048 | Test-MSE 285.6080 | Regret 0.7490 | Fair 401.8378\n",
      "Epoch 01/30 | Train-Loss 2886.0222 | Test-MSE 342.0696 | Regret 1.3105 | Fair 477.5711\n",
      "Epoch 10/30 | Train-Loss 1992.4254 | Test-MSE 323.4760 | Regret 0.9798 | Fair 452.9092\n",
      "Epoch 20/30 | Train-Loss 1763.3936 | Test-MSE 315.7693 | Regret 0.8667 | Fair 441.8776\n",
      "Epoch 30/30 | Train-Loss 1528.5529 | Test-MSE 301.8792 | Regret 0.7217 | Fair 423.8187\n",
      "Epoch 01/30 | Train-Loss 2770.4102 | Test-MSE 340.2644 | Regret 1.2106 | Fair 475.4957\n",
      "Epoch 10/30 | Train-Loss 1967.6399 | Test-MSE 320.9726 | Regret 0.9061 | Fair 449.9565\n",
      "Epoch 20/30 | Train-Loss 1755.5240 | Test-MSE 318.4336 | Regret 0.8219 | Fair 445.4344\n",
      "Epoch 30/30 | Train-Loss 1535.3967 | Test-MSE 310.7777 | Regret 0.7085 | Fair 435.1173\n",
      "[    LOSS] final-epoch mean = 1504.9515  |  std = 38.3188\n",
      "[     MSE] final-epoch mean = 299.4217  |  std = 10.4214\n",
      "[  REGRET] final-epoch mean = 0.7264  |  std = 0.0168\n",
      "[FAIRNESS] final-epoch mean = 420.2579  |  std = 13.8176\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 7012.3638 | Test-MSE 344.6229 | Regret 0.3029 | Fair 0.6582\n",
      "Epoch 10/30 | Train-Loss 5095.2656 | Test-MSE 331.5996 | Regret 0.2157 | Fair 0.6642\n",
      "Epoch 20/30 | Train-Loss 4275.5762 | Test-MSE 322.9350 | Regret 0.1915 | Fair 0.6621\n",
      "Epoch 30/30 | Train-Loss 3905.1011 | Test-MSE 313.8449 | Regret 0.1794 | Fair 0.6621\n",
      "Epoch 01/30 | Train-Loss 7018.8242 | Test-MSE 343.5056 | Regret 0.2770 | Fair 0.6607\n",
      "Epoch 10/30 | Train-Loss 5637.6128 | Test-MSE 329.1368 | Regret 0.2317 | Fair 0.6680\n",
      "Epoch 20/30 | Train-Loss 5313.8135 | Test-MSE 318.2343 | Regret 0.2244 | Fair 0.6712\n",
      "Epoch 30/30 | Train-Loss 5028.8530 | Test-MSE 309.3742 | Regret 0.2121 | Fair 0.6729\n",
      "Epoch 01/30 | Train-Loss 6745.9385 | Test-MSE 345.0539 | Regret 0.3010 | Fair 0.6590\n",
      "Epoch 10/30 | Train-Loss 5050.6636 | Test-MSE 328.5370 | Regret 0.2105 | Fair 0.6659\n",
      "Epoch 20/30 | Train-Loss 4348.3359 | Test-MSE 312.2397 | Regret 0.1880 | Fair 0.6669\n",
      "Epoch 30/30 | Train-Loss 3722.2827 | Test-MSE 306.9804 | Regret 0.1663 | Fair 0.6638\n",
      "[    LOSS] final-epoch mean = 4218.7456  |  std = 577.6742\n",
      "[     MSE] final-epoch mean = 310.0665  |  std = 2.8449\n",
      "[  REGRET] final-epoch mean = 0.1859  |  std = 0.0192\n",
      "[FAIRNESS] final-epoch mean = 0.6663  |  std = 0.0047\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6549 | Test-MSE 344.1820 | Regret 0.1308 | Fair 0.6571\n",
      "Epoch 10/30 | Train-Loss 0.6501 | Test-MSE 341.0984 | Regret 0.2677 | Fair 0.6532\n",
      "Epoch 20/30 | Train-Loss 0.6444 | Test-MSE 341.7779 | Regret 0.9037 | Fair 0.6485\n",
      "Epoch 30/30 | Train-Loss 0.6462 | Test-MSE 339.1377 | Regret 0.3630 | Fair 0.6501\n",
      "Epoch 01/30 | Train-Loss 0.6563 | Test-MSE 345.7332 | Regret -2.4435 | Fair 0.6592\n",
      "Epoch 10/30 | Train-Loss 0.6590 | Test-MSE 333.7169 | Regret -0.1756 | Fair 0.6636\n",
      "Epoch 20/30 | Train-Loss 0.6589 | Test-MSE 326.8654 | Regret -0.2441 | Fair 0.6631\n",
      "Epoch 30/30 | Train-Loss 0.6570 | Test-MSE 327.3424 | Regret -0.2000 | Fair 0.6614\n",
      "Epoch 01/30 | Train-Loss 0.6571 | Test-MSE 341.0746 | Regret -3.3834 | Fair 0.6617\n",
      "Epoch 10/30 | Train-Loss 0.6616 | Test-MSE 329.6411 | Regret -3.3642 | Fair 0.6653\n",
      "Epoch 20/30 | Train-Loss 0.6606 | Test-MSE 326.8907 | Regret -3.3328 | Fair 0.6651\n",
      "Epoch 30/30 | Train-Loss 180.3524 | Test-MSE 323.8744 | Regret -3.2084 | Fair 0.6651\n",
      "[    LOSS] final-epoch mean = 60.5519  |  std = 84.7118\n",
      "[     MSE] final-epoch mean = 330.1182  |  std = 6.5330\n",
      "[  REGRET] final-epoch mean = -1.0151  |  std = 1.5678\n",
      "[FAIRNESS] final-epoch mean = 0.6589  |  std = 0.0064\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 823.2555 | Test-MSE 341.0338 | Regret 0.2254 | Fair 0.6594\n",
      "Epoch 10/30 | Train-Loss 559.8318 | Test-MSE 322.6396 | Regret 0.1677 | Fair 0.6635\n",
      "Epoch 20/30 | Train-Loss 486.2424 | Test-MSE 311.3460 | Regret 0.1503 | Fair 0.6646\n",
      "Epoch 30/30 | Train-Loss 394.9710 | Test-MSE 301.3771 | Regret 0.1224 | Fair 0.6629\n",
      "Epoch 01/30 | Train-Loss 817.9879 | Test-MSE 343.9326 | Regret 0.2323 | Fair 0.6599\n",
      "Epoch 10/30 | Train-Loss 602.6917 | Test-MSE 330.4513 | Regret 0.1793 | Fair 0.6638\n",
      "Epoch 20/30 | Train-Loss 554.4164 | Test-MSE 323.0929 | Regret 0.1683 | Fair 0.6645\n",
      "Epoch 30/30 | Train-Loss 485.0062 | Test-MSE 316.9229 | Regret 0.1479 | Fair 0.6632\n",
      "Epoch 01/30 | Train-Loss 790.6566 | Test-MSE 342.4008 | Regret 0.2189 | Fair 0.6609\n",
      "Epoch 10/30 | Train-Loss 554.9576 | Test-MSE 317.2791 | Regret 0.1613 | Fair 0.6672\n",
      "Epoch 20/30 | Train-Loss 484.3589 | Test-MSE 303.3550 | Regret 0.1434 | Fair 0.6669\n",
      "Epoch 30/30 | Train-Loss 424.6606 | Test-MSE 302.6139 | Regret 0.1288 | Fair 0.6654\n",
      "[    LOSS] final-epoch mean = 434.8793  |  std = 37.4602\n",
      "[     MSE] final-epoch mean = 306.9713  |  std = 7.0549\n",
      "[  REGRET] final-epoch mean = 0.1330  |  std = 0.0109\n",
      "[FAIRNESS] final-epoch mean = 0.6638  |  std = 0.0011\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Beta': 'N/A', 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2410.5710 | Test-MSE 341.7085 | Regret 1.4296 | Fair 0.6584\n",
      "Epoch 10/30 | Train-Loss 1457.9564 | Test-MSE 313.8187 | Regret 0.9173 | Fair 0.6642\n",
      "Epoch 20/30 | Train-Loss 1114.2109 | Test-MSE 299.6648 | Regret 0.7480 | Fair 0.6630\n",
      "Epoch 30/30 | Train-Loss 987.8749 | Test-MSE 299.2202 | Regret 0.6646 | Fair 0.6613\n",
      "Epoch 01/30 | Train-Loss 2377.4504 | Test-MSE 342.0696 | Regret 1.3105 | Fair 0.6604\n",
      "Epoch 10/30 | Train-Loss 1574.9102 | Test-MSE 324.6541 | Regret 1.0101 | Fair 0.6628\n",
      "Epoch 20/30 | Train-Loss 1323.3651 | Test-MSE 318.7815 | Regret 0.8891 | Fair 0.6616\n",
      "Epoch 30/30 | Train-Loss 1123.8583 | Test-MSE 308.6546 | Regret 0.7769 | Fair 0.6628\n",
      "Epoch 01/30 | Train-Loss 2262.4595 | Test-MSE 340.2644 | Regret 1.2106 | Fair 0.6617\n",
      "Epoch 10/30 | Train-Loss 1514.6907 | Test-MSE 320.5653 | Regret 0.9293 | Fair 0.6649\n",
      "Epoch 20/30 | Train-Loss 1200.9553 | Test-MSE 307.0276 | Regret 0.8060 | Fair 0.6629\n",
      "Epoch 30/30 | Train-Loss 1014.3887 | Test-MSE 295.7412 | Regret 0.7174 | Fair 0.6632\n",
      "[    LOSS] final-epoch mean = 1042.0406  |  std = 58.8577\n",
      "[     MSE] final-epoch mean = 301.2053  |  std = 5.4555\n",
      "[  REGRET] final-epoch mean = 0.7197  |  std = 0.0458\n",
      "[FAIRNESS] final-epoch mean = 0.6624  |  std = 0.0008\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                           GRID SEARCH COMPLETE\n",
      "================================================================================\n",
      "\n",
      "--- Averaged Results Summary (Pandas DataFrame) ---\n",
      "    Group  Grad Method  Alpha Beta  Lambda    Fairness  Final Regret   Final MSE  Final Fairness\n",
      "0    True  closed-form    0.5  0.5     0.0        None      0.064265  308.300659        0.000000\n",
      "1    True  closed-form    0.5  1.5     0.0        None      0.053591  290.035441        0.000000\n",
      "2    True  closed-form    1.0  0.5     0.0        None      0.055729  308.070760        0.000000\n",
      "3    True  closed-form    1.0  1.5     0.0        None      0.015016  290.205475        0.000000\n",
      "4    True  closed-form    1.5  0.5     0.0        None      0.076455  308.012095        0.000000\n",
      "5    True  closed-form    1.5  1.5     0.0        None      0.049900  289.213104        0.000000\n",
      "6    True  closed-form    2.0  0.5     0.0        None      0.163766  307.994273        0.000000\n",
      "7    True  closed-form    2.0  1.5     0.0        None      0.104284  287.978017        0.000000\n",
      "8    True  closed-form    0.5  0.5     1.0         mad      0.064265  308.300659       68.704773\n",
      "9    True  closed-form    0.5  1.5     1.0         mad      0.053591  290.035441       57.747615\n",
      "10   True  closed-form    1.0  0.5     1.0         mad      0.055729  308.070760       68.708832\n",
      "11   True  closed-form    1.0  1.5     1.0         mad      0.015016  290.205475       56.609456\n",
      "12   True  closed-form    1.5  0.5     1.0         mad      0.076455  308.012095       68.499273\n",
      "13   True  closed-form    1.5  1.5     1.0         mad      0.049900  289.213104       54.680359\n",
      "14   True  closed-form    2.0  0.5     1.0         mad      0.163766  307.994273       68.415476\n",
      "15   True  closed-form    2.0  1.5     1.0         mad      0.104284  287.978017       53.416814\n",
      "16   True  closed-form    0.5  0.5     1.0  acc_parity      0.064265  308.300659      137.409546\n",
      "17   True  closed-form    0.5  1.5     1.0  acc_parity      0.053591  290.035441      115.495229\n",
      "18   True  closed-form    1.0  0.5     1.0  acc_parity      0.055729  308.070760      137.417664\n",
      "19   True  closed-form    1.0  1.5     1.0  acc_parity      0.015016  290.205475      113.218913\n",
      "20   True  closed-form    1.5  0.5     1.0  acc_parity      0.076455  308.012095      136.998545\n",
      "21   True  closed-form    1.5  1.5     1.0  acc_parity      0.049900  289.213104      109.360718\n",
      "22   True  closed-form    2.0  0.5     1.0  acc_parity      0.163766  307.994273      136.830953\n",
      "23   True  closed-form    2.0  1.5     1.0  acc_parity      0.104284  287.978017      106.833628\n",
      "24   True  closed-form    0.5  0.5     1.0    atkinson      0.064265  308.300659        0.004499\n",
      "25   True  closed-form    0.5  1.5     1.0    atkinson      0.053591  290.035441        0.010008\n",
      "26   True  closed-form    1.0  0.5     1.0    atkinson      0.055729  308.070760        0.004506\n",
      "27   True  closed-form    1.0  1.5     1.0    atkinson      0.015016  290.205475        0.009645\n",
      "28   True  closed-form    1.5  0.5     1.0    atkinson      0.076455  308.012095        0.004482\n",
      "29   True  closed-form    1.5  1.5     1.0    atkinson      0.049900  289.213104        0.009116\n",
      "30   True  closed-form    2.0  0.5     1.0    atkinson      0.163766  307.994273        0.004472\n",
      "31   True  closed-form    2.0  1.5     1.0    atkinson      0.104284  287.978017        0.008806\n",
      "32   True  finite-diff    0.5  0.5     0.0        None      0.070345  313.539073        0.000000\n",
      "33   True  finite-diff    0.5  1.5     0.0        None      0.058105  291.554921        0.000000\n",
      "34   True  finite-diff    1.0  0.5     0.0        None      0.062960  314.364482        0.000000\n",
      "35   True  finite-diff    1.0  1.5     0.0        None      0.016706  291.831940        0.000000\n",
      "36   True  finite-diff    1.5  0.5     0.0        None      0.084878  310.225484        0.000000\n",
      "37   True  finite-diff    1.5  1.5     0.0        None      0.063642  297.167938        0.000000\n",
      "38   True  finite-diff    2.0  0.5     0.0        None      0.198010  309.369273        0.000000\n",
      "39   True  finite-diff    2.0  1.5     0.0        None      0.122702  298.144358        0.000000\n",
      "40   True  finite-diff    0.5  0.5     1.0         mad      0.126471  307.994517       57.844991\n",
      "41   True  finite-diff    0.5  1.5     1.0         mad      0.095517  317.585826       60.423223\n",
      "42   True  finite-diff    1.0  0.5     1.0         mad      0.115334  315.982391       60.921799\n",
      "43   True  finite-diff    1.0  1.5     1.0         mad      0.030670  302.577810       57.195608\n",
      "44   True  finite-diff    1.5  0.5     1.0         mad      0.161284  311.608317       58.071752\n",
      "45   True  finite-diff    1.5  1.5     1.0         mad      0.095171  306.731211       57.085704\n",
      "46   True  finite-diff    2.0  0.5     1.0         mad      0.352520  314.872213       59.580851\n",
      "47   True  finite-diff    2.0  1.5     1.0         mad      0.129849  296.346598       59.842300\n",
      "48   True  finite-diff    0.5  0.5     1.0  acc_parity      0.132227  315.753398      119.140594\n",
      "49   True  finite-diff    0.5  1.5     1.0  acc_parity      0.101462  325.059621      126.731262\n",
      "50   True  finite-diff    1.0  0.5     1.0  acc_parity      0.120684  316.178853      120.572825\n",
      "51   True  finite-diff    1.0  1.5     1.0  acc_parity      0.030477  315.241323      118.944722\n",
      "52   True  finite-diff    1.5  0.5     1.0  acc_parity      0.158202  322.264099      123.612925\n",
      "53   True  finite-diff    1.5  1.5     1.0  acc_parity      0.092096  301.944600      114.400930\n",
      "54   True  finite-diff    2.0  0.5     1.0  acc_parity      0.359014  324.846781      126.459442\n",
      "55   True  finite-diff    2.0  1.5     1.0  acc_parity      0.132266  302.458262      122.627563\n",
      "56   True  finite-diff    0.5  0.5     1.0    atkinson      0.073042  315.260946        0.004479\n",
      "57   True  finite-diff    0.5  1.5     1.0    atkinson      0.108125  336.980194        0.011912\n",
      "58   True  finite-diff    1.0  0.5     1.0    atkinson      0.066204  318.632568        0.004387\n",
      "59   True  finite-diff    1.0  1.5     1.0    atkinson      0.020957  303.983348        0.010430\n",
      "60   True  finite-diff    1.5  0.5     1.0    atkinson      0.108446  325.218292        0.004347\n",
      "61   True  finite-diff    1.5  1.5     1.0    atkinson      0.061164  295.812744        0.010409\n",
      "62   True  finite-diff    2.0  0.5     1.0    atkinson      0.298458  324.823558        0.004219\n",
      "63   True  finite-diff    2.0  1.5     1.0    atkinson      0.122702  298.144358        0.010668\n",
      "64  False  closed-form    0.5  N/A     0.0        None      0.223541  307.425181        0.000000\n",
      "65  False  closed-form    1.0  N/A     0.0        None     -3.112608  347.339406        0.000000\n",
      "66  False  closed-form    1.5  N/A     0.0        None      0.410657  347.815745        0.000000\n",
      "67  False  closed-form    2.0  N/A     0.0        None      2.085073  339.213745        0.000000\n",
      "68  False  closed-form    0.5  N/A     1.0         mad      0.223541  307.425181      427.597504\n",
      "69  False  closed-form    1.0  N/A     1.0         mad     -3.112608  347.339406      484.368947\n",
      "70  False  closed-form    1.5  N/A     1.0         mad      0.410657  347.815745      488.656687\n",
      "71  False  closed-form    2.0  N/A     1.0         mad      2.085073  339.213745      475.620483\n",
      "72  False  closed-form    0.5  N/A     1.0    atkinson      0.223541  307.425181        0.657579\n",
      "73  False  closed-form    1.0  N/A     1.0    atkinson     -3.112608  347.339406        0.658797\n",
      "74  False  closed-form    1.5  N/A     1.0    atkinson      0.410657  347.815745        0.663117\n",
      "75  False  closed-form    2.0  N/A     1.0    atkinson      2.085073  339.213745        0.664956\n",
      "76  False  finite-diff    0.5  N/A     0.0        None      0.185934  310.066538        0.000000\n",
      "77  False  finite-diff    1.0  N/A     0.0        None     -3.112608  347.339406        0.000000\n",
      "78  False  finite-diff    1.5  N/A     0.0        None      0.133037  306.971313        0.000000\n",
      "79  False  finite-diff    2.0  N/A     0.0        None      0.719651  301.205332        0.000000\n",
      "80  False  finite-diff    0.5  N/A     1.0         mad      0.171276  314.182495      440.697937\n",
      "81  False  finite-diff    1.0  N/A     1.0         mad      0.050456  325.145599      453.698120\n",
      "82  False  finite-diff    1.5  N/A     1.0         mad      0.144259  306.199432      429.280558\n",
      "83  False  finite-diff    2.0  N/A     1.0         mad      0.726426  299.421651      420.257935\n",
      "84  False  finite-diff    0.5  N/A     1.0    atkinson      0.185934  310.066538        0.666265\n",
      "85  False  finite-diff    1.0  N/A     1.0    atkinson     -1.015138  330.118174        0.658891\n",
      "86  False  finite-diff    1.5  N/A     1.0    atkinson      0.133037  306.971313        0.663845\n",
      "87  False  finite-diff    2.0  N/A     1.0    atkinson      0.719651  301.205332        0.662436\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- LaTeX Table Output ---\n",
      "\\begin{table}\n",
      "\\caption{Averaged Experimental Results Across Different Parameters.}\n",
      "\\label{tab:avg_exp_results_expanded}\n",
      "\\begin{tabular}{rlrlrlrrr}\n",
      "\\toprule\n",
      "Group & Grad Method & Alpha & Beta & Lambda & Fairness & Final Regret & Final MSE & Final Fairness \\\\\n",
      "\\midrule\n",
      "True & closed-form & 0.5000 & 0.5000 & 0.0000 & None & 0.0643 & 308.3007 & 0.0000 \\\\\n",
      "True & closed-form & 0.5000 & 1.5000 & 0.0000 & None & 0.0536 & 290.0354 & 0.0000 \\\\\n",
      "True & closed-form & 1.0000 & 0.5000 & 0.0000 & None & 0.0557 & 308.0708 & 0.0000 \\\\\n",
      "True & closed-form & 1.0000 & 1.5000 & 0.0000 & None & 0.0150 & 290.2055 & 0.0000 \\\\\n",
      "True & closed-form & 1.5000 & 0.5000 & 0.0000 & None & 0.0765 & 308.0121 & 0.0000 \\\\\n",
      "True & closed-form & 1.5000 & 1.5000 & 0.0000 & None & 0.0499 & 289.2131 & 0.0000 \\\\\n",
      "True & closed-form & 2.0000 & 0.5000 & 0.0000 & None & 0.1638 & 307.9943 & 0.0000 \\\\\n",
      "True & closed-form & 2.0000 & 1.5000 & 0.0000 & None & 0.1043 & 287.9780 & 0.0000 \\\\\n",
      "True & closed-form & 0.5000 & 0.5000 & 1.0000 & mad & 0.0643 & 308.3007 & 68.7048 \\\\\n",
      "True & closed-form & 0.5000 & 1.5000 & 1.0000 & mad & 0.0536 & 290.0354 & 57.7476 \\\\\n",
      "True & closed-form & 1.0000 & 0.5000 & 1.0000 & mad & 0.0557 & 308.0708 & 68.7088 \\\\\n",
      "True & closed-form & 1.0000 & 1.5000 & 1.0000 & mad & 0.0150 & 290.2055 & 56.6095 \\\\\n",
      "True & closed-form & 1.5000 & 0.5000 & 1.0000 & mad & 0.0765 & 308.0121 & 68.4993 \\\\\n",
      "True & closed-form & 1.5000 & 1.5000 & 1.0000 & mad & 0.0499 & 289.2131 & 54.6804 \\\\\n",
      "True & closed-form & 2.0000 & 0.5000 & 1.0000 & mad & 0.1638 & 307.9943 & 68.4155 \\\\\n",
      "True & closed-form & 2.0000 & 1.5000 & 1.0000 & mad & 0.1043 & 287.9780 & 53.4168 \\\\\n",
      "True & closed-form & 0.5000 & 0.5000 & 1.0000 & acc_parity & 0.0643 & 308.3007 & 137.4095 \\\\\n",
      "True & closed-form & 0.5000 & 1.5000 & 1.0000 & acc_parity & 0.0536 & 290.0354 & 115.4952 \\\\\n",
      "True & closed-form & 1.0000 & 0.5000 & 1.0000 & acc_parity & 0.0557 & 308.0708 & 137.4177 \\\\\n",
      "True & closed-form & 1.0000 & 1.5000 & 1.0000 & acc_parity & 0.0150 & 290.2055 & 113.2189 \\\\\n",
      "True & closed-form & 1.5000 & 0.5000 & 1.0000 & acc_parity & 0.0765 & 308.0121 & 136.9985 \\\\\n",
      "True & closed-form & 1.5000 & 1.5000 & 1.0000 & acc_parity & 0.0499 & 289.2131 & 109.3607 \\\\\n",
      "True & closed-form & 2.0000 & 0.5000 & 1.0000 & acc_parity & 0.1638 & 307.9943 & 136.8310 \\\\\n",
      "True & closed-form & 2.0000 & 1.5000 & 1.0000 & acc_parity & 0.1043 & 287.9780 & 106.8336 \\\\\n",
      "True & closed-form & 0.5000 & 0.5000 & 1.0000 & atkinson & 0.0643 & 308.3007 & 0.0045 \\\\\n",
      "True & closed-form & 0.5000 & 1.5000 & 1.0000 & atkinson & 0.0536 & 290.0354 & 0.0100 \\\\\n",
      "True & closed-form & 1.0000 & 0.5000 & 1.0000 & atkinson & 0.0557 & 308.0708 & 0.0045 \\\\\n",
      "True & closed-form & 1.0000 & 1.5000 & 1.0000 & atkinson & 0.0150 & 290.2055 & 0.0096 \\\\\n",
      "True & closed-form & 1.5000 & 0.5000 & 1.0000 & atkinson & 0.0765 & 308.0121 & 0.0045 \\\\\n",
      "True & closed-form & 1.5000 & 1.5000 & 1.0000 & atkinson & 0.0499 & 289.2131 & 0.0091 \\\\\n",
      "True & closed-form & 2.0000 & 0.5000 & 1.0000 & atkinson & 0.1638 & 307.9943 & 0.0045 \\\\\n",
      "True & closed-form & 2.0000 & 1.5000 & 1.0000 & atkinson & 0.1043 & 287.9780 & 0.0088 \\\\\n",
      "True & finite-diff & 0.5000 & 0.5000 & 0.0000 & None & 0.0703 & 313.5391 & 0.0000 \\\\\n",
      "True & finite-diff & 0.5000 & 1.5000 & 0.0000 & None & 0.0581 & 291.5549 & 0.0000 \\\\\n",
      "True & finite-diff & 1.0000 & 0.5000 & 0.0000 & None & 0.0630 & 314.3645 & 0.0000 \\\\\n",
      "True & finite-diff & 1.0000 & 1.5000 & 0.0000 & None & 0.0167 & 291.8319 & 0.0000 \\\\\n",
      "True & finite-diff & 1.5000 & 0.5000 & 0.0000 & None & 0.0849 & 310.2255 & 0.0000 \\\\\n",
      "True & finite-diff & 1.5000 & 1.5000 & 0.0000 & None & 0.0636 & 297.1679 & 0.0000 \\\\\n",
      "True & finite-diff & 2.0000 & 0.5000 & 0.0000 & None & 0.1980 & 309.3693 & 0.0000 \\\\\n",
      "True & finite-diff & 2.0000 & 1.5000 & 0.0000 & None & 0.1227 & 298.1444 & 0.0000 \\\\\n",
      "True & finite-diff & 0.5000 & 0.5000 & 1.0000 & mad & 0.1265 & 307.9945 & 57.8450 \\\\\n",
      "True & finite-diff & 0.5000 & 1.5000 & 1.0000 & mad & 0.0955 & 317.5858 & 60.4232 \\\\\n",
      "True & finite-diff & 1.0000 & 0.5000 & 1.0000 & mad & 0.1153 & 315.9824 & 60.9218 \\\\\n",
      "True & finite-diff & 1.0000 & 1.5000 & 1.0000 & mad & 0.0307 & 302.5778 & 57.1956 \\\\\n",
      "True & finite-diff & 1.5000 & 0.5000 & 1.0000 & mad & 0.1613 & 311.6083 & 58.0718 \\\\\n",
      "True & finite-diff & 1.5000 & 1.5000 & 1.0000 & mad & 0.0952 & 306.7312 & 57.0857 \\\\\n",
      "True & finite-diff & 2.0000 & 0.5000 & 1.0000 & mad & 0.3525 & 314.8722 & 59.5809 \\\\\n",
      "True & finite-diff & 2.0000 & 1.5000 & 1.0000 & mad & 0.1298 & 296.3466 & 59.8423 \\\\\n",
      "True & finite-diff & 0.5000 & 0.5000 & 1.0000 & acc_parity & 0.1322 & 315.7534 & 119.1406 \\\\\n",
      "True & finite-diff & 0.5000 & 1.5000 & 1.0000 & acc_parity & 0.1015 & 325.0596 & 126.7313 \\\\\n",
      "True & finite-diff & 1.0000 & 0.5000 & 1.0000 & acc_parity & 0.1207 & 316.1789 & 120.5728 \\\\\n",
      "True & finite-diff & 1.0000 & 1.5000 & 1.0000 & acc_parity & 0.0305 & 315.2413 & 118.9447 \\\\\n",
      "True & finite-diff & 1.5000 & 0.5000 & 1.0000 & acc_parity & 0.1582 & 322.2641 & 123.6129 \\\\\n",
      "True & finite-diff & 1.5000 & 1.5000 & 1.0000 & acc_parity & 0.0921 & 301.9446 & 114.4009 \\\\\n",
      "True & finite-diff & 2.0000 & 0.5000 & 1.0000 & acc_parity & 0.3590 & 324.8468 & 126.4594 \\\\\n",
      "True & finite-diff & 2.0000 & 1.5000 & 1.0000 & acc_parity & 0.1323 & 302.4583 & 122.6276 \\\\\n",
      "True & finite-diff & 0.5000 & 0.5000 & 1.0000 & atkinson & 0.0730 & 315.2609 & 0.0045 \\\\\n",
      "True & finite-diff & 0.5000 & 1.5000 & 1.0000 & atkinson & 0.1081 & 336.9802 & 0.0119 \\\\\n",
      "True & finite-diff & 1.0000 & 0.5000 & 1.0000 & atkinson & 0.0662 & 318.6326 & 0.0044 \\\\\n",
      "True & finite-diff & 1.0000 & 1.5000 & 1.0000 & atkinson & 0.0210 & 303.9833 & 0.0104 \\\\\n",
      "True & finite-diff & 1.5000 & 0.5000 & 1.0000 & atkinson & 0.1084 & 325.2183 & 0.0043 \\\\\n",
      "True & finite-diff & 1.5000 & 1.5000 & 1.0000 & atkinson & 0.0612 & 295.8127 & 0.0104 \\\\\n",
      "True & finite-diff & 2.0000 & 0.5000 & 1.0000 & atkinson & 0.2985 & 324.8236 & 0.0042 \\\\\n",
      "True & finite-diff & 2.0000 & 1.5000 & 1.0000 & atkinson & 0.1227 & 298.1444 & 0.0107 \\\\\n",
      "False & closed-form & 0.5000 & N/A & 0.0000 & None & 0.2235 & 307.4252 & 0.0000 \\\\\n",
      "False & closed-form & 1.0000 & N/A & 0.0000 & None & -3.1126 & 347.3394 & 0.0000 \\\\\n",
      "False & closed-form & 1.5000 & N/A & 0.0000 & None & 0.4107 & 347.8157 & 0.0000 \\\\\n",
      "False & closed-form & 2.0000 & N/A & 0.0000 & None & 2.0851 & 339.2137 & 0.0000 \\\\\n",
      "False & closed-form & 0.5000 & N/A & 1.0000 & mad & 0.2235 & 307.4252 & 427.5975 \\\\\n",
      "False & closed-form & 1.0000 & N/A & 1.0000 & mad & -3.1126 & 347.3394 & 484.3689 \\\\\n",
      "False & closed-form & 1.5000 & N/A & 1.0000 & mad & 0.4107 & 347.8157 & 488.6567 \\\\\n",
      "False & closed-form & 2.0000 & N/A & 1.0000 & mad & 2.0851 & 339.2137 & 475.6205 \\\\\n",
      "False & closed-form & 0.5000 & N/A & 1.0000 & atkinson & 0.2235 & 307.4252 & 0.6576 \\\\\n",
      "False & closed-form & 1.0000 & N/A & 1.0000 & atkinson & -3.1126 & 347.3394 & 0.6588 \\\\\n",
      "False & closed-form & 1.5000 & N/A & 1.0000 & atkinson & 0.4107 & 347.8157 & 0.6631 \\\\\n",
      "False & closed-form & 2.0000 & N/A & 1.0000 & atkinson & 2.0851 & 339.2137 & 0.6650 \\\\\n",
      "False & finite-diff & 0.5000 & N/A & 0.0000 & None & 0.1859 & 310.0665 & 0.0000 \\\\\n",
      "False & finite-diff & 1.0000 & N/A & 0.0000 & None & -3.1126 & 347.3394 & 0.0000 \\\\\n",
      "False & finite-diff & 1.5000 & N/A & 0.0000 & None & 0.1330 & 306.9713 & 0.0000 \\\\\n",
      "False & finite-diff & 2.0000 & N/A & 0.0000 & None & 0.7197 & 301.2053 & 0.0000 \\\\\n",
      "False & finite-diff & 0.5000 & N/A & 1.0000 & mad & 0.1713 & 314.1825 & 440.6979 \\\\\n",
      "False & finite-diff & 1.0000 & N/A & 1.0000 & mad & 0.0505 & 325.1456 & 453.6981 \\\\\n",
      "False & finite-diff & 1.5000 & N/A & 1.0000 & mad & 0.1443 & 306.1994 & 429.2806 \\\\\n",
      "False & finite-diff & 2.0000 & N/A & 1.0000 & mad & 0.7264 & 299.4217 & 420.2579 \\\\\n",
      "False & finite-diff & 0.5000 & N/A & 1.0000 & atkinson & 0.1859 & 310.0665 & 0.6663 \\\\\n",
      "False & finite-diff & 1.0000 & N/A & 1.0000 & atkinson & -1.0151 & 330.1182 & 0.6589 \\\\\n",
      "False & finite-diff & 1.5000 & N/A & 1.0000 & atkinson & 0.1330 & 306.9713 & 0.6638 \\\\\n",
      "False & finite-diff & 2.0000 & N/A & 1.0000 & atkinson & 0.7197 & 301.2053 & 0.6624 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ===== REGRET-BASED TRAINING AND EXPERIMENT HARNESS\n",
    "# ==============================================================================\n",
    "\n",
    "def train_many_trials_regret(\n",
    "        n_trials=10, base_seed=2025, **kwargs):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` and average the results.\n",
    "    Accepts all arguments for `train_model_regret` via **kwargs.\n",
    "    \"\"\"\n",
    "    trials_logs = []\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        _, logs = train_model_regret(**kwargs)\n",
    "        trials_logs.append(logs)\n",
    "\n",
    "    keys = [\"loss_log\", \"mse_log\", \"regret_log\", \"fairness_log\"]\n",
    "    avg_logs = {}\n",
    "    for k in keys:\n",
    "        if trials_logs[0][k] is None or not trials_logs[0][k]:\n",
    "            avg_logs[k] = [0.0]\n",
    "            continue\n",
    "        stack = np.vstack([trial[k] for trial in trials_logs])\n",
    "        avg_logs[k] = stack.mean(axis=0).tolist()\n",
    "        std_k = stack.std(axis=0)[-1]\n",
    "        mean_k = avg_logs[k][-1]\n",
    "        print(f\"[{k.replace('_log','').upper():>8s}] final-epoch mean = {mean_k:.4f}  |  std = {std_k:.4f}\")\n",
    "\n",
    "    return avg_logs\n",
    "\n",
    "\n",
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 1, 1.5, 2]\n",
    "betas = [0.5, 1, 1.5, 2]\n",
    "fairness_lambdas = [0, 1.0]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form', 'finite-diff'] # New parameter\n",
    "results_list = []\n",
    "\n",
    "# --- Grid Search Execution ---\n",
    "for group in group_settings:\n",
    "    # UPDATED: Added 'acc_parity' for the group case\n",
    "    if group:\n",
    "        fairness_types = ['mad', 'acc_parity', 'atkinson']\n",
    "    else:\n",
    "        fairness_types = ['mad', 'atkinson']\n",
    "    \n",
    "    for grad_method in grad_methods:\n",
    "        for lam in fairness_lambdas:\n",
    "            for fairness in fairness_types:\n",
    "                if lam == 0 and fairness != fairness_types[0]: continue\n",
    "\n",
    "                for alpha in alphas:\n",
    "                    current_betas = betas if group else [betas[0]]\n",
    "                    \n",
    "                    for beta in current_betas:\n",
    "                        if group and abs(beta - 1.0) < 1e-9: continue\n",
    "\n",
    "                        run_params = {\n",
    "                            'Group': group,\n",
    "                            'Grad Method': grad_method, # New column\n",
    "                            'Alpha': alpha,\n",
    "                            'Beta': beta if group else 'N/A',\n",
    "                            'Lambda': lam,\n",
    "                            'Fairness': fairness\n",
    "                        }\n",
    "                        print(\"\\n\" + \"-\"*70)\n",
    "                        print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                        print(\"-\"*70)\n",
    "\n",
    "                        train_args = {\n",
    "                            'X_train': feats_train, 'y_train': b_train, 'race_train': race_train, 'cost_train': cost_train, 'gainF_train': gainF_train,\n",
    "                            'X_test': feats_test, 'y_test': b_test, 'race_test': race_test, 'cost_test': cost_test, 'gainF_test': gainF_test,\n",
    "                            'model_class': FairRiskPredictor, 'input_dim': feats_train.shape[1],\n",
    "                            'alpha': alpha, 'beta': beta, 'Q': Q,\n",
    "                            'lambda_fair': lam, 'fairness_type': fairness,\n",
    "                            'group': group,\n",
    "                            'grad_method': grad_method, # Pass the current grad method\n",
    "                            'num_epochs': 30, 'lr': 0.01, 'batch_size': None,\n",
    "                        }\n",
    "\n",
    "                        avg_logs = train_many_trials_regret(n_trials=3, **train_args) # Reduced trials for speed\n",
    "\n",
    "                        final_metrics = run_params.copy()\n",
    "                        final_metrics['Final Regret'] = avg_logs['regret_log'][-1]\n",
    "                        final_metrics['Final MSE'] = avg_logs['mse_log'][-1]\n",
    "                        final_metrics['Final Fairness'] = avg_logs['fairness_log'][-1] if avg_logs.get('fairness_log') else 0.0\n",
    "                        results_list.append(final_metrics)\n",
    "\n",
    "# --- Results Presentation ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "# UPDATED: Added 'Grad Method' to the column order\n",
    "column_order = ['Group', 'Grad Method', 'Alpha', 'Beta', 'Lambda', 'Fairness', 'Final Regret', 'Final MSE', 'Final Fairness']\n",
    "results_df = results_df[column_order]\n",
    "latex_table = results_df.to_latex(index=False, caption=\"Averaged Experimental Results Across Different Parameters.\", label=\"tab:avg_exp_results_expanded\", float_format=\"%.4f\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n--- Averaged Results Summary (Pandas DataFrame) ---\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"--- LaTeX Table Output ---\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1e66d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8e55bf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Grad Method</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Fairness</th>\n",
       "      <th>Final Regret</th>\n",
       "      <th>Final MSE</th>\n",
       "      <th>Final Fairness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.064265</td>\n",
       "      <td>308.300659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>290.035441</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>308.070760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>290.205475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>308.012095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.726426</td>\n",
       "      <td>299.421651</td>\n",
       "      <td>420.257935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.185934</td>\n",
       "      <td>310.066538</td>\n",
       "      <td>0.666265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>-1.015138</td>\n",
       "      <td>330.118174</td>\n",
       "      <td>0.658891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.133037</td>\n",
       "      <td>306.971313</td>\n",
       "      <td>0.663845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.719651</td>\n",
       "      <td>301.205332</td>\n",
       "      <td>0.662436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group  Grad Method  Alpha Beta  Lambda  Fairness  Final Regret  \\\n",
       "0    True  closed-form    0.5  0.5     0.0      None      0.064265   \n",
       "1    True  closed-form    0.5  1.5     0.0      None      0.053591   \n",
       "2    True  closed-form    1.0  0.5     0.0      None      0.055729   \n",
       "3    True  closed-form    1.0  1.5     0.0      None      0.015016   \n",
       "4    True  closed-form    1.5  0.5     0.0      None      0.076455   \n",
       "..    ...          ...    ...  ...     ...       ...           ...   \n",
       "83  False  finite-diff    2.0  N/A     1.0       mad      0.726426   \n",
       "84  False  finite-diff    0.5  N/A     1.0  atkinson      0.185934   \n",
       "85  False  finite-diff    1.0  N/A     1.0  atkinson     -1.015138   \n",
       "86  False  finite-diff    1.5  N/A     1.0  atkinson      0.133037   \n",
       "87  False  finite-diff    2.0  N/A     1.0  atkinson      0.719651   \n",
       "\n",
       "     Final MSE  Final Fairness  \n",
       "0   308.300659        0.000000  \n",
       "1   290.035441        0.000000  \n",
       "2   308.070760        0.000000  \n",
       "3   290.205475        0.000000  \n",
       "4   308.012095        0.000000  \n",
       "..         ...             ...  \n",
       "83  299.421651      420.257935  \n",
       "84  310.066538        0.666265  \n",
       "85  330.118174        0.658891  \n",
       "86  306.971313        0.663845  \n",
       "87  301.205332        0.662436  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3aa372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
