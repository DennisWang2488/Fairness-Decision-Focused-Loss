{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0ac945",
   "metadata": {},
   "source": [
    "Closed-Form for Group-Alpha is not correct.\n",
    "\n",
    "\n",
    "When n=2, mad = AccParity/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e98b16",
   "metadata": {},
   "source": [
    "1. Add autograd Fairness gradient in backward to closed-form\n",
    "\n",
    "2. Verify formula of MAD and Acc Parity on Group\n",
    "\n",
    "3. Let BETA = ALPHA\n",
    "\n",
    "4. Also report training time\n",
    "\n",
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "\n",
    "6. For Fold-OPT Change PGD closed-form to solver.\n",
    "\n",
    "7. Report fairness value when lambda = 0\n",
    "\n",
    "8. Verify Individual and Group Regret Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e5f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "     AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab506ae",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aba8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5919c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 0.1) \n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e5c64",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845eab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ef940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa098c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f1760a9",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfdeb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03c830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8363ef",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f983e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([0.1000])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1000])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([12.3363])\n",
      "sol_group: tensor([12.3363])\n",
      "obj_group: tensor([-1642.7346])\n",
      "obj_ind: tensor([-1642.7346])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9ba2b",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57dce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b1becd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from src.utils.myOptimization import solveIndProblem\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    A custom autograd Function to compute regret and its closed-form gradient.\n",
    "    The forward pass computes the regret value (the objective part of the loss).\n",
    "    The backward pass computes the analytical gradient of the regret w.r.t. the predictions.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method):\n",
    "        # --- Loss Calculation (Regret only) ---\n",
    "        # This part does not need to build a graph, so we use detached tensors.\n",
    "        pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "            lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "        try:\n",
    "            if group:\n",
    "                d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "                d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "                obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "                obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "            else:\n",
    "                d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "                d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "                obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "                obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "            regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "            d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "            print(f\"Warning: Solver failed in forward pass: {e}\")\n",
    "            regret_loss = torch.tensor(0.0, dtype=pred_r.dtype, device=pred_r.device, requires_grad=True) # Return a valid tensor\n",
    "            d_hat = None\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # --- REVISEMENT 1: Add autograd Fairness gradient in backward to closed-form ---\n",
    "        # This function now ONLY computes the gradient for the regret part of the loss.\n",
    "        # The fairness gradient is handled automatically by PyTorch's autograd in the main FDFLLoss module.\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 9\n",
    "\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "        # Gradient is only non-zero if regret is positive, which implies we need to compute it.\n",
    "        # This custom backward returns the gradient of the regret w.r.t pred_r.\n",
    "        try:\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                if params['group']:\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    v_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    Jac_mat = solve_coupled_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    vT_J_np = v_np @ Jac_mat\n",
    "                    grad_regret = -torch.from_numpy(vT_J_np).to(pred_r.device,dtype=pred_r.dtype)\n",
    "\n",
    "                else:\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - params['alpha']) * d_hat ** (-params['alpha']) # Grad of alpha-fairness obj\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "            # (finite-diff option remains unchanged)\n",
    "\n",
    "        except (ValueError, TypeError, np.linalg.LinAlgError) as e:\n",
    "            print(f\"Warning: Closed-form gradient failed: {e}. Returning zero grad for regret.\")\n",
    "\n",
    "        # The returned gradient is multiplied by grad_output by the chain rule\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "    Combines regret loss (from a custom autograd Function) and a standard fairness penalty.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Calculate regret loss using the custom function with its analytical gradient\n",
    "        regret_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method)\n",
    "\n",
    "        # 2. Calculate fairness penalty using standard torch operations.\n",
    "        # Autograd will handle the gradient for this part automatically.\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "        # 3. Total loss is the sum. The gradient will be a sum of the custom regret grad and autograd fairness grad.\n",
    "        total_loss = regret_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629342b2",
   "metadata": {},
   "source": [
    "# Training Gemini Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c1c3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "\n",
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor via direct regret minimization, logging detailed metrics\n",
    "    at each evaluation point.\n",
    "    \"\"\"\n",
    "    # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # --- Initialize Logs ---\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "    unique_groups = torch.unique(race_test).cpu().numpy()\n",
    "    per_group_mse_log = {g: [] for g in unique_groups}\n",
    "    per_group_obj_log = {g: [] for g in unique_groups}\n",
    "    per_group_true_benefit_log = {g: [] for g in unique_groups}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze().clamp(min=1e-4)\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # --- Periodic Evaluation on Test Set ---\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test).squeeze().clamp(min=1e-4)\n",
    "                # Overall MSE\n",
    "                mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "                mse_log.append(mse_val)\n",
    "\n",
    "                # Overall Regret\n",
    "                _, _, d_pred_np = _calculate_loss_and_decision(pred_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                _, _, d_true_np = _calculate_loss_and_decision(y_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                if d_pred_np is not None and d_true_np is not None:\n",
    "                    y_test_np = to_numpy_1d(y_test)\n",
    "                    race_test_np = to_numpy_1d(race_test)\n",
    "                    if group:\n",
    "                        true_obj = compute_coupled_group_obj(d_true_np, y_test_np, race_test_np, alpha)\n",
    "                        pred_obj = compute_coupled_group_obj(d_pred_np, y_test_np, race_test_np, alpha)\n",
    "                    else:\n",
    "                        true_obj = AlphaFairness(y_test_np * d_true_np, alpha)\n",
    "                        pred_obj = AlphaFairness(y_test_np * d_pred_np, alpha)\n",
    "                    norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "                else:\n",
    "                    norm_regret = np.nan\n",
    "                regret_log.append(norm_regret)\n",
    "\n",
    "                # Overall Fairness\n",
    "                fair_val = 0.0\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=0.5, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "                fairness_log.append(fair_val)\n",
    "\n",
    "                # Group-wise Metrics\n",
    "                for g in unique_groups:\n",
    "                    mask = (race_test == g)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    # Group MSE\n",
    "                    per_group_mse_log[g].append(((pred_test[mask] - y_test[mask]).pow(2)).mean().item())\n",
    "                    # Group True Benefit\n",
    "                    per_group_true_benefit_log[g].append(y_test[mask].mean().item())\n",
    "                    # Group Decision Objective\n",
    "                    if d_pred_np is not None:\n",
    "                        group_mask_np = (race_test_np == g)\n",
    "                        # We use the true benefits (y_test) to evaluate the utility of the decisions (d_pred_np)\n",
    "                        group_utility = y_test_np[group_mask_np] * d_pred_np[group_mask_np]\n",
    "                        # For simplicity, we report the mean utility as the objective\n",
    "                        per_group_obj_log[g].append(group_utility.mean())\n",
    "                    else:\n",
    "                        per_group_obj_log[g].append(np.nan)\n",
    "\n",
    "                print(f\"Epoch {epoch:03d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_log[-1]:.4f} | Regret {regret_log[-1]:.4f} | Fair-Val {fairness_log[-1]:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "    # Return a dictionary of all logs\n",
    "    return model, {\n",
    "        \"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log,\n",
    "        \"training_time\": total_time,\n",
    "        \"per_group_mse\": per_group_mse_log,\n",
    "        \"per_group_decision_objective\": per_group_obj_log,\n",
    "        \"per_group_true_benefit\": per_group_true_benefit_log\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d29453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/30 | Train-Loss 7458937632718848.0000 | Test-MSE 346.2205 | Regret 70617586.1859 | Fair-Val 0.6586\n",
      "Epoch 010/30 | Train-Loss 88974559281152.0000 | Test-MSE 343.4914 | Regret 11964254.9929 | Fair-Val 0.6589\n",
      "Epoch 020/30 | Train-Loss 59689651929088.0000 | Test-MSE 342.6916 | Regret 10073336.5666 | Fair-Val 0.6591\n",
      "Epoch 030/30 | Train-Loss 43731298287616.0000 | Test-MSE 342.2797 | Regret 8537249.9104 | Fair-Val 0.6594\n",
      "Training finished in 2.10s.\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"alpha\": 10.2,\n",
    "    \"Q\": 1000,\n",
    "    \"lambda_fair\": 0,\n",
    "    \"fairness_type\": \"atkinson\",   \n",
    "    \"group\": False,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"closed-form\",\n",
    "    \"num_epochs\": 30,        \n",
    "    \"lr\": 0.005,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Train-Loss 6295.5972 | Test-MSE 346.1301 | Regret 5.3562 | Fair 0.0204\n",
      "Epoch 10/50 | Train-Loss 5209.8638 | Test-MSE 343.1317 | Regret 4.4941 | Fair 0.0201\n",
      "Epoch 20/50 | Train-Loss 4566.1587 | Test-MSE 339.7930 | Regret 3.9832 | Fair 0.0198\n",
      "Epoch 30/50 | Train-Loss 4180.6831 | Test-MSE 337.0753 | Regret 3.6535 | Fair 0.0194\n",
      "Epoch 40/50 | Train-Loss 3897.2947 | Test-MSE 334.9794 | Regret 3.4328 | Fair 0.0191\n",
      "Epoch 50/50 | Train-Loss 3672.1292 | Test-MSE 333.2602 | Regret 3.2805 | Fair 0.0187\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"alpha\": alpha,\\''\n",
    "    \"Q\": Q,\n",
    "    \"lambda_fair\": 1,\n",
    "    \"fairness_type\": \"atkinson\",   \n",
    "    \"group\": True,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"closed-form\",\n",
    "    \"num_epochs\": 50,        \n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1432 | Test-MSE 340.7641 | Regret 0.1190 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.1952 | Test-MSE 316.7935 | Regret 0.0865 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9423 | Test-MSE 311.2867 | Regret 0.0732 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7583 | Test-MSE 307.6521 | Regret 0.0642 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.1837 | Test-MSE 341.7054 | Regret 0.1190 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.2376 | Test-MSE 318.7535 | Regret 0.0915 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9810 | Test-MSE 313.7587 | Regret 0.0756 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7959 | Test-MSE 308.9955 | Regret 0.0655 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 2.0675 | Test-MSE 340.0304 | Regret 0.1173 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 1.1914 | Test-MSE 318.1606 | Regret 0.0906 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.9286 | Test-MSE 311.3828 | Regret 0.0734 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.7630 | Test-MSE 308.2544 | Regret 0.0631 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.7724  |  std = 0.0167\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0265 | Test-MSE 340.7039 | Regret 0.1147 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0151 | Test-MSE 309.8002 | Regret 0.0676 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0110 | Test-MSE 295.9592 | Regret 0.0566 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0088 | Test-MSE 287.7383 | Regret 0.0519 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0273 | Test-MSE 341.7417 | Regret 0.1086 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0155 | Test-MSE 311.8926 | Regret 0.0703 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0114 | Test-MSE 299.9689 | Regret 0.0592 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0091 | Test-MSE 292.0424 | Regret 0.0536 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0257 | Test-MSE 340.0729 | Regret 0.0994 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0150 | Test-MSE 310.6141 | Regret 0.0694 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0110 | Test-MSE 297.8852 | Regret 0.0597 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0089 | Test-MSE 290.3256 | Regret 0.0553 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0089  |  std = 0.0002\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6453 | Test-MSE 340.7068 | Regret 0.1089 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3646 | Test-MSE 316.1238 | Regret 0.0761 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2872 | Test-MSE 310.6775 | Regret 0.0636 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2335 | Test-MSE 307.2025 | Regret 0.0554 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6531 | Test-MSE 341.6537 | Regret 0.1080 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3770 | Test-MSE 318.3612 | Regret 0.0794 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2966 | Test-MSE 313.2743 | Regret 0.0659 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2449 | Test-MSE 308.7423 | Regret 0.0567 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.6196 | Test-MSE 340.0304 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.3607 | Test-MSE 317.7393 | Regret 0.0781 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2823 | Test-MSE 310.9457 | Regret 0.0640 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.2343 | Test-MSE 308.2675 | Regret 0.0551 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.2376  |  std = 0.0052\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4521 | Test-MSE 340.6559 | Regret 0.0335 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2723 | Test-MSE 309.7896 | Regret 0.0211 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2108 | Test-MSE 295.1635 | Regret 0.0175 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1750 | Test-MSE 287.8890 | Regret 0.0149 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4579 | Test-MSE 341.6232 | Regret 0.0319 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2804 | Test-MSE 311.7780 | Regret 0.0218 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2179 | Test-MSE 299.2923 | Regret 0.0181 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1817 | Test-MSE 291.6497 | Regret 0.0152 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.4371 | Test-MSE 339.9308 | Regret 0.0300 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.2717 | Test-MSE 310.7772 | Regret 0.0214 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.2117 | Test-MSE 297.6441 | Regret 0.0178 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.1753 | Test-MSE 291.0777 | Regret 0.0150 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.1773  |  std = 0.0031\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2133 | Test-MSE 340.6805 | Regret 0.1571 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1185 | Test-MSE 315.7098 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0926 | Test-MSE 310.2985 | Regret 0.0874 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0755 | Test-MSE 306.9650 | Regret 0.0758 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2150 | Test-MSE 341.6537 | Regret 0.1550 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1224 | Test-MSE 318.1295 | Regret 0.1093 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0952 | Test-MSE 313.0290 | Regret 0.0907 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0790 | Test-MSE 308.7429 | Regret 0.0781 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.2036 | Test-MSE 340.0305 | Regret 0.1499 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.1165 | Test-MSE 317.5373 | Regret 0.1071 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0907 | Test-MSE 310.9568 | Regret 0.0878 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0754 | Test-MSE 308.3284 | Regret 0.0755 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0766  |  std = 0.0017\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.4945 | Test-MSE 340.6135 | Regret 0.1079 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.0240 | Test-MSE 309.7021 | Regret 0.0699 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 7.8070 | Test-MSE 293.2859 | Regret 0.0573 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.5563 | Test-MSE 286.7802 | Regret 0.0493 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 16.4774 | Test-MSE 341.5659 | Regret 0.1046 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.3664 | Test-MSE 311.7730 | Regret 0.0722 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 8.1187 | Test-MSE 297.5720 | Regret 0.0590 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.7457 | Test-MSE 290.6397 | Regret 0.0504 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 15.8833 | Test-MSE 339.8916 | Regret 0.1000 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 10.0402 | Test-MSE 310.9011 | Regret 0.0708 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 7.8035 | Test-MSE 296.3110 | Regret 0.0579 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 6.5140 | Test-MSE 290.2194 | Regret 0.0499 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 6.6053  |  std = 0.1007\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0736 | Test-MSE 340.6805 | Regret 0.3530 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0397 | Test-MSE 315.4717 | Regret 0.2281 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0307 | Test-MSE 310.0607 | Regret 0.1877 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0250 | Test-MSE 306.8338 | Regret 0.1621 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0740 | Test-MSE 341.6537 | Regret 0.3471 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0409 | Test-MSE 318.0392 | Regret 0.2353 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0315 | Test-MSE 312.8951 | Regret 0.1951 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0261 | Test-MSE 308.6572 | Regret 0.1674 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 0.0698 | Test-MSE 340.0305 | Regret 0.3328 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 0.0388 | Test-MSE 317.4532 | Regret 0.2299 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 0.0300 | Test-MSE 311.1502 | Regret 0.1877 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 0.0249 | Test-MSE 308.4918 | Regret 0.1618 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 0.0253  |  std = 0.0005\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 0, 'Fairness': 'None'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 822.6004 | Test-MSE 340.6135 | Regret 0.2221 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 491.6048 | Test-MSE 309.8607 | Regret 0.1438 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 376.4490 | Test-MSE 292.6831 | Regret 0.1175 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 319.5690 | Test-MSE 285.6093 | Regret 0.1031 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 817.3315 | Test-MSE 341.5659 | Regret 0.2167 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 509.3578 | Test-MSE 311.7619 | Regret 0.1480 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 392.2265 | Test-MSE 296.8894 | Regret 0.1205 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 328.6343 | Test-MSE 289.4665 | Regret 0.1054 | Fair 0.0000\n",
      "Epoch 01/30 | Train-Loss 789.9996 | Test-MSE 339.8174 | Regret 0.2079 | Fair 0.0000\n",
      "Epoch 10/30 | Train-Loss 492.4255 | Test-MSE 310.9540 | Regret 0.1448 | Fair 0.0000\n",
      "Epoch 20/30 | Train-Loss 376.1085 | Test-MSE 295.6365 | Regret 0.1187 | Fair 0.0000\n",
      "Epoch 30/30 | Train-Loss 317.9721 | Test-MSE 288.8583 | Regret 0.1043 | Fair 0.0000\n",
      "[    LOSS] final-epoch mean = 322.0585  |  std = 4.6953\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 0.0000  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 97.8727 | Test-MSE 340.7641 | Regret 0.1190 | Fair 80.0155\n",
      "Epoch 10/30 | Train-Loss 76.4691 | Test-MSE 316.7935 | Regret 0.0865 | Fair 68.1967\n",
      "Epoch 20/30 | Train-Loss 72.2987 | Test-MSE 311.2867 | Regret 0.0732 | Fair 67.7545\n",
      "Epoch 30/30 | Train-Loss 74.3380 | Test-MSE 307.6521 | Regret 0.0642 | Fair 69.0484\n",
      "Epoch 01/30 | Train-Loss 99.5045 | Test-MSE 341.7054 | Regret 0.1190 | Fair 80.8831\n",
      "Epoch 10/30 | Train-Loss 79.0971 | Test-MSE 318.7535 | Regret 0.0915 | Fair 68.4977\n",
      "Epoch 20/30 | Train-Loss 74.5401 | Test-MSE 313.7587 | Regret 0.0756 | Fair 68.3501\n",
      "Epoch 30/30 | Train-Loss 75.5218 | Test-MSE 308.9955 | Regret 0.0655 | Fair 69.3055\n",
      "Epoch 01/30 | Train-Loss 99.0658 | Test-MSE 340.0304 | Regret 0.1173 | Fair 78.5901\n",
      "Epoch 10/30 | Train-Loss 78.8628 | Test-MSE 318.1606 | Regret 0.0906 | Fair 67.3305\n",
      "Epoch 20/30 | Train-Loss 73.5733 | Test-MSE 311.3828 | Regret 0.0734 | Fair 66.1859\n",
      "Epoch 30/30 | Train-Loss 75.1926 | Test-MSE 308.2544 | Regret 0.0631 | Fair 67.7604\n",
      "[    LOSS] final-epoch mean = 75.0175  |  std = 0.4989\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 68.7048  |  std = 0.6760\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.7559 | Test-MSE 340.7039 | Regret 0.1147 | Fair 80.1184\n",
      "Epoch 10/30 | Train-Loss 70.2425 | Test-MSE 309.8002 | Regret 0.0676 | Fair 65.3703\n",
      "Epoch 20/30 | Train-Loss 58.4445 | Test-MSE 295.9592 | Regret 0.0566 | Fair 57.6398\n",
      "Epoch 30/30 | Train-Loss 53.7673 | Test-MSE 287.7383 | Regret 0.0519 | Fair 56.1501\n",
      "Epoch 01/30 | Train-Loss 97.3481 | Test-MSE 341.7417 | Regret 0.1086 | Fair 80.9350\n",
      "Epoch 10/30 | Train-Loss 73.0384 | Test-MSE 311.8926 | Regret 0.0703 | Fair 66.4532\n",
      "Epoch 20/30 | Train-Loss 61.8746 | Test-MSE 299.9689 | Regret 0.0592 | Fair 59.5035\n",
      "Epoch 30/30 | Train-Loss 56.7909 | Test-MSE 292.0424 | Regret 0.0536 | Fair 58.4140\n",
      "Epoch 01/30 | Train-Loss 97.0240 | Test-MSE 340.0729 | Regret 0.0994 | Fair 78.8321\n",
      "Epoch 10/30 | Train-Loss 71.8026 | Test-MSE 310.6141 | Regret 0.0694 | Fair 65.6564\n",
      "Epoch 20/30 | Train-Loss 60.6693 | Test-MSE 297.8852 | Regret 0.0597 | Fair 59.5198\n",
      "Epoch 30/30 | Train-Loss 56.0607 | Test-MSE 290.3256 | Regret 0.0553 | Fair 58.6787\n",
      "[    LOSS] final-epoch mean = 55.5397  |  std = 1.2882\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 57.7476  |  std = 1.1348\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.3747 | Test-MSE 340.7068 | Regret 0.1089 | Fair 80.0739\n",
      "Epoch 10/30 | Train-Loss 75.8548 | Test-MSE 316.1238 | Regret 0.0761 | Fair 67.5744\n",
      "Epoch 20/30 | Train-Loss 73.2627 | Test-MSE 310.6775 | Regret 0.0636 | Fair 67.3145\n",
      "Epoch 30/30 | Train-Loss 76.4344 | Test-MSE 307.2025 | Regret 0.0554 | Fair 68.9379\n",
      "Epoch 01/30 | Train-Loss 97.9739 | Test-MSE 341.6537 | Regret 0.1080 | Fair 80.8845\n",
      "Epoch 10/30 | Train-Loss 78.7169 | Test-MSE 318.3612 | Regret 0.0794 | Fair 68.3347\n",
      "Epoch 20/30 | Train-Loss 75.6597 | Test-MSE 313.2743 | Regret 0.0659 | Fair 68.2978\n",
      "Epoch 30/30 | Train-Loss 77.3636 | Test-MSE 308.7423 | Regret 0.0567 | Fair 69.4794\n",
      "Epoch 01/30 | Train-Loss 97.6179 | Test-MSE 340.0304 | Regret 0.1054 | Fair 78.5901\n",
      "Epoch 10/30 | Train-Loss 78.5472 | Test-MSE 317.7393 | Regret 0.0781 | Fair 66.9545\n",
      "Epoch 20/30 | Train-Loss 74.7743 | Test-MSE 310.9457 | Regret 0.0640 | Fair 65.9726\n",
      "Epoch 30/30 | Train-Loss 77.1026 | Test-MSE 308.2675 | Regret 0.0551 | Fair 67.7093\n",
      "[    LOSS] final-epoch mean = 76.9669  |  std = 0.3913\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 68.7088  |  std = 0.7405\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 96.1815 | Test-MSE 340.6559 | Regret 0.0335 | Fair 80.1142\n",
      "Epoch 10/30 | Train-Loss 71.4456 | Test-MSE 309.7896 | Regret 0.0211 | Fair 64.2056\n",
      "Epoch 20/30 | Train-Loss 59.4198 | Test-MSE 295.1635 | Regret 0.0175 | Fair 55.4350\n",
      "Epoch 30/30 | Train-Loss 55.4148 | Test-MSE 287.8890 | Regret 0.0149 | Fair 55.0195\n",
      "Epoch 01/30 | Train-Loss 97.7787 | Test-MSE 341.6232 | Regret 0.0319 | Fair 80.9260\n",
      "Epoch 10/30 | Train-Loss 73.8578 | Test-MSE 311.7780 | Regret 0.0218 | Fair 65.1869\n",
      "Epoch 20/30 | Train-Loss 62.9055 | Test-MSE 299.2923 | Regret 0.0181 | Fair 57.1669\n",
      "Epoch 30/30 | Train-Loss 59.0025 | Test-MSE 291.6497 | Regret 0.0152 | Fair 56.8663\n",
      "Epoch 01/30 | Train-Loss 97.4354 | Test-MSE 339.9308 | Regret 0.0300 | Fair 78.5669\n",
      "Epoch 10/30 | Train-Loss 72.7088 | Test-MSE 310.7772 | Regret 0.0214 | Fair 64.0588\n",
      "Epoch 20/30 | Train-Loss 61.4417 | Test-MSE 297.6441 | Regret 0.0178 | Fair 57.9549\n",
      "Epoch 30/30 | Train-Loss 58.1763 | Test-MSE 291.0777 | Regret 0.0150 | Fair 57.9426\n",
      "[    LOSS] final-epoch mean = 57.5312  |  std = 1.5341\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 56.6095  |  std = 1.2071\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.9427 | Test-MSE 340.6805 | Regret 0.1571 | Fair 80.0949\n",
      "Epoch 10/30 | Train-Loss 75.6314 | Test-MSE 315.7098 | Regret 0.1054 | Fair 66.9394\n",
      "Epoch 20/30 | Train-Loss 73.7400 | Test-MSE 310.2985 | Regret 0.0874 | Fair 66.5511\n",
      "Epoch 30/30 | Train-Loss 77.1148 | Test-MSE 306.9650 | Regret 0.0758 | Fair 68.5329\n",
      "Epoch 01/30 | Train-Loss 97.5358 | Test-MSE 341.6537 | Regret 0.1550 | Fair 80.8845\n",
      "Epoch 10/30 | Train-Loss 78.6093 | Test-MSE 318.1295 | Regret 0.1093 | Fair 68.0290\n",
      "Epoch 20/30 | Train-Loss 76.2550 | Test-MSE 313.0290 | Regret 0.0907 | Fair 67.7599\n",
      "Epoch 30/30 | Train-Loss 77.9645 | Test-MSE 308.7429 | Regret 0.0781 | Fair 69.2088\n",
      "Epoch 01/30 | Train-Loss 97.2019 | Test-MSE 340.0305 | Regret 0.1499 | Fair 78.5902\n",
      "Epoch 10/30 | Train-Loss 78.5651 | Test-MSE 317.5373 | Regret 0.1071 | Fair 66.6270\n",
      "Epoch 20/30 | Train-Loss 75.5554 | Test-MSE 310.9568 | Regret 0.0878 | Fair 65.6754\n",
      "Epoch 30/30 | Train-Loss 77.8731 | Test-MSE 308.3284 | Regret 0.0755 | Fair 67.7562\n",
      "[    LOSS] final-epoch mean = 77.6508  |  std = 0.3808\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 68.4993  |  std = 0.5935\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 112.2239 | Test-MSE 340.6135 | Regret 0.1079 | Fair 80.1107\n",
      "Epoch 10/30 | Train-Loss 81.9478 | Test-MSE 309.7021 | Regret 0.0699 | Fair 63.4320\n",
      "Epoch 20/30 | Train-Loss 67.4527 | Test-MSE 293.2859 | Regret 0.0573 | Fair 53.7726\n",
      "Epoch 30/30 | Train-Loss 62.6560 | Test-MSE 286.7802 | Regret 0.0493 | Fair 52.7241\n",
      "Epoch 01/30 | Train-Loss 113.7982 | Test-MSE 341.5659 | Regret 0.1046 | Fair 80.9170\n",
      "Epoch 10/30 | Train-Loss 84.2585 | Test-MSE 311.7730 | Regret 0.0722 | Fair 64.4054\n",
      "Epoch 20/30 | Train-Loss 71.2187 | Test-MSE 297.5720 | Regret 0.0590 | Fair 55.2862\n",
      "Epoch 30/30 | Train-Loss 66.6287 | Test-MSE 290.6397 | Regret 0.0504 | Fair 54.7893\n",
      "Epoch 01/30 | Train-Loss 112.8817 | Test-MSE 339.8916 | Regret 0.1000 | Fair 78.4000\n",
      "Epoch 10/30 | Train-Loss 83.3007 | Test-MSE 310.9011 | Regret 0.0708 | Fair 62.8102\n",
      "Epoch 20/30 | Train-Loss 69.8314 | Test-MSE 296.3110 | Regret 0.0579 | Fair 56.1629\n",
      "Epoch 30/30 | Train-Loss 65.5695 | Test-MSE 290.2194 | Regret 0.0499 | Fair 56.5277\n",
      "[    LOSS] final-epoch mean = 64.9514  |  std = 1.6797\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 54.6804  |  std = 1.5548\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 95.8030 | Test-MSE 340.6805 | Regret 0.3530 | Fair 80.0948\n",
      "Epoch 10/30 | Train-Loss 75.5273 | Test-MSE 315.4717 | Regret 0.2281 | Fair 66.4464\n",
      "Epoch 20/30 | Train-Loss 73.9158 | Test-MSE 310.0607 | Regret 0.1877 | Fair 65.8236\n",
      "Epoch 30/30 | Train-Loss 77.2415 | Test-MSE 306.8338 | Regret 0.1621 | Fair 68.0927\n",
      "Epoch 01/30 | Train-Loss 97.3948 | Test-MSE 341.6537 | Regret 0.3471 | Fair 80.8846\n",
      "Epoch 10/30 | Train-Loss 78.5151 | Test-MSE 318.0392 | Regret 0.2353 | Fair 67.7644\n",
      "Epoch 20/30 | Train-Loss 76.5820 | Test-MSE 312.8951 | Regret 0.1951 | Fair 67.1384\n",
      "Epoch 30/30 | Train-Loss 77.8694 | Test-MSE 308.6572 | Regret 0.1674 | Fair 68.6945\n",
      "Epoch 01/30 | Train-Loss 97.0681 | Test-MSE 340.0305 | Regret 0.3328 | Fair 78.5904\n",
      "Epoch 10/30 | Train-Loss 78.5843 | Test-MSE 317.4532 | Regret 0.2299 | Fair 66.5184\n",
      "Epoch 20/30 | Train-Loss 75.9874 | Test-MSE 311.1502 | Regret 0.1877 | Fair 65.8747\n",
      "Epoch 30/30 | Train-Loss 78.1206 | Test-MSE 308.4918 | Regret 0.1618 | Fair 68.4593\n",
      "[    LOSS] final-epoch mean = 77.7438  |  std = 0.3697\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 68.4155  |  std = 0.2476\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 918.3298 | Test-MSE 340.6135 | Regret 0.2221 | Fair 80.1107\n",
      "Epoch 10/30 | Train-Loss 563.9587 | Test-MSE 309.8607 | Regret 0.1438 | Fair 63.2583\n",
      "Epoch 20/30 | Train-Loss 436.3583 | Test-MSE 292.6831 | Regret 0.1175 | Fair 53.2400\n",
      "Epoch 30/30 | Train-Loss 375.5227 | Test-MSE 285.6093 | Regret 0.1031 | Fair 51.4129\n",
      "Epoch 01/30 | Train-Loss 914.6523 | Test-MSE 341.5659 | Regret 0.2167 | Fair 80.9170\n",
      "Epoch 10/30 | Train-Loss 583.5933 | Test-MSE 311.7619 | Regret 0.1480 | Fair 64.1849\n",
      "Epoch 20/30 | Train-Loss 455.5044 | Test-MSE 296.8894 | Regret 0.1205 | Fair 54.6447\n",
      "Epoch 30/30 | Train-Loss 388.1050 | Test-MSE 289.4665 | Regret 0.1054 | Fair 53.5461\n",
      "Epoch 01/30 | Train-Loss 886.9979 | Test-MSE 339.8174 | Regret 0.2079 | Fair 78.4081\n",
      "Epoch 10/30 | Train-Loss 566.4116 | Test-MSE 310.9540 | Regret 0.1448 | Fair 62.5484\n",
      "Epoch 20/30 | Train-Loss 438.4414 | Test-MSE 295.6365 | Regret 0.1187 | Fair 55.1882\n",
      "Epoch 30/30 | Train-Loss 376.4626 | Test-MSE 288.8583 | Regret 0.1043 | Fair 55.2914\n",
      "[    LOSS] final-epoch mean = 380.0301  |  std = 5.7227\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 53.4168  |  std = 1.5860\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 193.6021 | Test-MSE 340.7641 | Regret 0.1190 | Fair 160.0309\n",
      "Epoch 10/30 | Train-Loss 151.7429 | Test-MSE 316.7935 | Regret 0.0865 | Fair 136.3935\n",
      "Epoch 20/30 | Train-Loss 143.6551 | Test-MSE 311.2867 | Regret 0.0732 | Fair 135.5090\n",
      "Epoch 30/30 | Train-Loss 147.9177 | Test-MSE 307.6521 | Regret 0.0642 | Fair 138.0969\n",
      "Epoch 01/30 | Train-Loss 196.8253 | Test-MSE 341.7054 | Regret 0.1190 | Fair 161.7662\n",
      "Epoch 10/30 | Train-Loss 156.9566 | Test-MSE 318.7535 | Regret 0.0915 | Fair 136.9955\n",
      "Epoch 20/30 | Train-Loss 148.0992 | Test-MSE 313.7587 | Regret 0.0756 | Fair 136.7002\n",
      "Epoch 30/30 | Train-Loss 150.2477 | Test-MSE 308.9955 | Regret 0.0655 | Fair 138.6110\n",
      "Epoch 01/30 | Train-Loss 196.0641 | Test-MSE 340.0304 | Regret 0.1173 | Fair 157.1802\n",
      "Epoch 10/30 | Train-Loss 156.5342 | Test-MSE 318.1606 | Regret 0.0906 | Fair 134.6610\n",
      "Epoch 20/30 | Train-Loss 146.2180 | Test-MSE 311.3828 | Regret 0.0734 | Fair 132.3718\n",
      "Epoch 30/30 | Train-Loss 149.6222 | Test-MSE 308.2544 | Regret 0.0631 | Fair 135.5207\n",
      "[    LOSS] final-epoch mean = 149.2625  |  std = 0.9846\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 137.4095  |  std = 1.3520\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.4853 | Test-MSE 340.7039 | Regret 0.1147 | Fair 160.2368\n",
      "Epoch 10/30 | Train-Loss 140.4699 | Test-MSE 309.8002 | Regret 0.0676 | Fair 130.7407\n",
      "Epoch 20/30 | Train-Loss 116.8780 | Test-MSE 295.9592 | Regret 0.0566 | Fair 115.2795\n",
      "Epoch 30/30 | Train-Loss 107.5259 | Test-MSE 287.7383 | Regret 0.0519 | Fair 112.3002\n",
      "Epoch 01/30 | Train-Loss 194.6689 | Test-MSE 341.7417 | Regret 0.1086 | Fair 161.8699\n",
      "Epoch 10/30 | Train-Loss 146.0613 | Test-MSE 311.8926 | Regret 0.0703 | Fair 132.9063\n",
      "Epoch 20/30 | Train-Loss 123.7379 | Test-MSE 299.9689 | Regret 0.0592 | Fair 119.0070\n",
      "Epoch 30/30 | Train-Loss 113.5727 | Test-MSE 292.0424 | Regret 0.0536 | Fair 116.8280\n",
      "Epoch 01/30 | Train-Loss 194.0223 | Test-MSE 340.0729 | Regret 0.0994 | Fair 157.6642\n",
      "Epoch 10/30 | Train-Loss 143.5903 | Test-MSE 310.6141 | Regret 0.0694 | Fair 131.3128\n",
      "Epoch 20/30 | Train-Loss 121.3277 | Test-MSE 297.8852 | Regret 0.0597 | Fair 119.0397\n",
      "Epoch 30/30 | Train-Loss 112.1125 | Test-MSE 290.3256 | Regret 0.0553 | Fair 117.3575\n",
      "[    LOSS] final-epoch mean = 111.0704  |  std = 2.5762\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 115.4952  |  std = 2.2695\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 192.1041 | Test-MSE 340.7068 | Regret 0.1089 | Fair 160.1478\n",
      "Epoch 10/30 | Train-Loss 151.3451 | Test-MSE 316.1238 | Regret 0.0761 | Fair 135.1488\n",
      "Epoch 20/30 | Train-Loss 146.2381 | Test-MSE 310.6775 | Regret 0.0636 | Fair 134.6291\n",
      "Epoch 30/30 | Train-Loss 152.6353 | Test-MSE 307.2025 | Regret 0.0554 | Fair 137.8757\n",
      "Epoch 01/30 | Train-Loss 195.2947 | Test-MSE 341.6537 | Regret 0.1080 | Fair 161.7690\n",
      "Epoch 10/30 | Train-Loss 157.0568 | Test-MSE 318.3612 | Regret 0.0794 | Fair 136.6693\n",
      "Epoch 20/30 | Train-Loss 151.0229 | Test-MSE 313.2743 | Regret 0.0659 | Fair 136.5955\n",
      "Epoch 30/30 | Train-Loss 154.4823 | Test-MSE 308.7423 | Regret 0.0567 | Fair 138.9587\n",
      "Epoch 01/30 | Train-Loss 194.6162 | Test-MSE 340.0304 | Regret 0.1054 | Fair 157.1802\n",
      "Epoch 10/30 | Train-Loss 156.7337 | Test-MSE 317.7393 | Regret 0.0781 | Fair 133.9089\n",
      "Epoch 20/30 | Train-Loss 149.2663 | Test-MSE 310.9457 | Regret 0.0640 | Fair 131.9452\n",
      "Epoch 30/30 | Train-Loss 153.9709 | Test-MSE 308.2675 | Regret 0.0551 | Fair 135.4186\n",
      "[    LOSS] final-epoch mean = 153.6962  |  std = 0.7787\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 137.4177  |  std = 1.4811\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.9109 | Test-MSE 340.6559 | Regret 0.0335 | Fair 160.2284\n",
      "Epoch 10/30 | Train-Loss 142.6190 | Test-MSE 309.7896 | Regret 0.0211 | Fair 128.4113\n",
      "Epoch 20/30 | Train-Loss 118.6289 | Test-MSE 295.1635 | Regret 0.0175 | Fair 110.8701\n",
      "Epoch 30/30 | Train-Loss 110.6546 | Test-MSE 287.8890 | Regret 0.0149 | Fair 110.0389\n",
      "Epoch 01/30 | Train-Loss 195.0995 | Test-MSE 341.6232 | Regret 0.0319 | Fair 161.8520\n",
      "Epoch 10/30 | Train-Loss 147.4353 | Test-MSE 311.7780 | Regret 0.0218 | Fair 130.3737\n",
      "Epoch 20/30 | Train-Loss 125.5932 | Test-MSE 299.2923 | Regret 0.0181 | Fair 114.3337\n",
      "Epoch 30/30 | Train-Loss 117.8233 | Test-MSE 291.6497 | Regret 0.0152 | Fair 113.7327\n",
      "Epoch 01/30 | Train-Loss 194.4337 | Test-MSE 339.9308 | Regret 0.0300 | Fair 157.1338\n",
      "Epoch 10/30 | Train-Loss 145.1460 | Test-MSE 310.7772 | Regret 0.0214 | Fair 128.1176\n",
      "Epoch 20/30 | Train-Loss 122.6717 | Test-MSE 297.6441 | Regret 0.0178 | Fair 115.9098\n",
      "Epoch 30/30 | Train-Loss 116.1773 | Test-MSE 291.0777 | Regret 0.0150 | Fair 115.8851\n",
      "[    LOSS] final-epoch mean = 114.8851  |  std = 3.0660\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 113.2189  |  std = 2.4142\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.6721 | Test-MSE 340.6805 | Regret 0.1571 | Fair 160.1897\n",
      "Epoch 10/30 | Train-Loss 151.1443 | Test-MSE 315.7098 | Regret 0.1054 | Fair 133.8788\n",
      "Epoch 20/30 | Train-Loss 147.3874 | Test-MSE 310.2985 | Regret 0.0874 | Fair 133.1022\n",
      "Epoch 30/30 | Train-Loss 154.1541 | Test-MSE 306.9650 | Regret 0.0758 | Fair 137.0657\n",
      "Epoch 01/30 | Train-Loss 194.8566 | Test-MSE 341.6537 | Regret 0.1550 | Fair 161.7690\n",
      "Epoch 10/30 | Train-Loss 157.0962 | Test-MSE 318.1295 | Regret 0.1093 | Fair 136.0581\n",
      "Epoch 20/30 | Train-Loss 152.4149 | Test-MSE 313.0290 | Regret 0.0907 | Fair 135.5197\n",
      "Epoch 30/30 | Train-Loss 155.8500 | Test-MSE 308.7429 | Regret 0.0781 | Fair 138.4175\n",
      "Epoch 01/30 | Train-Loss 194.2002 | Test-MSE 340.0305 | Regret 0.1499 | Fair 157.1803\n",
      "Epoch 10/30 | Train-Loss 157.0137 | Test-MSE 317.5373 | Regret 0.1071 | Fair 133.2540\n",
      "Epoch 20/30 | Train-Loss 151.0202 | Test-MSE 310.9568 | Regret 0.0878 | Fair 131.3508\n",
      "Epoch 30/30 | Train-Loss 155.6709 | Test-MSE 308.3284 | Regret 0.0755 | Fair 135.5124\n",
      "[    LOSS] final-epoch mean = 155.2250  |  std = 0.7608\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 136.9985  |  std = 1.1870\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 207.9533 | Test-MSE 340.6135 | Regret 0.1079 | Fair 160.2215\n",
      "Epoch 10/30 | Train-Loss 153.8717 | Test-MSE 309.7021 | Regret 0.0699 | Fair 126.8639\n",
      "Epoch 20/30 | Train-Loss 127.0984 | Test-MSE 293.2859 | Regret 0.0573 | Fair 107.5453\n",
      "Epoch 30/30 | Train-Loss 118.7558 | Test-MSE 286.7802 | Regret 0.0493 | Fair 105.4481\n",
      "Epoch 01/30 | Train-Loss 211.1190 | Test-MSE 341.5659 | Regret 0.1046 | Fair 161.8339\n",
      "Epoch 10/30 | Train-Loss 158.1506 | Test-MSE 311.7730 | Regret 0.0722 | Fair 128.8109\n",
      "Epoch 20/30 | Train-Loss 134.3188 | Test-MSE 297.5720 | Regret 0.0590 | Fair 110.5725\n",
      "Epoch 30/30 | Train-Loss 126.5118 | Test-MSE 290.6397 | Regret 0.0504 | Fair 109.5786\n",
      "Epoch 01/30 | Train-Loss 209.8800 | Test-MSE 339.8916 | Regret 0.1000 | Fair 156.8000\n",
      "Epoch 10/30 | Train-Loss 156.5612 | Test-MSE 310.9011 | Regret 0.0708 | Fair 125.6205\n",
      "Epoch 20/30 | Train-Loss 131.8593 | Test-MSE 296.3110 | Regret 0.0579 | Fair 112.3259\n",
      "Epoch 30/30 | Train-Loss 124.6250 | Test-MSE 290.2194 | Regret 0.0499 | Fair 113.0555\n",
      "[    LOSS] final-epoch mean = 123.2975  |  std = 3.3026\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 109.3607  |  std = 3.1095\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 191.5324 | Test-MSE 340.6805 | Regret 0.3530 | Fair 160.1897\n",
      "Epoch 10/30 | Train-Loss 151.0149 | Test-MSE 315.4717 | Regret 0.2281 | Fair 132.8928\n",
      "Epoch 20/30 | Train-Loss 147.8009 | Test-MSE 310.0607 | Regret 0.1877 | Fair 131.6472\n",
      "Epoch 30/30 | Train-Loss 154.4579 | Test-MSE 306.8338 | Regret 0.1621 | Fair 136.1854\n",
      "Epoch 01/30 | Train-Loss 194.7156 | Test-MSE 341.6537 | Regret 0.3471 | Fair 161.7692\n",
      "Epoch 10/30 | Train-Loss 156.9893 | Test-MSE 318.0392 | Regret 0.2353 | Fair 135.5287\n",
      "Epoch 20/30 | Train-Loss 153.1324 | Test-MSE 312.8951 | Regret 0.1951 | Fair 134.2769\n",
      "Epoch 30/30 | Train-Loss 155.7127 | Test-MSE 308.6572 | Regret 0.1674 | Fair 137.3889\n",
      "Epoch 01/30 | Train-Loss 194.0665 | Test-MSE 340.0305 | Regret 0.3328 | Fair 157.1807\n",
      "Epoch 10/30 | Train-Loss 157.1298 | Test-MSE 317.4532 | Regret 0.2299 | Fair 133.0367\n",
      "Epoch 20/30 | Train-Loss 151.9448 | Test-MSE 311.1502 | Regret 0.1877 | Fair 131.7494\n",
      "Epoch 30/30 | Train-Loss 156.2163 | Test-MSE 308.4918 | Regret 0.1618 | Fair 136.9185\n",
      "[    LOSS] final-epoch mean = 155.4623  |  std = 0.7394\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 136.8310  |  std = 0.4952\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'acc_parity'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 1014.0592 | Test-MSE 340.6135 | Regret 0.2221 | Fair 160.2215\n",
      "Epoch 10/30 | Train-Loss 636.3126 | Test-MSE 309.8607 | Regret 0.1438 | Fair 126.5167\n",
      "Epoch 20/30 | Train-Loss 496.2677 | Test-MSE 292.6831 | Regret 0.1175 | Fair 106.4800\n",
      "Epoch 30/30 | Train-Loss 431.4764 | Test-MSE 285.6093 | Regret 0.1031 | Fair 102.8258\n",
      "Epoch 01/30 | Train-Loss 1011.9731 | Test-MSE 341.5659 | Regret 0.2167 | Fair 161.8339\n",
      "Epoch 10/30 | Train-Loss 657.8289 | Test-MSE 311.7619 | Regret 0.1480 | Fair 128.3698\n",
      "Epoch 20/30 | Train-Loss 518.7823 | Test-MSE 296.8894 | Regret 0.1205 | Fair 109.2895\n",
      "Epoch 30/30 | Train-Loss 447.5757 | Test-MSE 289.4665 | Regret 0.1054 | Fair 107.0922\n",
      "Epoch 01/30 | Train-Loss 983.9962 | Test-MSE 339.8174 | Regret 0.2079 | Fair 156.8162\n",
      "Epoch 10/30 | Train-Loss 640.3977 | Test-MSE 310.9540 | Regret 0.1448 | Fair 125.0969\n",
      "Epoch 20/30 | Train-Loss 500.7744 | Test-MSE 295.6365 | Regret 0.1187 | Fair 110.3764\n",
      "Epoch 30/30 | Train-Loss 434.9532 | Test-MSE 288.8583 | Regret 0.1043 | Fair 110.5829\n",
      "[    LOSS] final-epoch mean = 438.0017  |  std = 6.9170\n",
      "[     MSE] final-epoch mean = 287.9780  |  std = 1.6933\n",
      "[  REGRET] final-epoch mean = 0.1043  |  std = 0.0009\n",
      "[FAIRNESS] final-epoch mean = 106.8336  |  std = 3.1721\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 2.1490 | Test-MSE 340.7641 | Regret 0.1190 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.1995 | Test-MSE 316.7935 | Regret 0.0865 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.9463 | Test-MSE 311.2867 | Regret 0.0732 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.7627 | Test-MSE 307.6521 | Regret 0.0642 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 2.1896 | Test-MSE 341.7054 | Regret 0.1190 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 1.2422 | Test-MSE 318.7535 | Regret 0.0915 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.9852 | Test-MSE 313.7587 | Regret 0.0756 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.8004 | Test-MSE 308.9955 | Regret 0.0655 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 2.0734 | Test-MSE 340.0304 | Regret 0.1173 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 1.1959 | Test-MSE 318.1606 | Regret 0.0906 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.9328 | Test-MSE 311.3828 | Regret 0.0734 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.7675 | Test-MSE 308.2544 | Regret 0.0631 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.7768  |  std = 0.0168\n",
      "[     MSE] final-epoch mean = 308.3007  |  std = 0.5494\n",
      "[  REGRET] final-epoch mean = 0.0643  |  std = 0.0010\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0419 | Test-MSE 340.7039 | Regret 0.1147 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 0.0258 | Test-MSE 309.8002 | Regret 0.0676 | Fair 0.0111\n",
      "Epoch 20/30 | Train-Loss 0.0194 | Test-MSE 295.9592 | Regret 0.0566 | Fair 0.0096\n",
      "Epoch 30/30 | Train-Loss 0.0165 | Test-MSE 287.7383 | Regret 0.0519 | Fair 0.0097\n",
      "Epoch 01/30 | Train-Loss 0.0431 | Test-MSE 341.7417 | Regret 0.1086 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 0.0268 | Test-MSE 311.8926 | Regret 0.0703 | Fair 0.0113\n",
      "Epoch 20/30 | Train-Loss 0.0204 | Test-MSE 299.9689 | Regret 0.0592 | Fair 0.0099\n",
      "Epoch 30/30 | Train-Loss 0.0174 | Test-MSE 292.0424 | Regret 0.0536 | Fair 0.0101\n",
      "Epoch 01/30 | Train-Loss 0.0415 | Test-MSE 340.0729 | Regret 0.0994 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 0.0260 | Test-MSE 310.6141 | Regret 0.0694 | Fair 0.0111\n",
      "Epoch 20/30 | Train-Loss 0.0198 | Test-MSE 297.8852 | Regret 0.0597 | Fair 0.0101\n",
      "Epoch 30/30 | Train-Loss 0.0170 | Test-MSE 290.3256 | Regret 0.0553 | Fair 0.0103\n",
      "[    LOSS] final-epoch mean = 0.0170  |  std = 0.0004\n",
      "[     MSE] final-epoch mean = 290.0354  |  std = 1.7691\n",
      "[  REGRET] final-epoch mean = 0.0536  |  std = 0.0014\n",
      "[FAIRNESS] final-epoch mean = 0.0100  |  std = 0.0003\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.6511 | Test-MSE 340.7068 | Regret 0.1089 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.3690 | Test-MSE 316.1238 | Regret 0.0761 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.2915 | Test-MSE 310.6775 | Regret 0.0636 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.2382 | Test-MSE 307.2025 | Regret 0.0554 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 0.6590 | Test-MSE 341.6537 | Regret 0.1080 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.3817 | Test-MSE 318.3612 | Regret 0.0794 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.3010 | Test-MSE 313.2743 | Regret 0.0659 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.2496 | Test-MSE 308.7423 | Regret 0.0567 | Fair 0.0046\n",
      "Epoch 01/30 | Train-Loss 0.6255 | Test-MSE 340.0304 | Regret 0.1054 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.3653 | Test-MSE 317.7393 | Regret 0.0781 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.2866 | Test-MSE 310.9457 | Regret 0.0640 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.2390 | Test-MSE 308.2675 | Regret 0.0551 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.2423  |  std = 0.0052\n",
      "[     MSE] final-epoch mean = 308.0708  |  std = 0.6439\n",
      "[  REGRET] final-epoch mean = 0.0557  |  std = 0.0007\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.4675 | Test-MSE 340.6559 | Regret 0.0335 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 0.2833 | Test-MSE 309.7896 | Regret 0.0211 | Fair 0.0108\n",
      "Epoch 20/30 | Train-Loss 0.2196 | Test-MSE 295.1635 | Regret 0.0175 | Fair 0.0090\n",
      "Epoch 30/30 | Train-Loss 0.1831 | Test-MSE 287.8890 | Regret 0.0149 | Fair 0.0093\n",
      "Epoch 01/30 | Train-Loss 0.4737 | Test-MSE 341.6232 | Regret 0.0319 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 0.2920 | Test-MSE 311.7780 | Regret 0.0218 | Fair 0.0109\n",
      "Epoch 20/30 | Train-Loss 0.2273 | Test-MSE 299.2923 | Regret 0.0181 | Fair 0.0093\n",
      "Epoch 30/30 | Train-Loss 0.1906 | Test-MSE 291.6497 | Regret 0.0152 | Fair 0.0096\n",
      "Epoch 01/30 | Train-Loss 0.4529 | Test-MSE 339.9308 | Regret 0.0300 | Fair 0.0131\n",
      "Epoch 10/30 | Train-Loss 0.2829 | Test-MSE 310.7772 | Regret 0.0214 | Fair 0.0107\n",
      "Epoch 20/30 | Train-Loss 0.2208 | Test-MSE 297.6441 | Regret 0.0178 | Fair 0.0096\n",
      "Epoch 30/30 | Train-Loss 0.1839 | Test-MSE 291.0777 | Regret 0.0150 | Fair 0.0100\n",
      "[    LOSS] final-epoch mean = 0.1859  |  std = 0.0034\n",
      "[     MSE] final-epoch mean = 290.2055  |  std = 1.6545\n",
      "[  REGRET] final-epoch mean = 0.0150  |  std = 0.0002\n",
      "[FAIRNESS] final-epoch mean = 0.0096  |  std = 0.0003\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.2190 | Test-MSE 340.6805 | Regret 0.1571 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.1229 | Test-MSE 315.7098 | Regret 0.1054 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.0969 | Test-MSE 310.2985 | Regret 0.0874 | Fair 0.0042\n",
      "Epoch 30/30 | Train-Loss 0.0803 | Test-MSE 306.9650 | Regret 0.0758 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.2209 | Test-MSE 341.6537 | Regret 0.1550 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.1270 | Test-MSE 318.1295 | Regret 0.1093 | Fair 0.0042\n",
      "Epoch 20/30 | Train-Loss 0.0997 | Test-MSE 313.0290 | Regret 0.0907 | Fair 0.0043\n",
      "Epoch 30/30 | Train-Loss 0.0838 | Test-MSE 308.7429 | Regret 0.0781 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.2095 | Test-MSE 340.0305 | Regret 0.1499 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.1212 | Test-MSE 317.5373 | Regret 0.1071 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0952 | Test-MSE 310.9568 | Regret 0.0878 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0802 | Test-MSE 308.3284 | Regret 0.0755 | Fair 0.0044\n",
      "[    LOSS] final-epoch mean = 0.0815  |  std = 0.0017\n",
      "[     MSE] final-epoch mean = 308.0121  |  std = 0.7595\n",
      "[  REGRET] final-epoch mean = 0.0765  |  std = 0.0011\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0001\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 16.5099 | Test-MSE 340.6135 | Regret 0.1079 | Fair 0.0135\n",
      "Epoch 10/30 | Train-Loss 10.0353 | Test-MSE 309.7021 | Regret 0.0699 | Fair 0.0105\n",
      "Epoch 20/30 | Train-Loss 7.8160 | Test-MSE 293.2859 | Regret 0.0573 | Fair 0.0086\n",
      "Epoch 30/30 | Train-Loss 6.5648 | Test-MSE 286.7802 | Regret 0.0493 | Fair 0.0087\n",
      "Epoch 01/30 | Train-Loss 16.4932 | Test-MSE 341.5659 | Regret 0.1046 | Fair 0.0137\n",
      "Epoch 10/30 | Train-Loss 10.3781 | Test-MSE 311.7730 | Regret 0.0722 | Fair 0.0107\n",
      "Epoch 20/30 | Train-Loss 8.1284 | Test-MSE 297.5720 | Regret 0.0590 | Fair 0.0088\n",
      "Epoch 30/30 | Train-Loss 6.7550 | Test-MSE 290.6397 | Regret 0.0504 | Fair 0.0091\n",
      "Epoch 01/30 | Train-Loss 15.8991 | Test-MSE 339.8916 | Regret 0.1000 | Fair 0.0130\n",
      "Epoch 10/30 | Train-Loss 10.0517 | Test-MSE 310.9011 | Regret 0.0708 | Fair 0.0103\n",
      "Epoch 20/30 | Train-Loss 7.8130 | Test-MSE 296.3110 | Regret 0.0579 | Fair 0.0092\n",
      "Epoch 30/30 | Train-Loss 6.5230 | Test-MSE 290.2194 | Regret 0.0499 | Fair 0.0096\n",
      "[    LOSS] final-epoch mean = 6.6143  |  std = 0.1010\n",
      "[     MSE] final-epoch mean = 289.2131  |  std = 1.7289\n",
      "[  REGRET] final-epoch mean = 0.0499  |  std = 0.0005\n",
      "[FAIRNESS] final-epoch mean = 0.0091  |  std = 0.0004\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 01/30 | Train-Loss 0.0794 | Test-MSE 340.6805 | Regret 0.3530 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.0441 | Test-MSE 315.4717 | Regret 0.2281 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0350 | Test-MSE 310.0607 | Regret 0.1877 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0298 | Test-MSE 306.8338 | Regret 0.1621 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.0799 | Test-MSE 341.6537 | Regret 0.3471 | Fair 0.0050\n",
      "Epoch 10/30 | Train-Loss 0.0455 | Test-MSE 318.0392 | Regret 0.2353 | Fair 0.0041\n",
      "Epoch 20/30 | Train-Loss 0.0361 | Test-MSE 312.8951 | Regret 0.1951 | Fair 0.0042\n",
      "Epoch 30/30 | Train-Loss 0.0310 | Test-MSE 308.6572 | Regret 0.1674 | Fair 0.0045\n",
      "Epoch 01/30 | Train-Loss 0.0757 | Test-MSE 340.0305 | Regret 0.3328 | Fair 0.0048\n",
      "Epoch 10/30 | Train-Loss 0.0435 | Test-MSE 317.4532 | Regret 0.2299 | Fair 0.0040\n",
      "Epoch 20/30 | Train-Loss 0.0345 | Test-MSE 311.1502 | Regret 0.1877 | Fair 0.0041\n",
      "Epoch 30/30 | Train-Loss 0.0298 | Test-MSE 308.4918 | Regret 0.1618 | Fair 0.0045\n",
      "[    LOSS] final-epoch mean = 0.0302  |  std = 0.0005\n",
      "[     MSE] final-epoch mean = 307.9943  |  std = 0.8233\n",
      "[  REGRET] final-epoch mean = 0.1638  |  std = 0.0026\n",
      "[FAIRNESS] final-epoch mean = 0.0045  |  std = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Beta': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6115 | Test-MSE 367.6161 | Regret 0.2828 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 36.1008 | Test-MSE 349.4561 | Regret 0.1767 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 22.7728 | Test-MSE 343.4800 | Regret 0.1330 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 13.2802 | Test-MSE 345.7516 | Regret 0.0876 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 8.0597 | Test-MSE 344.2115 | Regret 0.0641 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 6.4612 | Test-MSE 342.6795 | Regret 0.0527 | Fair-Val 0.0049\n",
      "Training finished in 102.28s.\n",
      "Epoch 001/50 | Train-Loss 79.0285 | Test-MSE 367.9062 | Regret 0.2960 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 38.1091 | Test-MSE 350.9059 | Regret 0.1852 | Fair-Val 0.0042\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ===== REGRET-BASED TRAINING AND EXPERIMENT HARNESS\n",
    "# ==============================================================================\n",
    "\n",
    "def train_many_trials_regret(\n",
    "        n_trials=10, base_seed=2025, **kwargs):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` and average the results.\n",
    "    Accepts all arguments for `train_model_regret` via **kwargs.\n",
    "    \"\"\"\n",
    "    trials_logs = []\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        _, logs = train_model_regret(**kwargs)\n",
    "        trials_logs.append(logs)\n",
    "\n",
    "    keys = [\"loss_log\", \"mse_log\", \"regret_log\", \"fairness_log\"]\n",
    "    avg_logs = {}\n",
    "    for k in keys:\n",
    "        if trials_logs[0][k] is None or not trials_logs[0][k]:\n",
    "            avg_logs[k] = [0.0]\n",
    "            continue\n",
    "        stack = np.vstack([trial[k] for trial in trials_logs])\n",
    "        avg_logs[k] = stack.mean(axis=0).tolist()\n",
    "        std_k = stack.std(axis=0)[-1]\n",
    "        mean_k = avg_logs[k][-1]\n",
    "        print(f\"[{k.replace('_log','').upper():>8s}] final-epoch mean = {mean_k:.4f}  |  std = {std_k:.4f}\")\n",
    "\n",
    "    return avg_logs\n",
    "\n",
    "\n",
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 1, 1.5, 2]\n",
    "fairness_lambdas = [0, 1.0]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form', 'finite-diff'] # New parameter\n",
    "results_list = []\n",
    "\n",
    "# --- Grid Search Execution ---\n",
    "for group in group_settings:\n",
    "    # UPDATED: Added 'acc_parity' for the group case\n",
    "    if group:\n",
    "        fairness_types = ['mad', 'acc_parity', 'atkinson']\n",
    "    else:\n",
    "        fairness_types = ['mad', 'atkinson']\n",
    "    \n",
    "    for grad_method in grad_methods:\n",
    "        for lam in fairness_lambdas:\n",
    "            for fairness in fairness_types:\n",
    "                if lam == 0 and fairness != fairness_types[0]: continue\n",
    "\n",
    "                for alpha in alphas:\n",
    "                    \n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method, # New column\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = {\n",
    "                        'X_train': feats_train, 'y_train': b_train, 'race_train': race_train, 'cost_train': cost_train, 'gainF_train': gainF_train,\n",
    "                        'X_test': feats_test, 'y_test': b_test, 'race_test': race_test, 'cost_test': cost_test, 'gainF_test': gainF_test,\n",
    "                        'model_class': FairRiskPredictor, 'input_dim': feats_train.shape[1],\n",
    "                        'alpha': alpha, 'Q': Q,\n",
    "                        'lambda_fair': lam, 'fairness_type': fairness,\n",
    "                        'group': group,\n",
    "                        'grad_method': grad_method, # Pass the current grad method\n",
    "                        'num_epochs': 30, 'lr': 0.01, 'batch_size': None,\n",
    "                    }\n",
    "\n",
    "                    avg_logs = train_many_trials_regret(n_trials=3, **train_args) # Reduced trials for speed\n",
    "\n",
    "                    final_metrics = run_params.copy()\n",
    "                    final_metrics['Final Regret'] = avg_logs['regret_log'][-1]\n",
    "                    final_metrics['Final MSE'] = avg_logs['mse_log'][-1]\n",
    "                    final_metrics['Final Fairness'] = avg_logs['fairness_log'][-1] if avg_logs.get('fairness_log') else 0.0\n",
    "                    results_list.append(final_metrics)\n",
    "\n",
    "# --- Results Presentation ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "# UPDATED: Added 'Grad Method' to the column order\n",
    "column_order = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness', 'Final Regret', 'Final MSE', 'Final Fairness']\n",
    "results_df = results_df[column_order]\n",
    "latex_table = results_df.to_latex(index=False, caption=\"Averaged Experimental Results Across Different Parameters.\", label=\"tab:avg_exp_results_expanded\", float_format=\"%.4f\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n--- Averaged Results Summary (Pandas DataFrame) ---\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"--- LaTeX Table Output ---\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1e66d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8e55bf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Grad Method</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Fairness</th>\n",
       "      <th>Final Regret</th>\n",
       "      <th>Final MSE</th>\n",
       "      <th>Final Fairness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.064265</td>\n",
       "      <td>308.300659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>290.035441</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>308.070760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>290.205475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>308.012095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.726426</td>\n",
       "      <td>299.421651</td>\n",
       "      <td>420.257935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.185934</td>\n",
       "      <td>310.066538</td>\n",
       "      <td>0.666265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>-1.015138</td>\n",
       "      <td>330.118174</td>\n",
       "      <td>0.658891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.133037</td>\n",
       "      <td>306.971313</td>\n",
       "      <td>0.663845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.719651</td>\n",
       "      <td>301.205332</td>\n",
       "      <td>0.662436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group  Grad Method  Alpha Beta  Lambda  Fairness  Final Regret  \\\n",
       "0    True  closed-form    0.5  0.5     0.0      None      0.064265   \n",
       "1    True  closed-form    0.5  1.5     0.0      None      0.053591   \n",
       "2    True  closed-form    1.0  0.5     0.0      None      0.055729   \n",
       "3    True  closed-form    1.0  1.5     0.0      None      0.015016   \n",
       "4    True  closed-form    1.5  0.5     0.0      None      0.076455   \n",
       "..    ...          ...    ...  ...     ...       ...           ...   \n",
       "83  False  finite-diff    2.0  N/A     1.0       mad      0.726426   \n",
       "84  False  finite-diff    0.5  N/A     1.0  atkinson      0.185934   \n",
       "85  False  finite-diff    1.0  N/A     1.0  atkinson     -1.015138   \n",
       "86  False  finite-diff    1.5  N/A     1.0  atkinson      0.133037   \n",
       "87  False  finite-diff    2.0  N/A     1.0  atkinson      0.719651   \n",
       "\n",
       "     Final MSE  Final Fairness  \n",
       "0   308.300659        0.000000  \n",
       "1   290.035441        0.000000  \n",
       "2   308.070760        0.000000  \n",
       "3   290.205475        0.000000  \n",
       "4   308.012095        0.000000  \n",
       "..         ...             ...  \n",
       "83  299.421651      420.257935  \n",
       "84  310.066538        0.666265  \n",
       "85  330.118174        0.658891  \n",
       "86  306.971313        0.663845  \n",
       "87  301.205332        0.662436  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3aa372",
   "metadata": {},
   "source": [
    "# Verify closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468d756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
