{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0469d29",
   "metadata": {},
   "source": [
    "- Change min-risk from 0.001 to 1\n",
    "\n",
    "- Write fold-opt subsection.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9450ea9",
   "metadata": {},
   "source": [
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "     - Closed-Form Done\n",
    "     - <b>2-Stage Done</b>\n",
    "\n",
    "6. <b>For Fold-OPT Change PGD closed-form to solver.</b>\n",
    "\n",
    "8. Verify Individual and Group Regret Performance Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "     AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6818df",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = pd.read_csv('E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\data\\\\data.csv')\n",
    "\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 1) \n",
    "benefit = benefit + 1\n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f20f2",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd6cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df314909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d661549",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37802e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ea0565",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d84524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([2.])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1000])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([7.5626])\n",
      "sol_group: tensor([7.5626])\n",
      "obj_group: tensor([-218.5607])\n",
      "obj_ind: tensor([-218.5607])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847c1c",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee06d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "def _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group):\n",
    "    \"\"\"Helper to compute regret and the decision variable d_hat.\"\"\"\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        regret = obj_val_at_d_star - obj_val_at_d_hat\n",
    "        # regret = np.log1p(np.exp(regret * 10)) / 10\n",
    "        return regret, d_hat_np\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        # Return a zero regret and a placeholder for d_hat\n",
    "        return 0.0, np.zeros_like(pred_r_np)\n",
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd Function for regret with a closed-form or finite-difference gradient.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method):\n",
    "        # --- Loss Calculation (Regret) ---\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(\n",
    "            lambda t: to_numpy_1d(t.detach()), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "\n",
    "        regret, d_hat_np = _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group)\n",
    "        regret_loss = torch.tensor(regret, dtype=pred_r.dtype, device=pred_r.device)\n",
    "        # regret_loss = F.softplus(torch.tensor(regret, dtype=pred_r.dtype,device=pred_r.device), beta=10)\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 8\n",
    "\n",
    "        try:\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                # (Closed-form gradient calculation remains the same)\n",
    "                if params['group']:\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    v_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    Jac_mat = solve_coupled_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    vT_J_np = v_np @ Jac_mat\n",
    "                    grad_regret = -torch.from_numpy(vT_J_np).to(pred_r.device,dtype=pred_r.dtype)\n",
    "\n",
    "                else:\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - params['alpha']) * d_hat ** (-params['alpha']) # Grad of alpha-fairness obj\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "            \n",
    "            elif params['grad_method'] == 'finite-diff':\n",
    "\n",
    "                pred_r_np = to_numpy_1d(pred_r)\n",
    "                grad_regret_np = np.zeros_like(pred_r_np)\n",
    "\n",
    "                eps = 1e-3                                    # relative 0.1 %\n",
    "                eps_vec = eps * np.maximum(1.0, np.abs(pred_r_np))\n",
    "\n",
    "                # Detach and convert tensors needed for perturbations once\n",
    "                true_r_np, gainF_np, cost_np, race_np = map(\n",
    "                    lambda t: to_numpy_1d(t.detach()), [true_r, gainF, cost, race]\n",
    "                )\n",
    "\n",
    "                for i in range(len(pred_r_np)):\n",
    "                    # Perturb pred_r for forward and backward steps\n",
    "                    pred_r_plus = pred_r_np.copy(); pred_r_plus[i]  += eps_vec[i]\n",
    "                    pred_r_minus = pred_r_np.copy(); pred_r_minus[i] -= eps_vec[i]\n",
    "\n",
    "                    regret_plus, _ = _calculate_regret_and_d_hat(pred_r_plus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "                    regret_minus, _ = _calculate_regret_and_d_hat(pred_r_minus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "\n",
    "                    grad_regret_np[i] = (regret_plus - regret_minus) / (2 * eps_vec[i])\n",
    "\n",
    "                # The gradient of the loss is the negative of the gradient of the regret\n",
    "                grad_regret = torch.from_numpy(grad_regret_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Gradient calculation failed: {e}. Returning zero grad.\")\n",
    "\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Regret loss from the custom function\n",
    "        regret_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method)\n",
    "\n",
    "        # 2. Fairness penalty using standard PyTorch autograd\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "        \n",
    "        # 3. Total loss\n",
    "        total_loss = regret_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a09dba",
   "metadata": {},
   "source": [
    "# Training Gemini Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "\n",
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor via direct regret minimization, logging detailed metrics\n",
    "    at each evaluation point.\n",
    "    \"\"\"\n",
    "    # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # --- Initialize Logs ---\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "    unique_groups = torch.unique(race_test).cpu().numpy()\n",
    "    per_group_mse_log = {g: [] for g in unique_groups}\n",
    "    per_group_obj_log = {g: [] for g in unique_groups}\n",
    "    per_group_true_benefit_log = {g: [] for g in unique_groups}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze().clamp(min=1e-4)\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # --- Periodic Evaluation on Test Set ---\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test).squeeze().clamp(min=1e-4)\n",
    "                # Overall MSE\n",
    "                mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "                mse_log.append(mse_val)\n",
    "\n",
    "                # Overall Regret\n",
    "                _, _, d_pred_np = _calculate_loss_and_decision(pred_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                _, _, d_true_np = _calculate_loss_and_decision(y_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                if d_pred_np is not None and d_true_np is not None:\n",
    "                    y_test_np = to_numpy_1d(y_test)\n",
    "                    race_test_np = to_numpy_1d(race_test)\n",
    "                    if group:\n",
    "                        true_obj = compute_coupled_group_obj(d_true_np, y_test_np, race_test_np, alpha)\n",
    "                        pred_obj = compute_coupled_group_obj(d_pred_np, y_test_np, race_test_np, alpha)\n",
    "                    else:\n",
    "                        true_obj = AlphaFairness(y_test_np * d_true_np, alpha)\n",
    "                        pred_obj = AlphaFairness(y_test_np * d_pred_np, alpha)\n",
    "                    norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "                else:\n",
    "                    norm_regret = np.nan\n",
    "                regret_log.append(norm_regret)\n",
    "\n",
    "                # Overall Fairness\n",
    "                fair_val = 0.0\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=0.5, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "                fairness_log.append(fair_val)\n",
    "\n",
    "                # Group-wise Metrics\n",
    "                for g in unique_groups:\n",
    "                    mask = (race_test == g)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    # Group MSE\n",
    "                    per_group_mse_log[g].append(((pred_test[mask] - y_test[mask]).pow(2)).mean().item())\n",
    "                    # Group True Benefit\n",
    "                    per_group_true_benefit_log[g].append(y_test[mask].mean().item())\n",
    "                    # Group Decision Objective\n",
    "                    if d_pred_np is not None:\n",
    "                        group_mask_np = (race_test_np == g)\n",
    "                        # We use the true benefits (y_test) to evaluate the utility of the decisions (d_pred_np)\n",
    "                        group_utility = y_test_np[group_mask_np] * d_pred_np[group_mask_np]\n",
    "                        # For simplicity, we report the mean utility as the objective\n",
    "                        per_group_obj_log[g].append(group_utility.mean())\n",
    "                    else:\n",
    "                        per_group_obj_log[g].append(np.nan)\n",
    "\n",
    "                print(f\"Epoch {epoch:03d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_log[-1]:.4f} | Regret {regret_log[-1]:.4f} | Fair-Val {fairness_log[-1]:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "    # Return a dictionary of all logs\n",
    "    return model, {\n",
    "        \"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log,\n",
    "        \"training_time\": total_time,\n",
    "        \"per_group_mse\": per_group_mse_log,\n",
    "        \"per_group_decision_objective\": per_group_obj_log,\n",
    "        \"per_group_true_benefit\": per_group_true_benefit_log\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8386a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a66413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha = 2\n",
    "# hyperparams = {\n",
    "#     \"alpha\":alpha,\n",
    "#     \"Q\": 1000,\n",
    "#     \"lambda_fair\": 0,\n",
    "#     \"fairness_type\": \"atkinson\",   \n",
    "#     \"group\": True,            # Set to True for group fairness, False for individual\n",
    "#     \"grad_method\": \"finite-diff\",\n",
    "#     \"num_epochs\": 50,        \n",
    "#     \"lr\": 0.005,\n",
    "#     \"batch_size\": len(b_train),\n",
    "#     \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# }\n",
    "\n",
    "# final_model, logs = train_model_regret(\n",
    "#     X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "#     X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "#     model_class=FairRiskPredictor,\n",
    "#     input_dim=feats_train.shape[1],\n",
    "#     **hyperparams\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1.  MULTI-TRIAL REGRET TRAINING WITH FULL LOGGING\n",
    "# ---------------------------------------------------------------------\n",
    "def train_many_trials_regret(n_trials=3, base_seed=2025, **train_args):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` different seeds.\n",
    "    Returns a FLAT dict whose keys are:\n",
    "        regret, regret_std, mse, mse_std, fairness, fairness_std, …,\n",
    "        G0_mse, G0_mse_std, G0_decision_obj, G0_decision_obj_std, …\n",
    "    \"\"\"\n",
    "    # -------------------- run all trials -----------------------------\n",
    "    per_trial_metrics = defaultdict(list)      # collects trial-level scalars\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        _, logs = train_model_regret(**train_args)   # one full run\n",
    "\n",
    "        # ---- overall scalars ---------------------------------------\n",
    "        per_trial_metrics['regret'       ].append(logs['regret_log']  [-1])\n",
    "        per_trial_metrics['mse'          ].append(logs['mse_log']     [-1])\n",
    "        per_trial_metrics['fairness'     ].append(logs['fairness_log'][-1])\n",
    "        per_trial_metrics['training_time'].append(logs['training_time'])\n",
    "\n",
    "        # ---- per-group metrics (final epoch) -----------------------\n",
    "        for g_id, g_log in logs['per_group_mse'].items():\n",
    "            if g_log:                      # just in case\n",
    "                per_trial_metrics[f'G{int(g_id)}_mse'          ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_decision_objective'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_decision_obj' ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_true_benefit'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_true_benefit' ].append(g_log[-1])\n",
    "\n",
    "    # -------------------- aggregate over trials ----------------------\n",
    "    avg_results = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      AVERAGED RESULTS ACROSS ALL TRIALS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for key, values in per_trial_metrics.items():\n",
    "        μ, σ = np.mean(values), np.std(values)\n",
    "        avg_results[key]      = μ\n",
    "        avg_results[f'{key}_std'] = σ\n",
    "        print(f\"[{key.upper():>20s}]  μ = {μ:.4f} | σ = {σ:.4f}\")\n",
    "\n",
    "    return avg_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175a7d",
   "metadata": {},
   "source": [
    "# Verify finite-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68991548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1364 | Test-MSE 367.6678 | Regret 0.0577 | Fair-Val 86.3542\n",
      "Epoch 010/50 | Train-Loss 16.7930 | Test-MSE 351.5048 | Regret 0.0401 | Fair-Val 77.5069\n",
      "Epoch 020/50 | Train-Loss 11.3633 | Test-MSE 347.9750 | Regret 0.0302 | Fair-Val 78.1117\n",
      "Epoch 030/50 | Train-Loss 5.8068 | Test-MSE 344.7059 | Regret 0.0186 | Fair-Val 80.3949\n",
      "Epoch 040/50 | Train-Loss 2.5176 | Test-MSE 340.3410 | Regret 0.0112 | Fair-Val 81.9888\n",
      "Epoch 050/50 | Train-Loss 1.9036 | Test-MSE 335.2207 | Regret 0.0091 | Fair-Val 81.4599\n",
      "Training finished in 87.86s.\n",
      "Epoch 001/50 | Train-Loss 29.5292 | Test-MSE 368.0066 | Regret 0.0596 | Fair-Val 86.0443\n",
      "Epoch 010/50 | Train-Loss 17.3969 | Test-MSE 351.2521 | Regret 0.0426 | Fair-Val 77.0010\n",
      "Epoch 020/50 | Train-Loss 11.8313 | Test-MSE 347.6906 | Regret 0.0318 | Fair-Val 77.5101\n",
      "Epoch 030/50 | Train-Loss 6.5466 | Test-MSE 344.7417 | Regret 0.0202 | Fair-Val 79.8850\n",
      "Epoch 040/50 | Train-Loss 2.7086 | Test-MSE 341.7088 | Regret 0.0113 | Fair-Val 81.7395\n",
      "Epoch 050/50 | Train-Loss 1.9225 | Test-MSE 337.5176 | Regret 0.0090 | Fair-Val 81.6133\n",
      "Training finished in 85.73s.\n",
      "Epoch 001/50 | Train-Loss 28.7174 | Test-MSE 364.8509 | Regret 0.0557 | Fair-Val 85.5287\n",
      "Epoch 010/50 | Train-Loss 16.2374 | Test-MSE 348.0616 | Regret 0.0392 | Fair-Val 76.2679\n",
      "Epoch 020/50 | Train-Loss 10.1667 | Test-MSE 345.6376 | Regret 0.0276 | Fair-Val 77.7010\n",
      "Epoch 030/50 | Train-Loss 4.8059 | Test-MSE 342.3919 | Regret 0.0168 | Fair-Val 79.3444\n",
      "Epoch 040/50 | Train-Loss 2.3638 | Test-MSE 338.8708 | Regret 0.0101 | Fair-Val 81.3228\n",
      "Epoch 050/50 | Train-Loss 1.8097 | Test-MSE 332.7253 | Regret 0.0086 | Fair-Val 80.6949\n",
      "Training finished in 85.84s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0089 | σ = 0.0002\n",
      "[                 MSE]  μ = 335.1545 | σ = 1.9570\n",
      "[            FAIRNESS]  μ = 81.2561 | σ = 0.4017\n",
      "[       TRAINING_TIME]  μ = 86.4732 | σ = 0.9792\n",
      "[              G0_MSE]  μ = 315.7831 | σ = 1.8671\n",
      "[              G1_MSE]  μ = 478.2952 | σ = 2.6342\n",
      "[     G0_DECISION_OBJ]  μ = 45.5951 | σ = 0.3118\n",
      "[     G1_DECISION_OBJ]  μ = 262.3946 | σ = 2.0855\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0791 | Test-MSE 367.6869 | Regret 0.0011 | Fair-Val 86.3649\n",
      "Epoch 010/50 | Train-Loss 0.0432 | Test-MSE 353.3119 | Regret 0.0007 | Fair-Val 78.1437\n",
      "Epoch 020/50 | Train-Loss 0.0303 | Test-MSE 347.2993 | Regret 0.0005 | Fair-Val 75.7751\n",
      "Epoch 030/50 | Train-Loss 0.0192 | Test-MSE 343.6342 | Regret 0.0004 | Fair-Val 77.2162\n",
      "Epoch 040/50 | Train-Loss 0.0114 | Test-MSE 338.2744 | Regret 0.0003 | Fair-Val 78.7383\n",
      "Epoch 050/50 | Train-Loss 0.0078 | Test-MSE 332.0155 | Regret 0.0002 | Fair-Val 78.6607\n",
      "Training finished in 145.71s.\n",
      "Epoch 001/50 | Train-Loss 0.0811 | Test-MSE 369.5245 | Regret 0.0011 | Fair-Val 86.2682\n",
      "Epoch 010/50 | Train-Loss 0.0462 | Test-MSE 354.4642 | Regret 0.0008 | Fair-Val 78.2360\n",
      "Epoch 020/50 | Train-Loss 0.0312 | Test-MSE 346.2079 | Regret 0.0006 | Fair-Val 75.3841\n",
      "Epoch 030/50 | Train-Loss 0.0206 | Test-MSE 342.5598 | Regret 0.0004 | Fair-Val 76.9090\n",
      "Epoch 040/50 | Train-Loss 0.0114 | Test-MSE 346.0287 | Regret 0.0003 | Fair-Val 79.8772\n",
      "Epoch 050/50 | Train-Loss 0.0075 | Test-MSE 347.7295 | Regret 0.0002 | Fair-Val 81.4321\n",
      "Training finished in 146.17s.\n",
      "Epoch 001/50 | Train-Loss 0.0787 | Test-MSE 364.1833 | Regret 0.0011 | Fair-Val 85.5338\n",
      "Epoch 010/50 | Train-Loss 0.0442 | Test-MSE 341.3142 | Regret 0.0007 | Fair-Val 74.9750\n",
      "Epoch 020/50 | Train-Loss 0.0303 | Test-MSE 335.9294 | Regret 0.0006 | Fair-Val 73.6206\n",
      "Epoch 030/50 | Train-Loss 0.0203 | Test-MSE 327.6926 | Regret 0.0004 | Fair-Val 73.3860\n",
      "Epoch 040/50 | Train-Loss 0.0127 | Test-MSE 318.9271 | Regret 0.0003 | Fair-Val 73.1309\n",
      "Epoch 050/50 | Train-Loss 0.0082 | Test-MSE 313.6862 | Regret 0.0002 | Fair-Val 74.5665\n",
      "Training finished in 147.21s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0002 | σ = 0.0000\n",
      "[                 MSE]  μ = 331.1437 | σ = 13.9118\n",
      "[            FAIRNESS]  μ = 78.2197 | σ = 2.8202\n",
      "[       TRAINING_TIME]  μ = 146.3640 | σ = 0.6275\n",
      "[              G0_MSE]  μ = 312.4961 | σ = 13.2410\n",
      "[              G1_MSE]  μ = 468.9356 | σ = 18.8717\n",
      "[     G0_DECISION_OBJ]  μ = 21.0391 | σ = 0.0794\n",
      "[     G1_DECISION_OBJ]  μ = 171.5516 | σ = 0.4030\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.5959 | Test-MSE 367.6061 | Regret 0.0235 | Fair-Val 86.3237\n",
      "Epoch 010/50 | Train-Loss 1.3206 | Test-MSE 349.3848 | Regret 0.0147 | Fair-Val 76.1751\n",
      "Epoch 020/50 | Train-Loss 0.8445 | Test-MSE 344.3133 | Regret 0.0104 | Fair-Val 74.7084\n",
      "Epoch 030/50 | Train-Loss 0.4673 | Test-MSE 345.7541 | Regret 0.0065 | Fair-Val 78.9343\n",
      "Epoch 040/50 | Train-Loss 0.2756 | Test-MSE 342.3395 | Regret 0.0045 | Fair-Val 80.0937\n",
      "Epoch 050/50 | Train-Loss 0.2274 | Test-MSE 341.2335 | Regret 0.0038 | Fair-Val 80.6327\n",
      "Training finished in 146.75s.\n",
      "Epoch 001/50 | Train-Loss 2.7243 | Test-MSE 368.1580 | Regret 0.0244 | Fair-Val 85.9826\n",
      "Epoch 010/50 | Train-Loss 1.4082 | Test-MSE 349.1245 | Regret 0.0157 | Fair-Val 75.1861\n",
      "Epoch 020/50 | Train-Loss 0.9050 | Test-MSE 341.7624 | Regret 0.0111 | Fair-Val 73.0766\n",
      "Epoch 030/50 | Train-Loss 0.5508 | Test-MSE 336.7126 | Regret 0.0074 | Fair-Val 75.6356\n",
      "Epoch 040/50 | Train-Loss 0.3354 | Test-MSE 334.1299 | Regret 0.0051 | Fair-Val 77.2366\n",
      "Epoch 050/50 | Train-Loss 0.2389 | Test-MSE 333.9861 | Regret 0.0041 | Fair-Val 78.7729\n",
      "Training finished in 148.25s.\n",
      "Epoch 001/50 | Train-Loss 2.5923 | Test-MSE 364.4951 | Regret 0.0228 | Fair-Val 85.4672\n",
      "Epoch 010/50 | Train-Loss 1.2755 | Test-MSE 343.3654 | Regret 0.0144 | Fair-Val 73.7907\n",
      "Epoch 020/50 | Train-Loss 0.7901 | Test-MSE 335.2165 | Regret 0.0098 | Fair-Val 72.4112\n",
      "Epoch 030/50 | Train-Loss 0.4693 | Test-MSE 335.0576 | Regret 0.0065 | Fair-Val 76.3712\n",
      "Epoch 040/50 | Train-Loss 0.2870 | Test-MSE 335.1286 | Regret 0.0047 | Fair-Val 77.9336\n",
      "Epoch 050/50 | Train-Loss 0.2149 | Test-MSE 338.8712 | Regret 0.0039 | Fair-Val 79.8348\n",
      "Training finished in 147.77s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0039 | σ = 0.0001\n",
      "[                 MSE]  μ = 338.0303 | σ = 3.0179\n",
      "[            FAIRNESS]  μ = 79.7468 | σ = 0.7618\n",
      "[       TRAINING_TIME]  μ = 147.5913 | σ = 0.6253\n",
      "[              G0_MSE]  μ = 319.0186 | σ = 2.8376\n",
      "[              G1_MSE]  μ = 478.5122 | σ = 4.3536\n",
      "[     G0_DECISION_OBJ]  μ = 16.7932 | σ = 0.0086\n",
      "[     G1_DECISION_OBJ]  μ = 55.1012 | σ = 0.1032\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 367.6161 | Regret 0.2828 | Fair-Val 86.3052\n",
      "Epoch 010/50 | Train-Loss 36.1058 | Test-MSE 348.5959 | Regret 0.1768 | Fair-Val 75.7763\n",
      "Epoch 020/50 | Train-Loss 22.8421 | Test-MSE 342.8874 | Regret 0.1330 | Fair-Val 73.9689\n",
      "Epoch 030/50 | Train-Loss 13.3280 | Test-MSE 344.2250 | Regret 0.0888 | Fair-Val 77.9444\n",
      "Epoch 040/50 | Train-Loss 8.1164 | Test-MSE 343.2566 | Regret 0.0647 | Fair-Val 79.3913\n",
      "Epoch 050/50 | Train-Loss 6.3966 | Test-MSE 341.4930 | Regret 0.0534 | Fair-Val 79.5419\n",
      "Training finished in 102.52s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 367.9062 | Regret 0.2960 | Fair-Val 85.9982\n",
      "Epoch 010/50 | Train-Loss 38.3728 | Test-MSE 349.3184 | Regret 0.1872 | Fair-Val 75.1241\n",
      "Epoch 020/50 | Train-Loss 24.3640 | Test-MSE 342.3683 | Regret 0.1406 | Fair-Val 72.8827\n",
      "Epoch 030/50 | Train-Loss 14.9310 | Test-MSE 343.5956 | Regret 0.0960 | Fair-Val 76.9404\n",
      "Epoch 040/50 | Train-Loss 9.5231 | Test-MSE 343.5769 | Regret 0.0711 | Fair-Val 78.8945\n",
      "Epoch 050/50 | Train-Loss 6.7588 | Test-MSE 341.6943 | Regret 0.0567 | Fair-Val 79.4619\n",
      "Training finished in 102.23s.\n",
      "Epoch 001/50 | Train-Loss 73.9791 | Test-MSE 364.7512 | Regret 0.2730 | Fair-Val 85.4776\n",
      "Epoch 010/50 | Train-Loss 33.9885 | Test-MSE 344.2364 | Regret 0.1710 | Fair-Val 73.8491\n",
      "Epoch 020/50 | Train-Loss 20.5899 | Test-MSE 338.3221 | Regret 0.1224 | Fair-Val 73.1286\n",
      "Epoch 030/50 | Train-Loss 12.2485 | Test-MSE 338.5063 | Regret 0.0862 | Fair-Val 76.6990\n",
      "Epoch 040/50 | Train-Loss 7.8883 | Test-MSE 337.1924 | Regret 0.0627 | Fair-Val 78.0978\n",
      "Epoch 050/50 | Train-Loss 5.7595 | Test-MSE 336.2044 | Regret 0.0524 | Fair-Val 78.2652\n",
      "Training finished in 102.64s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0542 | σ = 0.0018\n",
      "[                 MSE]  μ = 339.7973 | σ = 2.5419\n",
      "[            FAIRNESS]  μ = 79.0897 | σ = 0.5839\n",
      "[       TRAINING_TIME]  μ = 102.4635 | σ = 0.1744\n",
      "[              G0_MSE]  μ = 320.9423 | σ = 2.4032\n",
      "[              G1_MSE]  μ = 479.1216 | σ = 3.5676\n",
      "[     G0_DECISION_OBJ]  μ = 16.5940 | σ = 0.0451\n",
      "[     G1_DECISION_OBJ]  μ = 21.5311 | σ = 0.1256\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 130.9927 | Test-MSE 367.7638 | Regret 0.0581 | Fair-Val 86.3262\n",
      "Epoch 010/50 | Train-Loss 95.9175 | Test-MSE 339.4573 | Regret 0.0490 | Fair-Val 70.4997\n",
      "Epoch 020/50 | Train-Loss 55.4483 | Test-MSE 295.7133 | Regret 0.0561 | Fair-Val 46.7734\n",
      "Epoch 030/50 | Train-Loss 28.4425 | Test-MSE 251.4445 | Regret 0.0567 | Fair-Val 29.9030\n",
      "Epoch 040/50 | Train-Loss 18.1549 | Test-MSE 234.8401 | Regret 0.0456 | Fair-Val 29.3916\n",
      "Epoch 050/50 | Train-Loss 16.6254 | Test-MSE 223.2457 | Regret 0.0397 | Fair-Val 29.6944\n",
      "Training finished in 85.18s.\n",
      "Epoch 001/50 | Train-Loss 131.3402 | Test-MSE 368.0721 | Regret 0.0598 | Fair-Val 85.9823\n",
      "Epoch 010/50 | Train-Loss 94.1046 | Test-MSE 338.4949 | Regret 0.0517 | Fair-Val 68.9302\n",
      "Epoch 020/50 | Train-Loss 53.4954 | Test-MSE 293.6122 | Regret 0.0590 | Fair-Val 45.2340\n",
      "Epoch 030/50 | Train-Loss 26.0933 | Test-MSE 252.3548 | Regret 0.0568 | Fair-Val 29.8323\n",
      "Epoch 040/50 | Train-Loss 18.5109 | Test-MSE 238.7287 | Regret 0.0451 | Fair-Val 29.8383\n",
      "Epoch 050/50 | Train-Loss 15.8025 | Test-MSE 228.2982 | Regret 0.0391 | Fair-Val 29.8342\n",
      "Training finished in 85.56s.\n",
      "Epoch 001/50 | Train-Loss 131.1555 | Test-MSE 364.8749 | Regret 0.0559 | Fair-Val 85.4323\n",
      "Epoch 010/50 | Train-Loss 91.0953 | Test-MSE 331.3417 | Regret 0.0511 | Fair-Val 66.5308\n",
      "Epoch 020/50 | Train-Loss 46.3087 | Test-MSE 284.9090 | Regret 0.0598 | Fair-Val 42.1641\n",
      "Epoch 030/50 | Train-Loss 29.0693 | Test-MSE 247.7563 | Regret 0.0558 | Fair-Val 29.0300\n",
      "Epoch 040/50 | Train-Loss 21.8561 | Test-MSE 238.1235 | Regret 0.0445 | Fair-Val 31.1155\n",
      "Epoch 050/50 | Train-Loss 17.9767 | Test-MSE 223.8148 | Regret 0.0405 | Fair-Val 28.9733\n",
      "Training finished in 85.37s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0398 | σ = 0.0006\n",
      "[                 MSE]  μ = 225.1196 | σ = 2.2596\n",
      "[            FAIRNESS]  μ = 29.5007 | σ = 0.3772\n",
      "[       TRAINING_TIME]  μ = 85.3685 | σ = 0.1562\n",
      "[              G0_MSE]  μ = 218.0866 | σ = 2.2122\n",
      "[              G1_MSE]  μ = 277.0879 | σ = 2.6784\n",
      "[     G0_DECISION_OBJ]  μ = 34.2996 | σ = 0.2923\n",
      "[     G1_DECISION_OBJ]  μ = 251.9204 | σ = 3.2178\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 101.9354 | Test-MSE 368.1804 | Regret 0.0011 | Fair-Val 86.3483\n",
      "Epoch 010/50 | Train-Loss 72.7712 | Test-MSE 342.9976 | Regret 0.0011 | Fair-Val 70.4812\n",
      "Epoch 020/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 030/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 040/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 050/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Training finished in 135.99s.\n",
      "Epoch 001/50 | Train-Loss 101.8921 | Test-MSE 368.6722 | Regret 0.0012 | Fair-Val 86.0446\n",
      "Epoch 010/50 | Train-Loss 71.3687 | Test-MSE 342.4066 | Regret 0.0011 | Fair-Val 69.1542\n",
      "Epoch 020/50 | Train-Loss 21.1789 | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 030/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 040/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 050/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Training finished in 135.58s.\n",
      "Epoch 001/50 | Train-Loss 102.5167 | Test-MSE 365.6272 | Regret 0.0011 | Fair-Val 85.5394\n",
      "Epoch 010/50 | Train-Loss 69.5379 | Test-MSE 335.9843 | Regret 0.0010 | Fair-Val 66.6348\n",
      "Epoch 020/50 | Train-Loss 15.2802 | Test-MSE 288.3159 | Regret 0.0016 | Fair-Val 41.1472\n",
      "Epoch 030/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 040/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 050/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Training finished in 137.41s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = nan | σ = nan\n",
      "[                 MSE]  μ = nan | σ = nan\n",
      "[            FAIRNESS]  μ = nan | σ = nan\n",
      "[       TRAINING_TIME]  μ = 136.3242 | σ = 0.7857\n",
      "[              G0_MSE]  μ = nan | σ = nan\n",
      "[              G1_MSE]  μ = nan | σ = nan\n",
      "[     G0_DECISION_OBJ]  μ = nan | σ = nan\n",
      "[     G1_DECISION_OBJ]  μ = nan | σ = nan\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 104.4521 | Test-MSE 367.9886 | Regret 0.0245 | Fair-Val 86.3246\n",
      "Epoch 010/50 | Train-Loss 74.4892 | Test-MSE 340.7816 | Regret 0.0201 | Fair-Val 70.1572\n",
      "Epoch 020/50 | Train-Loss 25.3406 | Test-MSE 295.6659 | Regret 0.0296 | Fair-Val 45.1725\n",
      "Epoch 030/50 | Train-Loss 10.2696 | Test-MSE 264.4565 | Regret 0.0342 | Fair-Val 34.0614\n",
      "Epoch 040/50 | Train-Loss 4.1579 | Test-MSE 253.9679 | Regret 0.0270 | Fair-Val 33.3138\n",
      "Epoch 050/50 | Train-Loss 3.1547 | Test-MSE 248.9847 | Regret 0.0259 | Fair-Val 32.6463\n",
      "Training finished in 147.20s.\n",
      "Epoch 001/50 | Train-Loss 104.5352 | Test-MSE 368.4889 | Regret 0.0254 | Fair-Val 86.0128\n",
      "Epoch 010/50 | Train-Loss 73.0023 | Test-MSE 340.2529 | Regret 0.0210 | Fair-Val 68.7618\n",
      "Epoch 020/50 | Train-Loss 24.1307 | Test-MSE 293.6083 | Regret 0.0299 | Fair-Val 43.8836\n",
      "Epoch 030/50 | Train-Loss 8.8699 | Test-MSE 262.8234 | Regret 0.0341 | Fair-Val 33.5538\n",
      "Epoch 040/50 | Train-Loss 4.0735 | Test-MSE 254.6048 | Regret 0.0267 | Fair-Val 32.4458\n",
      "Epoch 050/50 | Train-Loss 3.6381 | Test-MSE 247.4165 | Regret 0.0261 | Fair-Val 31.4658\n",
      "Training finished in 147.66s.\n",
      "Epoch 001/50 | Train-Loss 105.0304 | Test-MSE 365.3502 | Regret 0.0238 | Fair-Val 85.5008\n",
      "Epoch 010/50 | Train-Loss 70.8740 | Test-MSE 333.4826 | Regret 0.0194 | Fair-Val 66.2291\n",
      "Epoch 020/50 | Train-Loss 17.7450 | Test-MSE 284.2967 | Regret 0.0279 | Fair-Val 40.7777\n",
      "Epoch 030/50 | Train-Loss 10.5800 | Test-MSE 255.3263 | Regret 0.0309 | Fair-Val 32.6542\n",
      "Epoch 040/50 | Train-Loss 5.4263 | Test-MSE 247.4329 | Regret 0.0267 | Fair-Val 31.7124\n",
      "Epoch 050/50 | Train-Loss 3.7388 | Test-MSE 241.8746 | Regret 0.0268 | Fair-Val 31.3072\n",
      "Training finished in 147.71s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0263 | σ = 0.0004\n",
      "[                 MSE]  μ = 246.0920 | σ = 3.0500\n",
      "[            FAIRNESS]  μ = 31.8064 | σ = 0.5974\n",
      "[       TRAINING_TIME]  μ = 147.5253 | σ = 0.2276\n",
      "[              G0_MSE]  μ = 238.5093 | σ = 2.9452\n",
      "[              G1_MSE]  μ = 302.1222 | σ = 3.8996\n",
      "[     G0_DECISION_OBJ]  μ = 18.1330 | σ = 0.0452\n",
      "[     G1_DECISION_OBJ]  μ = 60.5319 | σ = 0.4230\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 176.4621 | Test-MSE 367.6067 | Regret 0.2830 | Fair-Val 86.2741\n",
      "Epoch 010/50 | Train-Loss 117.9542 | Test-MSE 340.7083 | Regret 0.1890 | Fair-Val 70.8993\n",
      "Epoch 020/50 | Train-Loss 76.5605 | Test-MSE 303.8043 | Regret 0.1831 | Fair-Val 49.5490\n",
      "Epoch 030/50 | Train-Loss 40.4634 | Test-MSE 266.6754 | Regret 0.1770 | Fair-Val 32.1408\n",
      "Epoch 040/50 | Train-Loss 25.7621 | Test-MSE 244.4801 | Regret 0.1505 | Fair-Val 26.0794\n",
      "Epoch 050/50 | Train-Loss 21.0838 | Test-MSE 235.9679 | Regret 0.1264 | Fair-Val 26.3700\n",
      "Training finished in 103.30s.\n",
      "Epoch 001/50 | Train-Loss 180.8339 | Test-MSE 367.9033 | Regret 0.2959 | Fair-Val 85.9693\n",
      "Epoch 010/50 | Train-Loss 117.9508 | Test-MSE 340.2245 | Regret 0.2018 | Fair-Val 69.4319\n",
      "Epoch 020/50 | Train-Loss 75.2802 | Test-MSE 303.1755 | Regret 0.1955 | Fair-Val 47.6347\n",
      "Epoch 030/50 | Train-Loss 42.6764 | Test-MSE 266.2580 | Regret 0.1931 | Fair-Val 30.7018\n",
      "Epoch 040/50 | Train-Loss 28.5131 | Test-MSE 246.1366 | Regret 0.1701 | Fair-Val 25.1856\n",
      "Epoch 050/50 | Train-Loss 21.9967 | Test-MSE 239.5001 | Regret 0.1334 | Fair-Val 26.4092\n",
      "Training finished in 103.95s.\n",
      "Epoch 001/50 | Train-Loss 176.4171 | Test-MSE 364.7444 | Regret 0.2732 | Fair-Val 85.4702\n",
      "Epoch 010/50 | Train-Loss 112.0311 | Test-MSE 334.6272 | Regret 0.1892 | Fair-Val 67.3530\n",
      "Epoch 020/50 | Train-Loss 67.3037 | Test-MSE 297.0110 | Regret 0.1816 | Fair-Val 45.8937\n",
      "Epoch 030/50 | Train-Loss 35.1140 | Test-MSE 261.8514 | Regret 0.1735 | Fair-Val 29.7883\n",
      "Epoch 040/50 | Train-Loss 23.1272 | Test-MSE 249.5023 | Regret 0.1416 | Fair-Val 27.9871\n",
      "Epoch 050/50 | Train-Loss 18.4888 | Test-MSE 241.3743 | Regret 0.1218 | Fair-Val 28.4638\n",
      "Training finished in 103.59s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.1272 | σ = 0.0048\n",
      "[                 MSE]  μ = 238.9474 | σ = 2.2415\n",
      "[            FAIRNESS]  μ = 27.0810 | σ = 0.9779\n",
      "[       TRAINING_TIME]  μ = 103.6114 | σ = 0.2631\n",
      "[              G0_MSE]  μ = 232.4913 | σ = 2.0658\n",
      "[              G1_MSE]  μ = 286.6534 | σ = 3.7396\n",
      "[     G0_DECISION_OBJ]  μ = 17.4656 | σ = 0.0270\n",
      "[     G1_DECISION_OBJ]  μ = 22.0508 | σ = 1.4834\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1420 | Test-MSE 367.6678 | Regret 0.0577 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 16.7765 | Test-MSE 351.9297 | Regret 0.0400 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 11.3554 | Test-MSE 348.4236 | Regret 0.0301 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 5.8094 | Test-MSE 343.9332 | Regret 0.0184 | Fair-Val 0.0049\n",
      "Epoch 040/50 | Train-Loss 2.4959 | Test-MSE 339.1479 | Regret 0.0111 | Fair-Val 0.0052\n",
      "Epoch 050/50 | Train-Loss 1.8876 | Test-MSE 333.6214 | Regret 0.0090 | Fair-Val 0.0053\n",
      "Training finished in 85.47s.\n",
      "Epoch 001/50 | Train-Loss 29.5348 | Test-MSE 368.0066 | Regret 0.0596 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 17.4128 | Test-MSE 351.2632 | Regret 0.0426 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 11.8794 | Test-MSE 347.2107 | Regret 0.0319 | Fair-Val 0.0045\n",
      "Epoch 030/50 | Train-Loss 6.5969 | Test-MSE 344.3484 | Regret 0.0204 | Fair-Val 0.0048\n",
      "Epoch 040/50 | Train-Loss 2.7389 | Test-MSE 342.4343 | Regret 0.0113 | Fair-Val 0.0051\n",
      "Epoch 050/50 | Train-Loss 1.9359 | Test-MSE 338.2726 | Regret 0.0090 | Fair-Val 0.0052\n",
      "Training finished in 84.99s.\n",
      "Epoch 001/50 | Train-Loss 28.7232 | Test-MSE 364.8507 | Regret 0.0557 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 16.2505 | Test-MSE 348.4914 | Regret 0.0392 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 10.1611 | Test-MSE 346.9634 | Regret 0.0276 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 4.8257 | Test-MSE 341.7339 | Regret 0.0167 | Fair-Val 0.0049\n",
      "Epoch 040/50 | Train-Loss 2.3475 | Test-MSE 337.0731 | Regret 0.0100 | Fair-Val 0.0052\n",
      "Epoch 050/50 | Train-Loss 1.8111 | Test-MSE 330.6800 | Regret 0.0085 | Fair-Val 0.0053\n",
      "Training finished in 85.24s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0089 | σ = 0.0002\n",
      "[                 MSE]  μ = 334.1913 | σ = 3.1258\n",
      "[            FAIRNESS]  μ = 0.0053 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 85.2316 | σ = 0.1955\n",
      "[              G0_MSE]  μ = 314.8569 | σ = 2.9844\n",
      "[              G1_MSE]  μ = 477.0584 | σ = 4.1807\n",
      "[     G0_DECISION_OBJ]  μ = 45.5764 | σ = 0.1954\n",
      "[     G1_DECISION_OBJ]  μ = 262.5516 | σ = 2.5022\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0847 | Test-MSE 367.6982 | Regret 0.0011 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.0512 | Test-MSE 345.2933 | Regret 0.0008 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 0.0383 | Test-MSE 336.2067 | Regret 0.0006 | Fair-Val 0.0041\n",
      "Epoch 030/50 | Train-Loss 0.0278 | Test-MSE 343.0694 | Regret 0.0004 | Fair-Val 0.0042\n",
      "Epoch 040/50 | Train-Loss 0.0190 | Test-MSE 344.5851 | Regret 0.0003 | Fair-Val 0.0044\n",
      "Epoch 050/50 | Train-Loss 0.0136 | Test-MSE 337.4286 | Regret 0.0002 | Fair-Val 0.0045\n",
      "Training finished in 147.12s.\n",
      "Epoch 001/50 | Train-Loss 0.0867 | Test-MSE 369.5273 | Regret 0.0011 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 0.0532 | Test-MSE 355.7396 | Regret 0.0008 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.0373 | Test-MSE 356.9369 | Regret 0.0006 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 0.0251 | Test-MSE 356.7030 | Regret 0.0004 | Fair-Val 0.0044\n",
      "Epoch 040/50 | Train-Loss 0.0162 | Test-MSE 357.1693 | Regret 0.0003 | Fair-Val 0.0046\n",
      "Epoch 050/50 | Train-Loss 0.0123 | Test-MSE 354.1853 | Regret 0.0002 | Fair-Val 0.0047\n",
      "Training finished in 145.30s.\n",
      "Epoch 001/50 | Train-Loss 0.0844 | Test-MSE 364.1977 | Regret 0.0011 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.0509 | Test-MSE 336.0923 | Regret 0.0008 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.0364 | Test-MSE 334.4648 | Regret 0.0006 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 0.0251 | Test-MSE 335.7621 | Regret 0.0004 | Fair-Val 0.0042\n",
      "Epoch 040/50 | Train-Loss 0.0169 | Test-MSE 338.0979 | Regret 0.0003 | Fair-Val 0.0044\n",
      "Epoch 050/50 | Train-Loss 0.0128 | Test-MSE 340.6045 | Regret 0.0002 | Fair-Val 0.0044\n",
      "Training finished in 147.09s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0002 | σ = 0.0000\n",
      "[                 MSE]  μ = 344.0728 | σ = 7.2672\n",
      "[            FAIRNESS]  μ = 0.0045 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 146.5035 | σ = 0.8522\n",
      "[              G0_MSE]  μ = 325.7340 | σ = 6.6443\n",
      "[              G1_MSE]  μ = 479.5830 | σ = 11.8895\n",
      "[     G0_DECISION_OBJ]  μ = 20.9290 | σ = 0.0867\n",
      "[     G1_DECISION_OBJ]  μ = 171.1465 | σ = 0.4660\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.6015 | Test-MSE 367.6061 | Regret 0.0235 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 1.3313 | Test-MSE 348.8250 | Regret 0.0147 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.8595 | Test-MSE 341.8218 | Regret 0.0105 | Fair-Val 0.0043\n",
      "Epoch 030/50 | Train-Loss 0.4891 | Test-MSE 341.0576 | Regret 0.0067 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 0.2864 | Test-MSE 337.8073 | Regret 0.0046 | Fair-Val 0.0049\n",
      "Epoch 050/50 | Train-Loss 0.2365 | Test-MSE 336.8123 | Regret 0.0039 | Fair-Val 0.0050\n",
      "Training finished in 147.46s.\n",
      "Epoch 001/50 | Train-Loss 2.7299 | Test-MSE 368.1580 | Regret 0.0244 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 1.4126 | Test-MSE 348.9287 | Regret 0.0157 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 0.9040 | Test-MSE 340.2602 | Regret 0.0111 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 0.5489 | Test-MSE 338.6064 | Regret 0.0073 | Fair-Val 0.0046\n",
      "Epoch 040/50 | Train-Loss 0.3380 | Test-MSE 340.9167 | Regret 0.0051 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 0.2476 | Test-MSE 341.8866 | Regret 0.0041 | Fair-Val 0.0050\n",
      "Training finished in 147.34s.\n",
      "Epoch 001/50 | Train-Loss 2.5981 | Test-MSE 364.4951 | Regret 0.0228 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 1.2873 | Test-MSE 340.8616 | Regret 0.0145 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 0.7982 | Test-MSE 330.8476 | Regret 0.0099 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 0.4828 | Test-MSE 330.2296 | Regret 0.0067 | Fair-Val 0.0046\n",
      "Epoch 040/50 | Train-Loss 0.2959 | Test-MSE 331.0558 | Regret 0.0048 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 0.2182 | Test-MSE 330.8055 | Regret 0.0040 | Fair-Val 0.0050\n",
      "Training finished in 147.76s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0040 | σ = 0.0001\n",
      "[                 MSE]  μ = 336.5015 | σ = 4.5291\n",
      "[            FAIRNESS]  μ = 0.0050 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 147.5175 | σ = 0.1760\n",
      "[              G0_MSE]  μ = 317.5907 | σ = 4.2756\n",
      "[              G1_MSE]  μ = 476.2385 | σ = 6.4322\n",
      "[     G0_DECISION_OBJ]  μ = 16.8233 | σ = 0.0093\n",
      "[     G1_DECISION_OBJ]  μ = 55.1798 | σ = 0.1042\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6115 | Test-MSE 367.6161 | Regret 0.2828 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 36.1008 | Test-MSE 349.4561 | Regret 0.1767 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 22.7728 | Test-MSE 343.4800 | Regret 0.1330 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 13.2802 | Test-MSE 345.7516 | Regret 0.0876 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 8.0597 | Test-MSE 344.2115 | Regret 0.0641 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 6.4612 | Test-MSE 342.6795 | Regret 0.0527 | Fair-Val 0.0049\n",
      "Training finished in 102.28s.\n",
      "Epoch 001/50 | Train-Loss 79.0285 | Test-MSE 367.9062 | Regret 0.2960 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 38.1091 | Test-MSE 350.9059 | Regret 0.1852 | Fair-Val 0.0042\n",
      "Epoch 010/50 | Train-Loss 2044.1875 | Test-MSE 353.2028 | Regret 0.0796 | Fair-Val 485.3503\n",
      "Epoch 020/50 | Train-Loss 1394.0293 | Test-MSE 350.2819 | Regret 0.0597 | Fair-Val 480.0713\n",
      "Epoch 030/50 | Train-Loss 689.7168 | Test-MSE 346.1506 | Regret 0.0344 | Fair-Val 472.1028\n",
      "Epoch 040/50 | Train-Loss 289.1016 | Test-MSE 340.3505 | Regret 0.0196 | Fair-Val 461.7527\n",
      "Epoch 050/50 | Train-Loss 236.9004 | Test-MSE 332.2181 | Regret 0.0169 | Fair-Val 450.4261\n",
      "Training finished in 28.29s.\n",
      "Epoch 001/50 | Train-Loss 3512.8027 | Test-MSE 368.0974 | Regret 0.1189 | Fair-Val 505.9565\n",
      "Epoch 010/50 | Train-Loss 2133.3867 | Test-MSE 350.4653 | Regret 0.0841 | Fair-Val 481.9890\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 0.8, 1.5, 2]\n",
    "fairness_lambdas = [0, 1.0]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form','finite-diff'] # New parameter\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  GRID-SEARCH HARNESS  (only the inner loop changed)\n",
    "# ---------------------------------------------------------------------\n",
    "results_list = []\n",
    "\n",
    "for group in group_settings:\n",
    "    fairness_types = ['mad', 'atkinson']           # keep acc_parity only if λ>0\n",
    "    for grad_method in grad_methods:\n",
    "        for lam in fairness_lambdas:\n",
    "            for fairness in fairness_types:\n",
    "                if lam == 0 and fairness != fairness_types[0]:\n",
    "                    continue                       # skip unattainable combos\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method,\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = dict(\n",
    "                        X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                        cost_train=cost_train, gainF_train=gainF_train,\n",
    "                        X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                        cost_test=cost_test, gainF_test=gainF_test,\n",
    "                        model_class=FairRiskPredictor,\n",
    "                        input_dim=feats_train.shape[1],\n",
    "                        alpha=alpha, Q=Q,\n",
    "                        lambda_fair=lam, fairness_type=fairness,\n",
    "                        group=group, grad_method=grad_method,\n",
    "                        num_epochs=50, lr=0.003\n",
    "                    )\n",
    "\n",
    "                    avg_results = train_many_trials_regret(\n",
    "                        n_trials=3, **train_args)\n",
    "\n",
    "                    # ---------------- build DataFrame row ------------\n",
    "                    row = run_params.copy()\n",
    "                    row.update(avg_results)          # every metric goes in\n",
    "                    results_list.append(row)\n",
    "\n",
    "# ---------------- DataFrame & LaTeX dump -----------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Put the hyper-parameters first; everything else follows automatically\n",
    "hp_cols = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness']\n",
    "other_cols = sorted([c for c in results_df.columns if c not in hp_cols])\n",
    "results_df = results_df[hp_cols + other_cols]\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Averaged Experimental Results Across Different Parameters.\",\n",
    "    label=\"tab:avg_exp_results_expanded\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None,\n",
    "                       'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n--- LaTeX Table Output ---\")\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"20250702results2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
