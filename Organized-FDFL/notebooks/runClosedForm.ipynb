{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0469d29",
   "metadata": {},
   "source": [
    "- Change min-risk from 0.001 to 1\n",
    "\n",
    "- Write fold-opt subsection.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9450ea9",
   "metadata": {},
   "source": [
    "5. For group, report group-wise performance (MSE and Decision Solution&Objective)\n",
    "     - Closed-Form Done\n",
    "     - <b>2-Stage Done</b>\n",
    "\n",
    "6. <b>For Fold-OPT Change PGD closed-form to solver.</b>\n",
    "\n",
    "8. Verify Individual and Group Regret Performance Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66fb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.myOptimization import (\n",
    "     AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev, compute_group_accuracy_parity\n",
    "\n",
    "from src.utils.myOptimization import AlphaFairness, AlphaFairnesstorch, solve_coupled_group_grad, compute_gradient_closed_form\n",
    "from src.utils.myOptimization import compute_group_gradient_analytical\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.features import get_all_features\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6818df",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39fa5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Save to json\n",
    "import json\n",
    "params = {\n",
    "    \"n_sample\": 5000 ,\n",
    "    \"alpha\": 2,\n",
    "    \"Q\": 1000,\n",
    "    \"epochs\": 50,\n",
    "    \"lambdas\": 1.0,\n",
    "    \"lr\": 0.01\n",
    "}\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"w\") as f:\n",
    "#     json.dump(params, f, indent=4)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\models\\\\config_CF.json\", \"r\") as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "n_sample = params[\"n_sample\"]\n",
    "alpha    = params[\"alpha\"]\n",
    "Q        = params[\"n_sample\"]//2\n",
    "epochs   = params[\"epochs\"]\n",
    "lambdas  = params[\"lambdas\"]\n",
    "lr       = params[\"lr\"]\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da3d9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "df = pd.read_csv('E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\data\\\\data.csv')\n",
    "\n",
    "df = df.sample(n=n_sample,random_state=42)\n",
    "\n",
    "# Normalized cost to 0.1-10 range\n",
    "cost = np.array(df['cost_t_capped'].values) * 10\n",
    "cost = np.maximum(cost, 0.1)\n",
    "\n",
    "# All features, standardized\n",
    "features = df[get_all_features(df)].values\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# True benefit, predictor label normalzied to 1-100 range\n",
    "benefit = np.array(df['benefit'].values) * 100\n",
    "benefit = np.maximum(benefit, 1) \n",
    "benefit = benefit + 1\n",
    "\n",
    "# Group labels, 0 is White (Majority), 1 is Black\n",
    "race = np.array(df['race'].values)\n",
    "\n",
    "gainF = np.ones_like(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f20f2",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cef7387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd6cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df314909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d661549",
   "metadata": {},
   "source": [
    "## JVP calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37802e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_coupled_group_jvp(b, c, group_idx, Q, alpha, beta, v):\n",
    "#     \"\"\"\n",
    "#     Computes the vector-Jacobian product v @ J for the coupled group-alpha problem\n",
    "#     without explicitly forming the full Jacobian matrix J.\n",
    "#     Complexity: O(n) for each element of the output, avoiding O(n^2).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     b, c, group_idx, v = map(np.asarray, [b, c, group_idx, v])\n",
    "#     n = len(b)\n",
    "#     final_grad = np.zeros(n)\n",
    "\n",
    "#     # --- 1. Forward Pass: Pre-compute terms from the solver ---\n",
    "#     # This part is identical to the start of the original _grad function\n",
    "#     if beta > 1:\n",
    "#         gamma = beta - 2 + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = (2 - alpha) / gamma\n",
    "#     else: # beta < 1\n",
    "#         gamma = beta + alpha - alpha * beta\n",
    "#         psi_s_exp_factor = -alpha / gamma\n",
    "\n",
    "#     d_star = solve_coupled_group_alpha(b, c, group_idx, Q, alpha, beta)\n",
    "#     unique_groups = np.unique(group_idx)\n",
    "#     S, H, Psi = {}, {}, {}\n",
    "#     for k in unique_groups:\n",
    "#         mask = (group_idx == k)\n",
    "#         G_k, b_k, c_k = np.sum(mask), b[mask], c[mask]\n",
    "#         S[k] = np.sum((c_k**(-(1-beta)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         H[k] = np.sum((c_k**((beta-1)/beta)) * (b_k**((1-beta)/beta)))\n",
    "#         const_factor = (beta - 1) if beta > 1 else (1 - beta)\n",
    "#         if beta > 1:\n",
    "#             Psi[k] = (S[k]**psi_s_exp_factor) * (const_factor**((alpha-2)/gamma))\n",
    "#         else:\n",
    "#             Psi[k] = (G_k**((alpha-1)/gamma)) * (S[k]**psi_s_exp_factor) * (const_factor**(alpha/gamma))\n",
    "#     Xi = np.sum([H[k] * Psi[k] for k in unique_groups])\n",
    "#     phi_all = (c**(-1/beta)) * (b**((1-beta)/beta))\n",
    "\n",
    "#     # --- 2. Compute the scalar term `Σᵢ vᵢ * dᵢ*` ---\n",
    "#     v_dot_d_star = np.dot(v, d_star)\n",
    "\n",
    "#     # --- 3. Backward Pass: Loop through each prediction `b_j` to get the j-th grad component ---\n",
    "#     for j in range(n):\n",
    "#         m = group_idx[j] # Group of the variable b_j\n",
    "\n",
    "#         # --- Calculate `∂Ξ/∂bⱼ` (same as before) ---\n",
    "#         dS_m_db_j = ((1-beta)/beta) * (c[j]**(-(1-beta)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dH_m_db_j = ((1-beta)/beta) * (c[j]**((beta-1)/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "#         dPsi_m_db_j = (psi_s_exp_factor / S[m]) * Psi[m] * dS_m_db_j\n",
    "#         dXi_db_j = dH_m_db_j * Psi[m] + H[m] * dPsi_m_db_j\n",
    "\n",
    "#         # --- Calculate the JVP-specific term `Σᵢ vᵢ * (∂Nᵢ/∂bⱼ)` ---\n",
    "#         # ∂Nᵢ/∂bⱼ = Q * ( (∂Ψₖ/∂bⱼ) * φᵢ + Ψₖ * (∂φᵢ/∂bⱼ) )\n",
    "#         # We need to sum vᵢ * (∂Nᵢ/∂bⱼ) over all i\n",
    "#         sum_v_dN_db_j = 0\n",
    "#         dphi_j_db_j = ((1-beta)/beta) * (c[j]**(-1/beta)) * (b[j]**((1-2*beta)/beta))\n",
    "\n",
    "#         # The derivative ∂Ψₖ/∂bⱼ is only non-zero if k == m\n",
    "#         # The derivative ∂φᵢ/∂bⱼ is only non-zero if i == j\n",
    "#         # This makes the sum sparse and efficient to compute\n",
    "#         sum_v_dN_db_j += Q * dPsi_m_db_j * np.dot(v[group_idx == m], phi_all[group_idx == m])\n",
    "#         sum_v_dN_db_j += Q * Psi[m] * v[j] * dphi_j_db_j\n",
    "\n",
    "#         # --- 4. Assemble the final gradient component ---\n",
    "#         final_grad[j] = (1/Xi) * sum_v_dN_db_j - (dXi_db_j / Xi) * v_dot_d_star\n",
    "\n",
    "#     return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ea0565",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a78a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "class optDataset(Dataset):\n",
    "    def __init__(self, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol_group = solve_coupled_group_alpha(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj_group = compute_coupled_group_obj(sol_group, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        sol_ind, _ = solve_closed_form(self.gainF, self.risk, self.cost, alpha=alpha, Q=Q)\n",
    "\n",
    "        obj_ind = AlphaFairness(self.risk*sol_ind,alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol_ind = torch.from_numpy(sol_ind).float()\n",
    "        self.sol_group = torch.from_numpy(sol_group).float()\n",
    "\n",
    "        # to array\n",
    "        obj_group = np.array(obj_group)\n",
    "        self.obj_group = torch.from_numpy(obj_group).float()\n",
    "        self.obj_ind = torch.tensor(obj_ind).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.feats[idx],\n",
    "            self.risk[idx],\n",
    "            self.gainF[idx],\n",
    "            self.cost[idx],\n",
    "            self.race[idx],\n",
    "            self.sol_ind[idx],\n",
    "            self.sol_group[idx],\n",
    "            self.obj_group,\n",
    "            self.obj_ind\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22d84524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Test size: 2500\n",
      "First five feats: tensor([[-1.3127, -0.1998, -0.3537, -0.4862,  1.7943]])\n",
      "risk: tensor([2.])\n",
      "gainF: tensor([1.])\n",
      "cost: tensor([0.1000])\n",
      "race: tensor([0.])\n",
      "sol_ind: tensor([7.5626])\n",
      "sol_group: tensor([7.5626])\n",
      "obj_group: tensor([-218.5607])\n",
      "obj_ind: tensor([-218.5607])\n"
     ]
    }
   ],
   "source": [
    "optmodel_group = solve_coupled_group_alpha\n",
    "optmodel_ind = solve_closed_form\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, b_train, b_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    features, gainF, benefit, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(feats_train, b_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(feats_test, b_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predmodel.to(device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "for batch in dataloader_train:\n",
    "    names = [\n",
    "        \"feats\", \"risk\", \"gainF\", \"cost\", \"race\",\n",
    "        \"sol_ind\", \"sol_group\", \"obj_group\", \"obj_ind\"\n",
    "    ]\n",
    "    for name, item in zip(names, batch):\n",
    "        # Only show first five elements for feats\n",
    "        if name == \"feats\":\n",
    "            print(f\"First five {name}: {item[:1, :5]}\")\n",
    "        else:\n",
    "            print(f\"{name}: {item[:1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847c1c",
   "metadata": {},
   "source": [
    "## Regret Loss nn.Module Gemini Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2886a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_loss_and_decision(pred_r, true_r, gainF, cost, race, Q, alpha, lambdas, fairness_type, group, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to compute loss. Detaches inputs to prevent this logic from being part of the graph,\n",
    "    as its gradient is handled manually in the backward pass.\n",
    "    \"\"\"\n",
    "    # Use detached tensors for calculation\n",
    "    pred_r_d, true_r_d, gainF_d, cost_d, race_d = map(\n",
    "        lambda t: t.detach(), [pred_r, true_r, gainF, cost, race]\n",
    "    )\n",
    "    pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(to_numpy_1d, [pred_r_d, true_r_d, gainF_d, cost_d, race_d])\n",
    "\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        # Ensure regret is not negative due to solver noise\n",
    "        regret_loss = torch.tensor(max(0, obj_val_at_d_star - obj_val_at_d_hat), dtype=pred_r.dtype, device=pred_r.device)\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        return torch.tensor(0.0), torch.tensor(0.0), None\n",
    "\n",
    "    # Use the original tensors (with graph) for fairness calculation for autograd\n",
    "    fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "    if fairness_type != 'none':\n",
    "        mode = 'between' if group else 'individual'\n",
    "        if fairness_type == 'atkinson': fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "        elif fairness_type == 'mad': fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "        elif fairness_type == 'acc_parity' and group: fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "\n",
    "    total_loss = regret_loss + lambdas * fairness_penalty\n",
    "    return total_loss, fairness_penalty, d_hat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ee06d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# (Assuming all previous helper functions like to_numpy_1d, solvers, etc. are defined)\n",
    "\n",
    "def _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group):\n",
    "    \"\"\"Helper to compute regret and the decision variable d_hat.\"\"\"\n",
    "    try:\n",
    "        if group:\n",
    "            d_hat_np = solve_coupled_group_alpha(pred_r_np, cost_np, race_np, Q, alpha)\n",
    "            d_star_np = solve_coupled_group_alpha(true_r_np, cost_np, race_np, Q, alpha)\n",
    "            obj_val_at_d_hat = compute_coupled_group_obj(d_hat_np, true_r_np, race_np, alpha)\n",
    "            obj_val_at_d_star = compute_coupled_group_obj(d_star_np, true_r_np, race_np, alpha)\n",
    "        else:\n",
    "            d_hat_np, _ = solve_closed_form(gainF_np, pred_r_np, cost_np, alpha, Q)\n",
    "            d_star_np, _ = solve_closed_form(gainF_np, true_r_np, cost_np, alpha, Q)\n",
    "            obj_val_at_d_hat = AlphaFairness(true_r_np * d_hat_np, alpha)\n",
    "            obj_val_at_d_star = AlphaFairness(true_r_np * d_star_np, alpha)\n",
    "\n",
    "        regret = obj_val_at_d_star - obj_val_at_d_hat\n",
    "        # regret = np.log1p(np.exp(regret * 10)) / 10\n",
    "        return regret, d_hat_np\n",
    "\n",
    "    except (ValueError, cp.error.SolverError, np.linalg.LinAlgError) as e:\n",
    "        print(f\"Warning: Solver failed: {e}\")\n",
    "        # Return a zero regret and a placeholder for d_hat\n",
    "        return 0.0, np.zeros_like(pred_r_np)\n",
    "\n",
    "class RegretLossFn(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd Function for regret with a closed-form or finite-difference gradient.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, gainF, cost, race, Q, alpha, group, grad_method):\n",
    "        # --- Loss Calculation (Regret) ---\n",
    "        pred_r_np, true_r_np, gainF_np, cost_np, race_np = map(\n",
    "            lambda t: to_numpy_1d(t.detach()), [pred_r, true_r, gainF, cost, race]\n",
    "        )\n",
    "\n",
    "        regret, d_hat_np = _calculate_regret_and_d_hat(pred_r_np, true_r_np, gainF_np, cost_np, race_np, Q, alpha, group)\n",
    "        regret_loss = torch.tensor(regret, dtype=pred_r.dtype, device=pred_r.device)\n",
    "        # regret_loss = F.softplus(torch.tensor(regret, dtype=pred_r.dtype,device=pred_r.device), beta=10)\n",
    "        d_hat = torch.from_numpy(d_hat_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        # --- Save for Backward ---\n",
    "        ctx.save_for_backward(pred_r, true_r, gainF, cost, race, d_hat)\n",
    "        ctx.params = {'Q': Q, 'alpha': alpha, 'group': group, 'grad_method': grad_method}\n",
    "        return regret_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, true_r, gainF, cost, race, d_hat = ctx.saved_tensors\n",
    "        params = ctx.params\n",
    "        grad_regret = torch.zeros_like(pred_r)\n",
    "\n",
    "        if d_hat is None:\n",
    "            return (torch.zeros_like(pred_r),) + (None,) * 8\n",
    "\n",
    "        try:\n",
    "            if params['grad_method'] == 'closed-form':\n",
    "                # (Closed-form gradient calculation remains the same)\n",
    "                if params['group']:\n",
    "                    pred_r_np, cost_np, race_np = map(to_numpy_1d, [pred_r, cost, race])\n",
    "                    grad_obj_wrt_d_hat = compute_group_gradient_analytical(d_hat, true_r, race, params['alpha'])\n",
    "                    v_np = to_numpy_1d(grad_obj_wrt_d_hat)\n",
    "                    Jac_mat = solve_coupled_group_grad(pred_r_np, cost_np, race_np, params['Q'], params['alpha'])\n",
    "                    vT_J_np = v_np @ Jac_mat\n",
    "                    grad_regret = -torch.from_numpy(vT_J_np).to(pred_r.device,dtype=pred_r.dtype)\n",
    "\n",
    "                else:\n",
    "                    pred_r_np, cost_np, gainF_np = map(to_numpy_1d, [pred_r, cost, gainF])\n",
    "                    jac = compute_gradient_closed_form(gainF_np, pred_r_np, cost_np, params['alpha'], params['Q'])\n",
    "                    grad_obj_wrt_d_hat = (true_r * gainF) ** (1 - params['alpha']) * d_hat ** (-params['alpha']) # Grad of alpha-fairness obj\n",
    "                    jac_tensor = torch.from_numpy(jac).to(pred_r.device, dtype=pred_r.dtype)\n",
    "                    grad_obj_tensor = grad_obj_wrt_d_hat.to(dtype=pred_r.dtype, device=pred_r.device)\n",
    "                    grad_regret = -grad_obj_tensor @ jac_tensor\n",
    "            \n",
    "            elif params['grad_method'] == 'finite-diff':\n",
    "\n",
    "                pred_r_np = to_numpy_1d(pred_r)\n",
    "                grad_regret_np = np.zeros_like(pred_r_np)\n",
    "\n",
    "                eps = 1e-3                                    # relative 0.1 %\n",
    "                eps_vec = eps * np.maximum(1.0, np.abs(pred_r_np))\n",
    "\n",
    "                # Detach and convert tensors needed for perturbations once\n",
    "                true_r_np, gainF_np, cost_np, race_np = map(\n",
    "                    lambda t: to_numpy_1d(t.detach()), [true_r, gainF, cost, race]\n",
    "                )\n",
    "\n",
    "                for i in range(len(pred_r_np)):\n",
    "                    # Perturb pred_r for forward and backward steps\n",
    "                    pred_r_plus = pred_r_np.copy(); pred_r_plus[i]  += eps_vec[i]\n",
    "                    pred_r_minus = pred_r_np.copy(); pred_r_minus[i] -= eps_vec[i]\n",
    "\n",
    "                    regret_plus, _ = _calculate_regret_and_d_hat(pred_r_plus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "                    regret_minus, _ = _calculate_regret_and_d_hat(pred_r_minus, true_r_np, gainF_np, cost_np, race_np, params['Q'], params['alpha'], params['group'])\n",
    "\n",
    "                    grad_regret_np[i] = (regret_plus - regret_minus) / (2 * eps_vec[i])\n",
    "\n",
    "                # The gradient of the loss is the negative of the gradient of the regret\n",
    "                grad_regret = torch.from_numpy(grad_regret_np).to(pred_r.device, dtype=pred_r.dtype)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Gradient calculation failed: {e}. Returning zero grad.\")\n",
    "\n",
    "        return (grad_output * grad_regret, None, None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class FDFLLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision-Focused + Fairness Loss Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, alpha, lambdas, fairness_type, group, grad_method='closed-form'):\n",
    "        super().__init__()\n",
    "        self.Q, self.alpha, self.lambdas = Q, alpha, lambdas\n",
    "        self.fairness_type, self.group, self.grad_method = fairness_type, group, grad_method\n",
    "\n",
    "    def forward(self, pred_r, true_r, gainF, cost, race):\n",
    "        # 1. Regret loss from the custom function\n",
    "        regret_loss = RegretLossFn.apply(pred_r, true_r, gainF, cost, race, self.Q, self.alpha, self.group, self.grad_method)\n",
    "\n",
    "        # 2. Fairness penalty using standard PyTorch autograd\n",
    "        fairness_penalty = torch.tensor(0.0, device=pred_r.device)\n",
    "        if self.lambdas > 0 and self.fairness_type != 'none':\n",
    "            mode = 'between' if self.group else 'individual'\n",
    "            if self.fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_r, true_r, race=race, beta=0.5, mode=mode)\n",
    "            elif self.fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_r, true_r, race=race, mode=mode)\n",
    "            elif self.fairness_type == 'acc_parity' and self.group:\n",
    "                fairness_penalty = compute_group_accuracy_parity(pred_r, true_r, race)\n",
    "        \n",
    "        # 3. Total loss\n",
    "        total_loss = regret_loss + self.lambdas * fairness_penalty\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a09dba",
   "metadata": {},
   "source": [
    "# Training Gemini Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0ebf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume helper functions (FDFLLoss, _calculate_loss_and_decision, etc.) are defined elsewhere\n",
    "\n",
    "def train_model_regret(\n",
    "        X_train, y_train, race_train, cost_train, gainF_train,\n",
    "        X_test,  y_test,  race_test,  cost_test, gainF_test,\n",
    "        model_class, input_dim,\n",
    "        alpha, Q,\n",
    "        lambda_fair=0.0, fairness_type=\"none\", group=True, grad_method='closed-form',\n",
    "        num_epochs=30, lr=1e-2, batch_size=None,\n",
    "        dropout_rate=0.1, weight_decay=1e-4,\n",
    "        device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a predictor via direct regret minimization, logging detailed metrics\n",
    "    at each evaluation point.\n",
    "    \"\"\"\n",
    "    # --- Setup (Tensors, Dataloader, Model, etc.) ---\n",
    "    tensors = [X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test]\n",
    "    X_train, y_train, race_train, cost_train, gainF_train, X_test, y_test, race_test, cost_test, gainF_test = [\n",
    "        torch.tensor(t, dtype=torch.float32, device=device) if not isinstance(t, torch.Tensor) else t.to(device) for t in tensors\n",
    "    ]\n",
    "    train_ds = TensorDataset(X_train, y_train, race_train, cost_train, gainF_train)\n",
    "    if batch_size is None: batch_size = len(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = model_class(input_dim, dropout_rate=dropout_rate).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = FDFLLoss(Q, alpha, lambda_fair, fairness_type, group, grad_method)\n",
    "\n",
    "    # --- Initialize Logs ---\n",
    "    loss_log, mse_log, regret_log, fairness_log = [], [], [], []\n",
    "    unique_groups = torch.unique(race_test).cpu().numpy()\n",
    "    per_group_mse_log = {g: [] for g in unique_groups}\n",
    "    per_group_obj_log = {g: [] for g in unique_groups}\n",
    "    per_group_true_benefit_log = {g: [] for g in unique_groups}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_b, y_b, r_b, c_b, g_b in train_loader:\n",
    "            pred_b = model(x_b).squeeze().clamp(min=1e-4)\n",
    "            loss = crit(pred_b, y_b, g_b, c_b, r_b)\n",
    "            optim.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            epoch_loss += loss.item() * x_b.size(0)\n",
    "        loss_log.append(epoch_loss / len(train_ds))\n",
    "\n",
    "        # --- Periodic Evaluation on Test Set ---\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test).squeeze().clamp(min=1e-4)\n",
    "                # Overall MSE\n",
    "                mse_val = ((pred_test - y_test).pow(2)).mean().item()\n",
    "                mse_log.append(mse_val)\n",
    "\n",
    "                # Overall Regret\n",
    "                _, _, d_pred_np = _calculate_loss_and_decision(pred_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                _, _, d_true_np = _calculate_loss_and_decision(y_test, y_test, gainF_test, cost_test, race_test, Q, alpha, 0, 'none', group)\n",
    "                if d_pred_np is not None and d_true_np is not None:\n",
    "                    y_test_np = to_numpy_1d(y_test)\n",
    "                    race_test_np = to_numpy_1d(race_test)\n",
    "                    if group:\n",
    "                        true_obj = compute_coupled_group_obj(d_true_np, y_test_np, race_test_np, alpha)\n",
    "                        pred_obj = compute_coupled_group_obj(d_pred_np, y_test_np, race_test_np, alpha)\n",
    "                    else:\n",
    "                        true_obj = AlphaFairness(y_test_np * d_true_np, alpha)\n",
    "                        pred_obj = AlphaFairness(y_test_np * d_pred_np, alpha)\n",
    "                    norm_regret = (true_obj - pred_obj) / (abs(true_obj) + 1e-7)\n",
    "                else:\n",
    "                    norm_regret = np.nan\n",
    "                regret_log.append(norm_regret)\n",
    "\n",
    "                # Overall Fairness\n",
    "                fair_val = 0.0\n",
    "                mode = 'between' if group else 'individual'\n",
    "                if fairness_type == \"acc_parity\" and group: fair_val = compute_group_accuracy_parity(pred_test, y_test, race_test).item()\n",
    "                elif fairness_type == \"atkinson\": fair_val = atkinson_loss(pred_test, y_test, race_test, beta=0.5, mode=mode).item()\n",
    "                elif fairness_type == \"mad\": fair_val = mean_abs_dev(pred_test, y_test, race_test, mode=mode).item()\n",
    "                fairness_log.append(fair_val)\n",
    "\n",
    "                # Group-wise Metrics\n",
    "                for g in unique_groups:\n",
    "                    mask = (race_test == g)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    # Group MSE\n",
    "                    per_group_mse_log[g].append(((pred_test[mask] - y_test[mask]).pow(2)).mean().item())\n",
    "                    # Group True Benefit\n",
    "                    per_group_true_benefit_log[g].append(y_test[mask].mean().item())\n",
    "                    # Group Decision Objective\n",
    "                    if d_pred_np is not None:\n",
    "                        group_mask_np = (race_test_np == g)\n",
    "                        # We use the true benefits (y_test) to evaluate the utility of the decisions (d_pred_np)\n",
    "                        group_utility = y_test_np[group_mask_np] * d_pred_np[group_mask_np]\n",
    "                        # For simplicity, we report the mean utility as the objective\n",
    "                        per_group_obj_log[g].append(group_utility.mean())\n",
    "                    else:\n",
    "                        per_group_obj_log[g].append(np.nan)\n",
    "\n",
    "                print(f\"Epoch {epoch:03d}/{num_epochs} | Train-Loss {loss_log[-1]:.4f} | Test-MSE {mse_log[-1]:.4f} | Regret {regret_log[-1]:.4f} | Fair-Val {fairness_log[-1]:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f}s.\")\n",
    "\n",
    "    # Return a dictionary of all logs\n",
    "    return model, {\n",
    "        \"loss_log\": loss_log, \"mse_log\": mse_log, \"regret_log\": regret_log, \"fairness_log\": fairness_log,\n",
    "        \"training_time\": total_time,\n",
    "        \"per_group_mse\": per_group_mse_log,\n",
    "        \"per_group_decision_objective\": per_group_obj_log,\n",
    "        \"per_group_true_benefit\": per_group_true_benefit_log\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8386a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23a66413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha = 2\n",
    "# hyperparams = {\n",
    "#     \"alpha\":alpha,\n",
    "#     \"Q\": 1000,\n",
    "#     \"lambda_fair\": 0,\n",
    "#     \"fairness_type\": \"atkinson\",   \n",
    "#     \"group\": True,            # Set to True for group fairness, False for individual\n",
    "#     \"grad_method\": \"finite-diff\",\n",
    "#     \"num_epochs\": 50,        \n",
    "#     \"lr\": 0.005,\n",
    "#     \"batch_size\": len(b_train),\n",
    "#     \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# }\n",
    "\n",
    "# final_model, logs = train_model_regret(\n",
    "#     X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "#     X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "#     model_class=FairRiskPredictor,\n",
    "#     input_dim=feats_train.shape[1],\n",
    "#     **hyperparams\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab4122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1.  MULTI-TRIAL REGRET TRAINING WITH FULL LOGGING\n",
    "# ---------------------------------------------------------------------\n",
    "def train_many_trials_regret(n_trials=3, base_seed=2025, **train_args):\n",
    "    \"\"\"\n",
    "    Run `train_model_regret` for `n_trials` different seeds.\n",
    "    Returns a FLAT dict whose keys are:\n",
    "        regret, regret_std, mse, mse_std, fairness, fairness_std, …,\n",
    "        G0_mse, G0_mse_std, G0_decision_obj, G0_decision_obj_std, …\n",
    "    \"\"\"\n",
    "    # -------------------- run all trials -----------------------------\n",
    "    per_trial_metrics = defaultdict(list)      # collects trial-level scalars\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        seed = base_seed + t\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        _, logs = train_model_regret(**train_args)   # one full run\n",
    "\n",
    "        # ---- overall scalars ---------------------------------------\n",
    "        per_trial_metrics['regret'       ].append(logs['regret_log']  [-1])\n",
    "        per_trial_metrics['mse'          ].append(logs['mse_log']     [-1])\n",
    "        per_trial_metrics['fairness'     ].append(logs['fairness_log'][-1])\n",
    "        per_trial_metrics['training_time'].append(logs['training_time'])\n",
    "\n",
    "        # ---- per-group metrics (final epoch) -----------------------\n",
    "        for g_id, g_log in logs['per_group_mse'].items():\n",
    "            if g_log:                      # just in case\n",
    "                per_trial_metrics[f'G{int(g_id)}_mse'          ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_decision_objective'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_decision_obj' ].append(g_log[-1])\n",
    "        for g_id, g_log in logs['per_group_true_benefit'].items():\n",
    "            if g_log:\n",
    "                per_trial_metrics[f'G{int(g_id)}_true_benefit' ].append(g_log[-1])\n",
    "\n",
    "    # -------------------- aggregate over trials ----------------------\n",
    "    avg_results = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      AVERAGED RESULTS ACROSS ALL TRIALS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for key, values in per_trial_metrics.items():\n",
    "        μ, σ = np.mean(values), np.std(values)\n",
    "        avg_results[key]      = μ\n",
    "        avg_results[f'{key}_std'] = σ\n",
    "        print(f\"[{key.upper():>20s}]  μ = {μ:.4f} | σ = {σ:.4f}\")\n",
    "\n",
    "    return avg_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175a7d",
   "metadata": {},
   "source": [
    "# Verify finite-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68991548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1364 | Test-MSE 367.8798 | Regret 0.0582 | Fair-Val 86.3141\n",
      "Epoch 010/50 | Train-Loss 18.0152 | Test-MSE 351.3795 | Regret 0.0441 | Fair-Val 75.3053\n",
      "Epoch 020/50 | Train-Loss 14.6181 | Test-MSE 345.6021 | Regret 0.0395 | Fair-Val 70.8882\n",
      "Epoch 030/50 | Train-Loss 10.8967 | Test-MSE 345.0088 | Regret 0.0339 | Fair-Val 72.0103\n",
      "Epoch 040/50 | Train-Loss 9.3171 | Test-MSE 343.7295 | Regret 0.0323 | Fair-Val 72.5748\n",
      "Epoch 050/50 | Train-Loss 9.6627 | Test-MSE 345.1768 | Regret 0.0342 | Fair-Val 72.4109\n",
      "Training finished in 260.65s.\n",
      "Epoch 001/50 | Train-Loss 29.5292 | Test-MSE 368.4040 | Regret 0.0598 | Fair-Val 86.0064\n",
      "Epoch 010/50 | Train-Loss 18.8730 | Test-MSE 352.0433 | Regret 0.0464 | Fair-Val 74.4984\n",
      "Epoch 020/50 | Train-Loss 14.8075 | Test-MSE 346.7471 | Regret 0.0399 | Fair-Val 70.7053\n",
      "Epoch 030/50 | Train-Loss 11.2057 | Test-MSE 345.8056 | Regret 0.0332 | Fair-Val 71.7752\n",
      "Epoch 040/50 | Train-Loss 9.0782 | Test-MSE 345.2228 | Regret 0.0311 | Fair-Val 72.4830\n",
      "Epoch 050/50 | Train-Loss 9.7759 | Test-MSE 346.4650 | Regret 0.0334 | Fair-Val 72.3844\n",
      "Training finished in 288.59s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0338 | σ = 0.0004\n",
      "[                 MSE]  μ = 345.8209 | σ = 0.6441\n",
      "[            FAIRNESS]  μ = 72.3976 | σ = 0.0132\n",
      "[       TRAINING_TIME]  μ = 274.6215 | σ = 13.9728\n",
      "[              G0_MSE]  μ = 328.5613 | σ = 0.6472\n",
      "[              G1_MSE]  μ = 473.3566 | σ = 0.6208\n",
      "[     G0_DECISION_OBJ]  μ = 42.8856 | σ = 0.3811\n",
      "[     G1_DECISION_OBJ]  μ = 326.1791 | σ = 3.3642\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0791 | Test-MSE 368.9313 | Regret 0.0012 | Fair-Val 86.8169\n",
      "Epoch 010/50 | Train-Loss 0.0590 | Test-MSE 363.0511 | Regret 0.0009 | Fair-Val 83.4507\n",
      "Epoch 020/50 | Train-Loss 0.0511 | Test-MSE 359.3484 | Regret 0.0008 | Fair-Val 81.0775\n",
      "Epoch 030/50 | Train-Loss 0.0447 | Test-MSE 355.9090 | Regret 0.0007 | Fair-Val 79.3244\n",
      "Epoch 040/50 | Train-Loss 0.0405 | Test-MSE 354.3688 | Regret 0.0007 | Fair-Val 78.6764\n",
      "Epoch 050/50 | Train-Loss 0.0385 | Test-MSE 354.4651 | Regret 0.0006 | Fair-Val 78.7496\n",
      "Training finished in 265.35s.\n",
      "Epoch 001/50 | Train-Loss 0.0811 | Test-MSE 369.4257 | Regret 0.0012 | Fair-Val 86.6950\n",
      "Epoch 010/50 | Train-Loss 0.0607 | Test-MSE 363.4334 | Regret 0.0010 | Fair-Val 83.4439\n",
      "Epoch 020/50 | Train-Loss 0.0517 | Test-MSE 359.2051 | Regret 0.0008 | Fair-Val 80.9613\n",
      "Epoch 030/50 | Train-Loss 0.0450 | Test-MSE 355.7538 | Regret 0.0007 | Fair-Val 79.3688\n",
      "Epoch 040/50 | Train-Loss 0.0407 | Test-MSE 354.2676 | Regret 0.0007 | Fair-Val 78.7886\n",
      "Epoch 050/50 | Train-Loss 0.0387 | Test-MSE 354.2306 | Regret 0.0007 | Fair-Val 78.6933\n",
      "Training finished in 264.19s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0007 | σ = 0.0000\n",
      "[                 MSE]  μ = 354.3478 | σ = 0.1172\n",
      "[            FAIRNESS]  μ = 78.7214 | σ = 0.0282\n",
      "[       TRAINING_TIME]  μ = 264.7722 | σ = 0.5807\n",
      "[              G0_MSE]  μ = 335.5806 | σ = 0.1105\n",
      "[              G1_MSE]  μ = 493.0235 | σ = 0.1669\n",
      "[     G0_DECISION_OBJ]  μ = 19.4207 | σ = 0.0006\n",
      "[     G1_DECISION_OBJ]  μ = 162.4401 | σ = 0.0228\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.5959 | Test-MSE 367.7102 | Regret 0.0235 | Fair-Val 86.3295\n",
      "Epoch 010/50 | Train-Loss 1.3192 | Test-MSE 349.5978 | Regret 0.0147 | Fair-Val 76.1594\n",
      "Epoch 020/50 | Train-Loss 0.8375 | Test-MSE 344.6614 | Regret 0.0103 | Fair-Val 75.0299\n",
      "Epoch 030/50 | Train-Loss 0.4609 | Test-MSE 345.2528 | Regret 0.0064 | Fair-Val 78.9392\n",
      "Epoch 040/50 | Train-Loss 0.2716 | Test-MSE 343.6415 | Regret 0.0045 | Fair-Val 80.6732\n",
      "Epoch 050/50 | Train-Loss 0.2228 | Test-MSE 340.2235 | Regret 0.0038 | Fair-Val 80.3261\n",
      "Training finished in 265.36s.\n",
      "Epoch 001/50 | Train-Loss 2.7243 | Test-MSE 367.9658 | Regret 0.0244 | Fair-Val 85.9866\n",
      "Epoch 010/50 | Train-Loss 1.4062 | Test-MSE 348.4402 | Regret 0.0158 | Fair-Val 74.9749\n",
      "Epoch 020/50 | Train-Loss 0.9004 | Test-MSE 341.3633 | Regret 0.0112 | Fair-Val 73.1813\n",
      "Epoch 030/50 | Train-Loss 0.5294 | Test-MSE 341.7073 | Regret 0.0072 | Fair-Val 77.0826\n",
      "Epoch 040/50 | Train-Loss 0.3175 | Test-MSE 341.0757 | Regret 0.0049 | Fair-Val 79.3061\n",
      "Epoch 050/50 | Train-Loss 0.2310 | Test-MSE 340.1309 | Regret 0.0040 | Fair-Val 80.4818\n",
      "Training finished in 284.23s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0039 | σ = 0.0001\n",
      "[                 MSE]  μ = 340.1772 | σ = 0.0463\n",
      "[            FAIRNESS]  μ = 80.4039 | σ = 0.0779\n",
      "[       TRAINING_TIME]  μ = 274.7976 | σ = 9.4328\n",
      "[              G0_MSE]  μ = 321.0090 | σ = 0.0649\n",
      "[              G1_MSE]  μ = 481.8168 | σ = 0.0908\n",
      "[     G0_DECISION_OBJ]  μ = 16.7920 | σ = 0.0027\n",
      "[     G1_DECISION_OBJ]  μ = 55.1753 | σ = 0.0672\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 367.6938 | Regret 0.2827 | Fair-Val 86.3223\n",
      "Epoch 010/50 | Train-Loss 35.9700 | Test-MSE 349.2939 | Regret 0.1762 | Fair-Val 75.9704\n",
      "Epoch 020/50 | Train-Loss 22.4655 | Test-MSE 343.6129 | Regret 0.1314 | Fair-Val 74.4045\n",
      "Epoch 030/50 | Train-Loss 12.9719 | Test-MSE 343.9438 | Regret 0.0869 | Fair-Val 78.1823\n",
      "Epoch 040/50 | Train-Loss 7.8578 | Test-MSE 342.8050 | Regret 0.0633 | Fair-Val 79.4415\n",
      "Epoch 050/50 | Train-Loss 6.2986 | Test-MSE 341.1035 | Regret 0.0524 | Fair-Val 79.3678\n",
      "Training finished in 297.81s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 367.9811 | Regret 0.2959 | Fair-Val 85.9856\n",
      "Epoch 010/50 | Train-Loss 38.3287 | Test-MSE 348.4311 | Regret 0.1880 | Fair-Val 74.7761\n",
      "Epoch 020/50 | Train-Loss 24.1528 | Test-MSE 340.6932 | Regret 0.1399 | Fair-Val 72.4568\n",
      "Epoch 030/50 | Train-Loss 14.7981 | Test-MSE 340.2128 | Regret 0.0967 | Fair-Val 75.9388\n",
      "Epoch 040/50 | Train-Loss 9.4566 | Test-MSE 339.5769 | Regret 0.0711 | Fair-Val 77.9233\n",
      "Epoch 050/50 | Train-Loss 6.6620 | Test-MSE 339.0392 | Regret 0.0568 | Fair-Val 78.8487\n",
      "Training finished in 290.79s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0546 | σ = 0.0022\n",
      "[                 MSE]  μ = 340.0714 | σ = 1.0322\n",
      "[            FAIRNESS]  μ = 79.1083 | σ = 0.2596\n",
      "[       TRAINING_TIME]  μ = 294.2979 | σ = 3.5094\n",
      "[              G0_MSE]  μ = 321.2119 | σ = 0.9703\n",
      "[              G1_MSE]  μ = 479.4285 | σ = 1.4895\n",
      "[     G0_DECISION_OBJ]  μ = 16.5952 | σ = 0.0130\n",
      "[     G1_DECISION_OBJ]  μ = 21.6078 | σ = 0.0297\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 130.9927 | Test-MSE 368.1827 | Regret 0.0605 | Fair-Val 86.3484\n",
      "Epoch 010/50 | Train-Loss 96.7300 | Test-MSE 343.0323 | Regret 0.0585 | Fair-Val 70.4800\n",
      "Epoch 020/50 | Train-Loss 57.5015 | Test-MSE 299.8220 | Regret 0.0865 | Fair-Val 45.5721\n",
      "Epoch 030/50 | Train-Loss 53.5532 | Test-MSE 271.6016 | Regret 0.1044 | Fair-Val 34.3819\n",
      "Epoch 040/50 | Train-Loss 43.2272 | Test-MSE 264.1869 | Regret 0.1012 | Fair-Val 34.3406\n",
      "Epoch 050/50 | Train-Loss 44.3510 | Test-MSE 261.2510 | Regret 0.1010 | Fair-Val 34.4612\n",
      "Training finished in 288.00s.\n",
      "Epoch 001/50 | Train-Loss 131.3402 | Test-MSE 368.6674 | Regret 0.0619 | Fair-Val 86.0437\n",
      "Epoch 010/50 | Train-Loss 95.7219 | Test-MSE 342.4155 | Regret 0.0587 | Fair-Val 69.1483\n",
      "Epoch 020/50 | Train-Loss 57.0154 | Test-MSE 298.4135 | Regret 0.0879 | Fair-Val 44.3277\n",
      "Epoch 030/50 | Train-Loss 53.1393 | Test-MSE 271.1172 | Regret 0.1069 | Fair-Val 33.7902\n",
      "Epoch 040/50 | Train-Loss 44.2380 | Test-MSE 265.7292 | Regret 0.1022 | Fair-Val 33.7083\n",
      "Epoch 050/50 | Train-Loss 45.8977 | Test-MSE 260.8488 | Regret 0.1049 | Fair-Val 33.5920\n",
      "Training finished in 374.42s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.1029 | σ = 0.0019\n",
      "[                 MSE]  μ = 261.0499 | σ = 0.2011\n",
      "[            FAIRNESS]  μ = 34.0266 | σ = 0.4346\n",
      "[       TRAINING_TIME]  μ = 331.2092 | σ = 43.2095\n",
      "[              G0_MSE]  μ = 252.9380 | σ = 0.0975\n",
      "[              G1_MSE]  μ = 320.9912 | σ = 0.9667\n",
      "[     G0_DECISION_OBJ]  μ = 37.0175 | σ = 0.3397\n",
      "[     G1_DECISION_OBJ]  μ = 254.4055 | σ = 1.5530\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 101.9354 | Test-MSE 368.1843 | Regret 0.0011 | Fair-Val 86.3485\n",
      "Epoch 010/50 | Train-Loss 72.7652 | Test-MSE 343.0642 | Regret 0.0011 | Fair-Val 70.4795\n",
      "Epoch 020/50 | Train-Loss 22.5270 | Test-MSE 299.8628 | Regret 0.0017 | Fair-Val 45.5697\n",
      "Epoch 030/50 | Train-Loss 9.6397 | Test-MSE 271.7302 | Regret 0.0022 | Fair-Val 34.3378\n",
      "Epoch 040/50 | Train-Loss 0.1606 | Test-MSE 264.3363 | Regret 0.0020 | Fair-Val 34.2117\n",
      "Epoch 050/50 | Train-Loss 0.9015 | Test-MSE 261.5043 | Regret 0.0020 | Fair-Val 34.3057\n",
      "Training finished in 367.96s.\n",
      "Epoch 001/50 | Train-Loss 101.8921 | Test-MSE 368.6685 | Regret 0.0012 | Fair-Val 86.0434\n",
      "Epoch 010/50 | Train-Loss 71.3641 | Test-MSE 342.4558 | Regret 0.0011 | Fair-Val 69.1525\n",
      "Epoch 020/50 | Train-Loss 21.1506 | Test-MSE 298.5081 | Regret 0.0017 | Fair-Val 44.3235\n",
      "Epoch 030/50 | Train-Loss 8.2268 | Test-MSE 271.2350 | Regret 0.0023 | Fair-Val 33.7938\n",
      "Epoch 040/50 | Train-Loss 0.5205 | Test-MSE 265.7590 | Regret 0.0020 | Fair-Val 33.7651\n",
      "Epoch 050/50 | Train-Loss 0.8228 | Test-MSE 260.9469 | Regret 0.0021 | Fair-Val 33.6743\n",
      "Training finished in 407.00s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0020 | σ = 0.0001\n",
      "[                 MSE]  μ = 261.2256 | σ = 0.2787\n",
      "[            FAIRNESS]  μ = 33.9900 | σ = 0.3157\n",
      "[       TRAINING_TIME]  μ = 387.4786 | σ = 19.5194\n",
      "[              G0_MSE]  μ = 253.1224 | σ = 0.2034\n",
      "[              G1_MSE]  μ = 321.1023 | σ = 0.8348\n",
      "[     G0_DECISION_OBJ]  μ = 20.0244 | σ = 0.0328\n",
      "[     G1_DECISION_OBJ]  μ = 166.4415 | σ = 0.4693\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 104.4521 | Test-MSE 367.9945 | Regret 0.0245 | Fair-Val 86.3245\n",
      "Epoch 010/50 | Train-Loss 74.4972 | Test-MSE 340.7768 | Regret 0.0201 | Fair-Val 70.1660\n",
      "Epoch 020/50 | Train-Loss 25.3472 | Test-MSE 295.6144 | Regret 0.0295 | Fair-Val 45.2030\n",
      "Epoch 030/50 | Train-Loss 10.2747 | Test-MSE 264.2261 | Regret 0.0342 | Fair-Val 34.1149\n",
      "Epoch 040/50 | Train-Loss 4.1180 | Test-MSE 253.6909 | Regret 0.0270 | Fair-Val 33.3725\n",
      "Epoch 050/50 | Train-Loss 3.2398 | Test-MSE 248.9266 | Regret 0.0257 | Fair-Val 32.6145\n",
      "Training finished in 439.96s.\n",
      "Epoch 001/50 | Train-Loss 104.5352 | Test-MSE 368.4731 | Regret 0.0254 | Fair-Val 86.0094\n",
      "Epoch 010/50 | Train-Loss 72.9718 | Test-MSE 340.1438 | Regret 0.0210 | Fair-Val 68.7448\n",
      "Epoch 020/50 | Train-Loss 24.0954 | Test-MSE 293.4260 | Regret 0.0299 | Fair-Val 43.8591\n",
      "Epoch 030/50 | Train-Loss 8.8489 | Test-MSE 262.4763 | Regret 0.0340 | Fair-Val 33.5511\n",
      "Epoch 040/50 | Train-Loss 4.1156 | Test-MSE 254.1625 | Regret 0.0269 | Fair-Val 32.4795\n",
      "Epoch 050/50 | Train-Loss 3.6402 | Test-MSE 246.8473 | Regret 0.0265 | Fair-Val 31.5595\n",
      "Training finished in 349.90s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0261 | σ = 0.0004\n",
      "[                 MSE]  μ = 247.8869 | σ = 1.0396\n",
      "[            FAIRNESS]  μ = 32.0870 | σ = 0.5275\n",
      "[       TRAINING_TIME]  μ = 394.9320 | σ = 45.0305\n",
      "[              G0_MSE]  μ = 240.2374 | σ = 0.9139\n",
      "[              G1_MSE]  μ = 304.4114 | σ = 1.9688\n",
      "[     G0_DECISION_OBJ]  μ = 18.1650 | σ = 0.0180\n",
      "[     G1_DECISION_OBJ]  μ = 60.6730 | σ = 0.0367\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 176.4621 | Test-MSE 367.6102 | Regret 0.2830 | Fair-Val 86.2852\n",
      "Epoch 010/50 | Train-Loss 117.9997 | Test-MSE 341.2782 | Regret 0.1884 | Fair-Val 70.9358\n",
      "Epoch 020/50 | Train-Loss 76.8042 | Test-MSE 305.1606 | Regret 0.1827 | Fair-Val 49.7680\n",
      "Epoch 030/50 | Train-Loss 40.2841 | Test-MSE 267.6674 | Regret 0.1759 | Fair-Val 32.2525\n",
      "Epoch 040/50 | Train-Loss 25.5816 | Test-MSE 245.5383 | Regret 0.1490 | Fair-Val 26.0923\n",
      "Epoch 050/50 | Train-Loss 20.8500 | Test-MSE 236.8726 | Regret 0.1251 | Fair-Val 26.4464\n",
      "Training finished in 261.06s.\n",
      "Epoch 001/50 | Train-Loss 180.8339 | Test-MSE 367.9559 | Regret 0.2959 | Fair-Val 85.9678\n",
      "Epoch 010/50 | Train-Loss 117.9005 | Test-MSE 340.0684 | Regret 0.2023 | Fair-Val 69.3820\n",
      "Epoch 020/50 | Train-Loss 75.0964 | Test-MSE 302.8564 | Regret 0.1952 | Fair-Val 47.6144\n",
      "Epoch 030/50 | Train-Loss 42.5384 | Test-MSE 265.9330 | Regret 0.1937 | Fair-Val 30.7163\n",
      "Epoch 040/50 | Train-Loss 28.5388 | Test-MSE 245.5000 | Regret 0.1717 | Fair-Val 25.1527\n",
      "Epoch 050/50 | Train-Loss 21.7622 | Test-MSE 238.6871 | Regret 0.1343 | Fair-Val 26.2034\n",
      "Training finished in 257.97s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.1297 | σ = 0.0046\n",
      "[                 MSE]  μ = 237.7798 | σ = 0.9072\n",
      "[            FAIRNESS]  μ = 26.3249 | σ = 0.1215\n",
      "[       TRAINING_TIME]  μ = 259.5163 | σ = 1.5443\n",
      "[              G0_MSE]  μ = 231.5040 | σ = 0.9362\n",
      "[              G1_MSE]  μ = 284.1538 | σ = 0.6932\n",
      "[     G0_DECISION_OBJ]  μ = 17.4279 | σ = 0.0290\n",
      "[     G1_DECISION_OBJ]  μ = 22.8619 | σ = 1.9791\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1420 | Test-MSE 367.8849 | Regret 0.0582 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 18.1931 | Test-MSE 350.7636 | Regret 0.0448 | Fair-Val 0.0041\n",
      "Epoch 020/50 | Train-Loss 15.3316 | Test-MSE 342.2633 | Regret 0.0419 | Fair-Val 0.0036\n",
      "Epoch 030/50 | Train-Loss 11.9044 | Test-MSE 338.3623 | Regret 0.0370 | Fair-Val 0.0035\n",
      "Epoch 040/50 | Train-Loss 10.2442 | Test-MSE 334.4513 | Regret 0.0356 | Fair-Val 0.0035\n",
      "Epoch 050/50 | Train-Loss 10.2876 | Test-MSE 333.1858 | Regret 0.0370 | Fair-Val 0.0034\n",
      "Training finished in 257.30s.\n",
      "Epoch 001/50 | Train-Loss 29.5348 | Test-MSE 368.4062 | Regret 0.0599 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 19.0916 | Test-MSE 351.1875 | Regret 0.0472 | Fair-Val 0.0040\n",
      "Epoch 020/50 | Train-Loss 15.5270 | Test-MSE 343.2560 | Regret 0.0420 | Fair-Val 0.0035\n",
      "Epoch 030/50 | Train-Loss 12.1963 | Test-MSE 339.2955 | Regret 0.0362 | Fair-Val 0.0034\n",
      "Epoch 040/50 | Train-Loss 9.9024 | Test-MSE 335.9558 | Regret 0.0344 | Fair-Val 0.0034\n",
      "Epoch 050/50 | Train-Loss 10.4502 | Test-MSE 334.3353 | Regret 0.0364 | Fair-Val 0.0033\n",
      "Training finished in 257.07s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0367 | σ = 0.0003\n",
      "[                 MSE]  μ = 333.7605 | σ = 0.5748\n",
      "[            FAIRNESS]  μ = 0.0034 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 257.1827 | σ = 0.1170\n",
      "[              G0_MSE]  μ = 318.5927 | σ = 0.6685\n",
      "[              G1_MSE]  μ = 445.8398 | σ = 0.1179\n",
      "[     G0_DECISION_OBJ]  μ = 42.3697 | σ = 0.0371\n",
      "[     G1_DECISION_OBJ]  μ = 328.4026 | σ = 2.1837\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0847 | Test-MSE 369.1399 | Regret 0.0012 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 0.0655 | Test-MSE 355.0094 | Regret 0.0011 | Fair-Val 0.0040\n",
      "Epoch 020/50 | Train-Loss 0.0831 | Test-MSE 324.0255 | Regret 0.0015 | Fair-Val 0.0023\n",
      "Epoch 030/50 | Train-Loss 0.0983 | Test-MSE 294.9270 | Regret 0.0018 | Fair-Val 0.0014\n",
      "Epoch 040/50 | Train-Loss 0.0810 | Test-MSE 290.4591 | Regret 0.0014 | Fair-Val 0.0014\n",
      "Epoch 050/50 | Train-Loss 0.0681 | Test-MSE 294.7134 | Regret 0.0012 | Fair-Val 0.0016\n",
      "Training finished in 258.45s.\n",
      "Epoch 001/50 | Train-Loss 0.0867 | Test-MSE 369.5161 | Regret 0.0012 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 0.0671 | Test-MSE 353.3637 | Regret 0.0011 | Fair-Val 0.0038\n",
      "Epoch 020/50 | Train-Loss 0.0885 | Test-MSE 320.3190 | Regret 0.0016 | Fair-Val 0.0022\n",
      "Epoch 030/50 | Train-Loss 0.1018 | Test-MSE 292.4025 | Regret 0.0018 | Fair-Val 0.0013\n",
      "Epoch 040/50 | Train-Loss 0.0810 | Test-MSE 288.3471 | Regret 0.0015 | Fair-Val 0.0014\n",
      "Epoch 050/50 | Train-Loss 0.0692 | Test-MSE 292.5597 | Regret 0.0012 | Fair-Val 0.0016\n",
      "Training finished in 258.21s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0012 | σ = 0.0000\n",
      "[                 MSE]  μ = 293.6366 | σ = 1.0769\n",
      "[            FAIRNESS]  μ = 0.0016 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 258.3284 | σ = 0.1172\n",
      "[              G0_MSE]  μ = 284.6656 | σ = 0.9985\n",
      "[              G1_MSE]  μ = 359.9256 | σ = 1.6555\n",
      "[     G0_DECISION_OBJ]  μ = 19.5459 | σ = 0.0090\n",
      "[     G1_DECISION_OBJ]  μ = 166.6878 | σ = 0.1942\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.6015 | Test-MSE 367.7094 | Regret 0.0235 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 1.3242 | Test-MSE 349.5745 | Regret 0.0147 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.8428 | Test-MSE 344.5785 | Regret 0.0103 | Fair-Val 0.0043\n",
      "Epoch 030/50 | Train-Loss 0.4665 | Test-MSE 345.1439 | Regret 0.0064 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 0.2771 | Test-MSE 343.5475 | Regret 0.0045 | Fair-Val 0.0049\n",
      "Epoch 050/50 | Train-Loss 0.2283 | Test-MSE 340.1650 | Regret 0.0038 | Fair-Val 0.0050\n",
      "Training finished in 258.05s.\n",
      "Epoch 001/50 | Train-Loss 2.7299 | Test-MSE 367.9658 | Regret 0.0244 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 1.4109 | Test-MSE 348.4189 | Regret 0.0158 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 0.9053 | Test-MSE 341.2744 | Regret 0.0112 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 0.5348 | Test-MSE 341.5698 | Regret 0.0072 | Fair-Val 0.0046\n",
      "Epoch 040/50 | Train-Loss 0.3232 | Test-MSE 340.9334 | Regret 0.0049 | Fair-Val 0.0049\n",
      "Epoch 050/50 | Train-Loss 0.2367 | Test-MSE 339.9988 | Regret 0.0040 | Fair-Val 0.0050\n",
      "Training finished in 258.93s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0039 | σ = 0.0001\n",
      "[                 MSE]  μ = 340.0819 | σ = 0.0831\n",
      "[            FAIRNESS]  μ = 0.0050 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 258.4942 | σ = 0.4402\n",
      "[              G0_MSE]  μ = 320.9521 | σ = 0.0986\n",
      "[              G1_MSE]  μ = 481.4373 | σ = 0.0310\n",
      "[     G0_DECISION_OBJ]  μ = 16.7946 | σ = 0.0024\n",
      "[     G1_DECISION_OBJ]  μ = 55.1699 | σ = 0.0685\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6115 | Test-MSE 367.6938 | Regret 0.2827 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 35.9749 | Test-MSE 349.2930 | Regret 0.1762 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 22.4711 | Test-MSE 343.6101 | Regret 0.1314 | Fair-Val 0.0043\n",
      "Epoch 030/50 | Train-Loss 12.9770 | Test-MSE 343.9417 | Regret 0.0869 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 7.8630 | Test-MSE 342.8056 | Regret 0.0633 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 6.3033 | Test-MSE 341.1059 | Regret 0.0523 | Fair-Val 0.0049\n",
      "Training finished in 260.67s.\n",
      "Epoch 001/50 | Train-Loss 79.0285 | Test-MSE 367.9811 | Regret 0.2959 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 38.3336 | Test-MSE 348.4301 | Regret 0.1880 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 24.1579 | Test-MSE 340.6907 | Regret 0.1399 | Fair-Val 0.0041\n",
      "Epoch 030/50 | Train-Loss 14.8036 | Test-MSE 340.2093 | Regret 0.0967 | Fair-Val 0.0045\n",
      "Epoch 040/50 | Train-Loss 9.4625 | Test-MSE 339.5716 | Regret 0.0711 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 6.6677 | Test-MSE 339.0337 | Regret 0.0568 | Fair-Val 0.0049\n",
      "Training finished in 263.04s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0546 | σ = 0.0022\n",
      "[                 MSE]  μ = 340.0698 | σ = 1.0361\n",
      "[            FAIRNESS]  μ = 0.0049 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 261.8514 | σ = 1.1858\n",
      "[              G0_MSE]  μ = 321.2138 | σ = 0.9748\n",
      "[              G1_MSE]  μ = 479.4019 | σ = 1.4890\n",
      "[     G0_DECISION_OBJ]  μ = 16.5958 | σ = 0.0125\n",
      "[     G1_DECISION_OBJ]  μ = 21.6084 | σ = 0.0291\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1364 | Test-MSE 367.6678 | Regret 0.0577 | Fair-Val 86.3542\n",
      "Epoch 010/50 | Train-Loss 16.7930 | Test-MSE 351.5048 | Regret 0.0401 | Fair-Val 77.5069\n",
      "Epoch 020/50 | Train-Loss 11.3633 | Test-MSE 347.9750 | Regret 0.0302 | Fair-Val 78.1117\n",
      "Epoch 030/50 | Train-Loss 5.8068 | Test-MSE 344.7059 | Regret 0.0186 | Fair-Val 80.3949\n",
      "Epoch 040/50 | Train-Loss 2.5176 | Test-MSE 340.3410 | Regret 0.0112 | Fair-Val 81.9888\n",
      "Epoch 050/50 | Train-Loss 1.9036 | Test-MSE 335.2207 | Regret 0.0091 | Fair-Val 81.4599\n",
      "Training finished in 82.23s.\n",
      "Epoch 001/50 | Train-Loss 29.5292 | Test-MSE 368.0066 | Regret 0.0596 | Fair-Val 86.0443\n",
      "Epoch 010/50 | Train-Loss 17.3969 | Test-MSE 351.2521 | Regret 0.0426 | Fair-Val 77.0010\n",
      "Epoch 020/50 | Train-Loss 11.8313 | Test-MSE 347.6906 | Regret 0.0318 | Fair-Val 77.5101\n",
      "Epoch 030/50 | Train-Loss 6.5466 | Test-MSE 344.7417 | Regret 0.0202 | Fair-Val 79.8850\n",
      "Epoch 040/50 | Train-Loss 2.7086 | Test-MSE 341.7088 | Regret 0.0113 | Fair-Val 81.7395\n",
      "Epoch 050/50 | Train-Loss 1.9225 | Test-MSE 337.5176 | Regret 0.0090 | Fair-Val 81.6133\n",
      "Training finished in 82.23s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0090 | σ = 0.0000\n",
      "[                 MSE]  μ = 336.3692 | σ = 1.1484\n",
      "[            FAIRNESS]  μ = 81.5366 | σ = 0.0767\n",
      "[       TRAINING_TIME]  μ = 82.2301 | σ = 0.0017\n",
      "[              G0_MSE]  μ = 316.9308 | σ = 1.1301\n",
      "[              G1_MSE]  μ = 480.0041 | σ = 1.2836\n",
      "[     G0_DECISION_OBJ]  μ = 45.6962 | σ = 0.3394\n",
      "[     G1_DECISION_OBJ]  μ = 262.8155 | σ = 2.4480\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0791 | Test-MSE 367.6869 | Regret 0.0011 | Fair-Val 86.3649\n",
      "Epoch 010/50 | Train-Loss 0.0432 | Test-MSE 353.3119 | Regret 0.0007 | Fair-Val 78.1437\n",
      "Epoch 020/50 | Train-Loss 0.0303 | Test-MSE 347.2993 | Regret 0.0005 | Fair-Val 75.7751\n",
      "Epoch 030/50 | Train-Loss 0.0192 | Test-MSE 343.6342 | Regret 0.0004 | Fair-Val 77.2162\n",
      "Epoch 040/50 | Train-Loss 0.0114 | Test-MSE 338.2744 | Regret 0.0003 | Fair-Val 78.7383\n",
      "Epoch 050/50 | Train-Loss 0.0078 | Test-MSE 332.0155 | Regret 0.0002 | Fair-Val 78.6607\n",
      "Training finished in 142.08s.\n",
      "Epoch 001/50 | Train-Loss 0.0811 | Test-MSE 369.5245 | Regret 0.0011 | Fair-Val 86.2682\n",
      "Epoch 010/50 | Train-Loss 0.0462 | Test-MSE 354.4642 | Regret 0.0008 | Fair-Val 78.2360\n",
      "Epoch 020/50 | Train-Loss 0.0312 | Test-MSE 346.2079 | Regret 0.0006 | Fair-Val 75.3841\n",
      "Epoch 030/50 | Train-Loss 0.0206 | Test-MSE 342.5598 | Regret 0.0004 | Fair-Val 76.9090\n",
      "Epoch 040/50 | Train-Loss 0.0114 | Test-MSE 346.0287 | Regret 0.0003 | Fair-Val 79.8772\n",
      "Epoch 050/50 | Train-Loss 0.0075 | Test-MSE 347.7295 | Regret 0.0002 | Fair-Val 81.4321\n",
      "Training finished in 142.23s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0002 | σ = 0.0000\n",
      "[                 MSE]  μ = 339.8725 | σ = 7.8570\n",
      "[            FAIRNESS]  μ = 80.0464 | σ = 1.3857\n",
      "[       TRAINING_TIME]  μ = 142.1575 | σ = 0.0740\n",
      "[              G0_MSE]  μ = 320.7894 | σ = 7.5266\n",
      "[              G1_MSE]  μ = 480.8821 | σ = 10.2980\n",
      "[     G0_DECISION_OBJ]  μ = 21.0947 | σ = 0.0140\n",
      "[     G1_DECISION_OBJ]  μ = 171.8365 | σ = 0.0082\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.5959 | Test-MSE 367.6061 | Regret 0.0235 | Fair-Val 86.3237\n",
      "Epoch 010/50 | Train-Loss 1.3206 | Test-MSE 349.3848 | Regret 0.0147 | Fair-Val 76.1751\n",
      "Epoch 020/50 | Train-Loss 0.8445 | Test-MSE 344.3133 | Regret 0.0104 | Fair-Val 74.7084\n",
      "Epoch 030/50 | Train-Loss 0.4673 | Test-MSE 345.7541 | Regret 0.0065 | Fair-Val 78.9343\n",
      "Epoch 040/50 | Train-Loss 0.2756 | Test-MSE 342.3395 | Regret 0.0045 | Fair-Val 80.0937\n",
      "Epoch 050/50 | Train-Loss 0.2274 | Test-MSE 341.2335 | Regret 0.0038 | Fair-Val 80.6327\n",
      "Training finished in 142.95s.\n",
      "Epoch 001/50 | Train-Loss 2.7243 | Test-MSE 368.1580 | Regret 0.0244 | Fair-Val 85.9826\n",
      "Epoch 010/50 | Train-Loss 1.4082 | Test-MSE 349.1245 | Regret 0.0157 | Fair-Val 75.1861\n",
      "Epoch 020/50 | Train-Loss 0.9050 | Test-MSE 341.7624 | Regret 0.0111 | Fair-Val 73.0766\n",
      "Epoch 030/50 | Train-Loss 0.5508 | Test-MSE 336.7126 | Regret 0.0074 | Fair-Val 75.6356\n",
      "Epoch 040/50 | Train-Loss 0.3354 | Test-MSE 334.1299 | Regret 0.0051 | Fair-Val 77.2366\n",
      "Epoch 050/50 | Train-Loss 0.2389 | Test-MSE 333.9861 | Regret 0.0041 | Fair-Val 78.7729\n",
      "Training finished in 143.38s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0040 | σ = 0.0001\n",
      "[                 MSE]  μ = 337.6098 | σ = 3.6237\n",
      "[            FAIRNESS]  μ = 79.7028 | σ = 0.9299\n",
      "[       TRAINING_TIME]  μ = 143.1616 | σ = 0.2153\n",
      "[              G0_MSE]  μ = 318.6087 | σ = 3.4020\n",
      "[              G1_MSE]  μ = 478.0143 | σ = 5.2618\n",
      "[     G0_DECISION_OBJ]  μ = 16.7965 | σ = 0.0088\n",
      "[     G1_DECISION_OBJ]  μ = 55.1737 | σ = 0.0147\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 367.6161 | Regret 0.2828 | Fair-Val 86.3052\n",
      "Epoch 010/50 | Train-Loss 36.1058 | Test-MSE 348.5959 | Regret 0.1768 | Fair-Val 75.7763\n",
      "Epoch 020/50 | Train-Loss 22.8421 | Test-MSE 342.8874 | Regret 0.1330 | Fair-Val 73.9689\n",
      "Epoch 030/50 | Train-Loss 13.3280 | Test-MSE 344.2250 | Regret 0.0888 | Fair-Val 77.9444\n",
      "Epoch 040/50 | Train-Loss 8.1164 | Test-MSE 343.2566 | Regret 0.0647 | Fair-Val 79.3913\n",
      "Epoch 050/50 | Train-Loss 6.3966 | Test-MSE 341.4930 | Regret 0.0534 | Fair-Val 79.5419\n",
      "Training finished in 98.42s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 367.9062 | Regret 0.2960 | Fair-Val 85.9982\n",
      "Epoch 010/50 | Train-Loss 38.3728 | Test-MSE 349.3184 | Regret 0.1872 | Fair-Val 75.1241\n",
      "Epoch 020/50 | Train-Loss 24.3640 | Test-MSE 342.3683 | Regret 0.1406 | Fair-Val 72.8827\n",
      "Epoch 030/50 | Train-Loss 14.9310 | Test-MSE 343.5956 | Regret 0.0960 | Fair-Val 76.9404\n",
      "Epoch 040/50 | Train-Loss 9.5231 | Test-MSE 343.5769 | Regret 0.0711 | Fair-Val 78.8945\n",
      "Epoch 050/50 | Train-Loss 6.7588 | Test-MSE 341.6943 | Regret 0.0567 | Fair-Val 79.4619\n",
      "Training finished in 98.76s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0550 | σ = 0.0017\n",
      "[                 MSE]  μ = 341.5937 | σ = 0.1006\n",
      "[            FAIRNESS]  μ = 79.5019 | σ = 0.0400\n",
      "[       TRAINING_TIME]  μ = 98.5866 | σ = 0.1698\n",
      "[              G0_MSE]  μ = 322.6404 | σ = 0.1102\n",
      "[              G1_MSE]  μ = 481.6442 | σ = 0.0302\n",
      "[     G0_DECISION_OBJ]  μ = 16.6246 | σ = 0.0153\n",
      "[     G1_DECISION_OBJ]  μ = 21.6179 | σ = 0.0321\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 130.9927 | Test-MSE 367.7638 | Regret 0.0581 | Fair-Val 86.3262\n",
      "Epoch 010/50 | Train-Loss 95.9175 | Test-MSE 339.4573 | Regret 0.0490 | Fair-Val 70.4997\n",
      "Epoch 020/50 | Train-Loss 55.4483 | Test-MSE 295.7133 | Regret 0.0561 | Fair-Val 46.7734\n",
      "Epoch 030/50 | Train-Loss 28.4425 | Test-MSE 251.4445 | Regret 0.0567 | Fair-Val 29.9030\n",
      "Epoch 040/50 | Train-Loss 18.1549 | Test-MSE 234.8401 | Regret 0.0456 | Fair-Val 29.3916\n",
      "Epoch 050/50 | Train-Loss 16.6254 | Test-MSE 223.2457 | Regret 0.0397 | Fair-Val 29.6944\n",
      "Training finished in 82.21s.\n",
      "Epoch 001/50 | Train-Loss 131.3402 | Test-MSE 368.0721 | Regret 0.0598 | Fair-Val 85.9823\n",
      "Epoch 010/50 | Train-Loss 94.1046 | Test-MSE 338.4949 | Regret 0.0517 | Fair-Val 68.9302\n",
      "Epoch 020/50 | Train-Loss 53.4954 | Test-MSE 293.6122 | Regret 0.0590 | Fair-Val 45.2340\n",
      "Epoch 030/50 | Train-Loss 26.0933 | Test-MSE 252.3548 | Regret 0.0568 | Fair-Val 29.8323\n",
      "Epoch 040/50 | Train-Loss 18.5109 | Test-MSE 238.7287 | Regret 0.0451 | Fair-Val 29.8383\n",
      "Epoch 050/50 | Train-Loss 15.8025 | Test-MSE 228.2982 | Regret 0.0391 | Fair-Val 29.8342\n",
      "Training finished in 82.83s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0394 | σ = 0.0003\n",
      "[                 MSE]  μ = 225.7719 | σ = 2.5263\n",
      "[            FAIRNESS]  μ = 29.7643 | σ = 0.0699\n",
      "[       TRAINING_TIME]  μ = 82.5194 | σ = 0.3130\n",
      "[              G0_MSE]  μ = 218.6761 | σ = 2.5096\n",
      "[              G1_MSE]  μ = 278.2047 | σ = 2.6493\n",
      "[     G0_DECISION_OBJ]  μ = 34.1190 | σ = 0.1740\n",
      "[     G1_DECISION_OBJ]  μ = 249.6451 | σ = 0.0148\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 101.9354 | Test-MSE 368.1804 | Regret 0.0011 | Fair-Val 86.3483\n",
      "Epoch 010/50 | Train-Loss 72.7712 | Test-MSE 342.9976 | Regret 0.0011 | Fair-Val 70.4812\n",
      "Epoch 020/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 030/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 040/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 050/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Training finished in 130.91s.\n",
      "Epoch 001/50 | Train-Loss 101.8921 | Test-MSE 368.6722 | Regret 0.0012 | Fair-Val 86.0446\n",
      "Epoch 010/50 | Train-Loss 71.3687 | Test-MSE 342.4066 | Regret 0.0011 | Fair-Val 69.1542\n",
      "Epoch 020/50 | Train-Loss 21.1789 | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 030/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 040/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Epoch 050/50 | Train-Loss nan | Test-MSE nan | Regret nan | Fair-Val nan\n",
      "Training finished in 131.33s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = nan | σ = nan\n",
      "[                 MSE]  μ = nan | σ = nan\n",
      "[            FAIRNESS]  μ = nan | σ = nan\n",
      "[       TRAINING_TIME]  μ = 131.1208 | σ = 0.2122\n",
      "[              G0_MSE]  μ = nan | σ = nan\n",
      "[              G1_MSE]  μ = nan | σ = nan\n",
      "[     G0_DECISION_OBJ]  μ = nan | σ = nan\n",
      "[     G1_DECISION_OBJ]  μ = nan | σ = nan\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 104.4521 | Test-MSE 367.9886 | Regret 0.0245 | Fair-Val 86.3246\n",
      "Epoch 010/50 | Train-Loss 74.4892 | Test-MSE 340.7816 | Regret 0.0201 | Fair-Val 70.1572\n",
      "Epoch 020/50 | Train-Loss 25.3406 | Test-MSE 295.6659 | Regret 0.0296 | Fair-Val 45.1725\n",
      "Epoch 030/50 | Train-Loss 10.2696 | Test-MSE 264.4565 | Regret 0.0342 | Fair-Val 34.0614\n",
      "Epoch 040/50 | Train-Loss 4.1579 | Test-MSE 253.9679 | Regret 0.0270 | Fair-Val 33.3138\n",
      "Epoch 050/50 | Train-Loss 3.1547 | Test-MSE 248.9847 | Regret 0.0259 | Fair-Val 32.6463\n",
      "Training finished in 143.00s.\n",
      "Epoch 001/50 | Train-Loss 104.5352 | Test-MSE 368.4889 | Regret 0.0254 | Fair-Val 86.0128\n",
      "Epoch 010/50 | Train-Loss 73.0023 | Test-MSE 340.2529 | Regret 0.0210 | Fair-Val 68.7618\n",
      "Epoch 020/50 | Train-Loss 24.1307 | Test-MSE 293.6083 | Regret 0.0299 | Fair-Val 43.8836\n",
      "Epoch 030/50 | Train-Loss 8.8699 | Test-MSE 262.8234 | Regret 0.0341 | Fair-Val 33.5538\n",
      "Epoch 040/50 | Train-Loss 4.0735 | Test-MSE 254.6048 | Regret 0.0267 | Fair-Val 32.4458\n",
      "Epoch 050/50 | Train-Loss 3.6381 | Test-MSE 247.4165 | Regret 0.0261 | Fair-Val 31.4658\n",
      "Training finished in 142.66s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0260 | σ = 0.0001\n",
      "[                 MSE]  μ = 248.2006 | σ = 0.7841\n",
      "[            FAIRNESS]  μ = 32.0561 | σ = 0.5903\n",
      "[       TRAINING_TIME]  μ = 142.8273 | σ = 0.1701\n",
      "[              G0_MSE]  μ = 240.5585 | σ = 0.6433\n",
      "[              G1_MSE]  μ = 304.6706 | σ = 1.8239\n",
      "[     G0_DECISION_OBJ]  μ = 18.1645 | σ = 0.0098\n",
      "[     G1_DECISION_OBJ]  μ = 60.6257 | σ = 0.4919\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 176.4621 | Test-MSE 367.6067 | Regret 0.2830 | Fair-Val 86.2741\n",
      "Epoch 010/50 | Train-Loss 117.9542 | Test-MSE 340.7083 | Regret 0.1890 | Fair-Val 70.8993\n",
      "Epoch 020/50 | Train-Loss 76.5605 | Test-MSE 303.8043 | Regret 0.1831 | Fair-Val 49.5490\n",
      "Epoch 030/50 | Train-Loss 40.4634 | Test-MSE 266.6754 | Regret 0.1770 | Fair-Val 32.1408\n",
      "Epoch 040/50 | Train-Loss 25.7621 | Test-MSE 244.4801 | Regret 0.1505 | Fair-Val 26.0794\n",
      "Epoch 050/50 | Train-Loss 21.0838 | Test-MSE 235.9679 | Regret 0.1264 | Fair-Val 26.3700\n",
      "Training finished in 100.65s.\n",
      "Epoch 001/50 | Train-Loss 180.8339 | Test-MSE 367.9033 | Regret 0.2959 | Fair-Val 85.9693\n",
      "Epoch 010/50 | Train-Loss 117.9508 | Test-MSE 340.2245 | Regret 0.2018 | Fair-Val 69.4319\n",
      "Epoch 020/50 | Train-Loss 75.2802 | Test-MSE 303.1755 | Regret 0.1955 | Fair-Val 47.6347\n",
      "Epoch 030/50 | Train-Loss 42.6764 | Test-MSE 266.2580 | Regret 0.1931 | Fair-Val 30.7018\n",
      "Epoch 040/50 | Train-Loss 28.5131 | Test-MSE 246.1366 | Regret 0.1701 | Fair-Val 25.1856\n",
      "Epoch 050/50 | Train-Loss 21.9967 | Test-MSE 239.5001 | Regret 0.1334 | Fair-Val 26.4092\n",
      "Training finished in 100.07s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.1299 | σ = 0.0035\n",
      "[                 MSE]  μ = 237.7340 | σ = 1.7661\n",
      "[            FAIRNESS]  μ = 26.3896 | σ = 0.0196\n",
      "[       TRAINING_TIME]  μ = 100.3624 | σ = 0.2918\n",
      "[              G0_MSE]  μ = 231.4427 | σ = 1.7614\n",
      "[              G1_MSE]  μ = 284.2220 | σ = 1.8006\n",
      "[     G0_DECISION_OBJ]  μ = 17.4487 | σ = 0.0153\n",
      "[     G1_DECISION_OBJ]  μ = 22.6069 | σ = 1.5404\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 29.1420 | Test-MSE 367.6678 | Regret 0.0577 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 16.7765 | Test-MSE 351.9297 | Regret 0.0400 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 11.3554 | Test-MSE 348.4236 | Regret 0.0301 | Fair-Val 0.0046\n",
      "Epoch 030/50 | Train-Loss 5.8094 | Test-MSE 343.9332 | Regret 0.0184 | Fair-Val 0.0049\n",
      "Epoch 040/50 | Train-Loss 2.4959 | Test-MSE 339.1479 | Regret 0.0111 | Fair-Val 0.0052\n",
      "Epoch 050/50 | Train-Loss 1.8876 | Test-MSE 333.6214 | Regret 0.0090 | Fair-Val 0.0053\n",
      "Training finished in 81.91s.\n",
      "Epoch 001/50 | Train-Loss 29.5348 | Test-MSE 368.0066 | Regret 0.0596 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 17.4128 | Test-MSE 351.2632 | Regret 0.0426 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 11.8794 | Test-MSE 347.2107 | Regret 0.0319 | Fair-Val 0.0045\n",
      "Epoch 030/50 | Train-Loss 6.5969 | Test-MSE 344.3484 | Regret 0.0204 | Fair-Val 0.0048\n",
      "Epoch 040/50 | Train-Loss 2.7389 | Test-MSE 342.4343 | Regret 0.0113 | Fair-Val 0.0051\n",
      "Epoch 050/50 | Train-Loss 1.9359 | Test-MSE 338.2726 | Regret 0.0090 | Fair-Val 0.0052\n",
      "Training finished in 82.50s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0090 | σ = 0.0000\n",
      "[                 MSE]  μ = 335.9470 | σ = 2.3256\n",
      "[            FAIRNESS]  μ = 0.0053 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 82.2052 | σ = 0.2926\n",
      "[              G0_MSE]  μ = 316.5173 | σ = 2.2561\n",
      "[              G1_MSE]  μ = 479.5184 | σ = 2.8394\n",
      "[     G0_DECISION_OBJ]  μ = 45.6543 | σ = 0.1977\n",
      "[     G1_DECISION_OBJ]  μ = 262.4419 | σ = 3.0586\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 0.0847 | Test-MSE 367.6982 | Regret 0.0011 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 0.0512 | Test-MSE 345.2933 | Regret 0.0008 | Fair-Val 0.0044\n",
      "Epoch 020/50 | Train-Loss 0.0383 | Test-MSE 336.2067 | Regret 0.0006 | Fair-Val 0.0041\n",
      "Epoch 030/50 | Train-Loss 0.0278 | Test-MSE 343.0694 | Regret 0.0004 | Fair-Val 0.0042\n",
      "Epoch 040/50 | Train-Loss 0.0190 | Test-MSE 344.5851 | Regret 0.0003 | Fair-Val 0.0044\n",
      "Epoch 050/50 | Train-Loss 0.0136 | Test-MSE 337.4286 | Regret 0.0002 | Fair-Val 0.0045\n",
      "Training finished in 142.93s.\n",
      "Epoch 001/50 | Train-Loss 0.0867 | Test-MSE 369.5273 | Regret 0.0011 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 0.0532 | Test-MSE 355.7396 | Regret 0.0008 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.0373 | Test-MSE 356.9369 | Regret 0.0006 | Fair-Val 0.0044\n",
      "Epoch 030/50 | Train-Loss 0.0251 | Test-MSE 356.7030 | Regret 0.0004 | Fair-Val 0.0044\n",
      "Epoch 040/50 | Train-Loss 0.0162 | Test-MSE 357.1693 | Regret 0.0003 | Fair-Val 0.0046\n",
      "Epoch 050/50 | Train-Loss 0.0123 | Test-MSE 354.1853 | Regret 0.0002 | Fair-Val 0.0047\n",
      "Training finished in 141.15s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0002 | σ = 0.0000\n",
      "[                 MSE]  μ = 345.8069 | σ = 8.3784\n",
      "[            FAIRNESS]  μ = 0.0046 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 142.0392 | σ = 0.8862\n",
      "[              G0_MSE]  μ = 327.2650 | σ = 7.6934\n",
      "[              G1_MSE]  μ = 482.8182 | σ = 13.4402\n",
      "[     G0_DECISION_OBJ]  μ = 20.9205 | σ = 0.1051\n",
      "[     G1_DECISION_OBJ]  μ = 170.8172 | σ = 0.0211\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 2.6015 | Test-MSE 367.6061 | Regret 0.0235 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 1.3313 | Test-MSE 348.8250 | Regret 0.0147 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 0.8595 | Test-MSE 341.8218 | Regret 0.0105 | Fair-Val 0.0043\n",
      "Epoch 030/50 | Train-Loss 0.4891 | Test-MSE 341.0576 | Regret 0.0067 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 0.2864 | Test-MSE 337.8073 | Regret 0.0046 | Fair-Val 0.0049\n",
      "Epoch 050/50 | Train-Loss 0.2365 | Test-MSE 336.8123 | Regret 0.0039 | Fair-Val 0.0050\n",
      "Training finished in 142.88s.\n",
      "Epoch 001/50 | Train-Loss 2.7299 | Test-MSE 368.1580 | Regret 0.0244 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 1.4126 | Test-MSE 348.9287 | Regret 0.0157 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 0.9040 | Test-MSE 340.2602 | Regret 0.0111 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 0.5489 | Test-MSE 338.6064 | Regret 0.0073 | Fair-Val 0.0046\n",
      "Epoch 040/50 | Train-Loss 0.3380 | Test-MSE 340.9167 | Regret 0.0051 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 0.2476 | Test-MSE 341.8866 | Regret 0.0041 | Fair-Val 0.0050\n",
      "Training finished in 142.96s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0040 | σ = 0.0001\n",
      "[                 MSE]  μ = 339.3494 | σ = 2.5371\n",
      "[            FAIRNESS]  μ = 0.0050 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 142.9198 | σ = 0.0375\n",
      "[              G0_MSE]  μ = 320.2543 | σ = 2.4771\n",
      "[              G1_MSE]  μ = 480.4486 | σ = 2.9810\n",
      "[     G0_DECISION_OBJ]  μ = 16.8243 | σ = 0.0112\n",
      "[     G1_DECISION_OBJ]  μ = 55.1938 | σ = 0.1253\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': True, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6115 | Test-MSE 367.6161 | Regret 0.2828 | Fair-Val 0.0050\n",
      "Epoch 010/50 | Train-Loss 36.1008 | Test-MSE 349.4561 | Regret 0.1767 | Fair-Val 0.0043\n",
      "Epoch 020/50 | Train-Loss 22.7728 | Test-MSE 343.4800 | Regret 0.1330 | Fair-Val 0.0042\n",
      "Epoch 030/50 | Train-Loss 13.2802 | Test-MSE 345.7516 | Regret 0.0876 | Fair-Val 0.0047\n",
      "Epoch 040/50 | Train-Loss 8.0597 | Test-MSE 344.2115 | Regret 0.0641 | Fair-Val 0.0048\n",
      "Epoch 050/50 | Train-Loss 6.4612 | Test-MSE 342.6795 | Regret 0.0527 | Fair-Val 0.0049\n",
      "Training finished in 98.19s.\n",
      "Epoch 001/50 | Train-Loss 79.0285 | Test-MSE 367.9062 | Regret 0.2960 | Fair-Val 0.0049\n",
      "Epoch 010/50 | Train-Loss 38.1091 | Test-MSE 350.9059 | Regret 0.1852 | Fair-Val 0.0042\n",
      "Epoch 020/50 | Train-Loss 24.2324 | Test-MSE 342.7487 | Regret 0.1400 | Fair-Val 0.0041\n",
      "Epoch 030/50 | Train-Loss 14.9497 | Test-MSE 341.7292 | Regret 0.0967 | Fair-Val 0.0045\n",
      "Epoch 040/50 | Train-Loss 9.5040 | Test-MSE 341.8304 | Regret 0.0709 | Fair-Val 0.0047\n",
      "Epoch 050/50 | Train-Loss 6.7587 | Test-MSE 342.1439 | Regret 0.0569 | Fair-Val 0.0049\n",
      "Training finished in 98.80s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0548 | σ = 0.0021\n",
      "[                 MSE]  μ = 342.4117 | σ = 0.2678\n",
      "[            FAIRNESS]  μ = 0.0049 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 98.4964 | σ = 0.3056\n",
      "[              G0_MSE]  μ = 323.4492 | σ = 0.2345\n",
      "[              G1_MSE]  μ = 482.5302 | σ = 0.5146\n",
      "[     G0_DECISION_OBJ]  μ = 16.6304 | σ = 0.0141\n",
      "[     G1_DECISION_OBJ]  μ = 21.5779 | σ = 0.0324\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3415.9688 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 507.7022\n",
      "Epoch 010/50 | Train-Loss 2802.0605 | Test-MSE 374.4723 | Regret 0.1120 | Fair-Val 511.0275\n",
      "Epoch 020/50 | Train-Loss 4189.1445 | Test-MSE 382.4344 | Regret 0.1789 | Fair-Val 519.9187\n",
      "Epoch 030/50 | Train-Loss 6786.4160 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 523.4739\n",
      "Epoch 040/50 | Train-Loss 9089.9043 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 524.5817\n",
      "Epoch 050/50 | Train-Loss 7128.9688 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 524.8821\n",
      "Training finished in 2.54s.\n",
      "Epoch 001/50 | Train-Loss 3512.8027 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 508.3664\n",
      "Epoch 010/50 | Train-Loss 2822.3457 | Test-MSE 374.7670 | Regret 0.1102 | Fair-Val 511.2897\n",
      "Epoch 020/50 | Train-Loss 4341.7910 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 519.7877\n",
      "Epoch 030/50 | Train-Loss 6752.1914 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 523.4116\n",
      "Epoch 040/50 | Train-Loss 9232.6562 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 524.5447\n",
      "Epoch 050/50 | Train-Loss 7382.6035 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 524.8646\n",
      "Training finished in 2.53s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 524.8734 | σ = 0.0088\n",
      "[       TRAINING_TIME]  μ = 2.5357 | σ = 0.0050\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0133\n",
      "[     G0_DECISION_OBJ]  μ = 27.0203 | σ = 4.7743\n",
      "[     G1_DECISION_OBJ]  μ = 379.4148 | σ = 193.2612\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 143.6367 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 507.6813\n",
      "Epoch 010/50 | Train-Loss 110.6230 | Test-MSE 372.6393 | Regret 0.0052 | Fair-Val 508.6251\n",
      "Epoch 020/50 | Train-Loss 331.1816 | Test-MSE 380.4272 | Regret 0.0177 | Fair-Val 517.4680\n",
      "Epoch 030/50 | Train-Loss 607.3633 | Test-MSE 384.6749 | Regret 0.0288 | Fair-Val 522.4727\n",
      "Epoch 040/50 | Train-Loss 506.2715 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 524.0574\n",
      "Epoch 050/50 | Train-Loss 315.1445 | Test-MSE 386.4113 | Regret 0.0149 | Fair-Val 524.6382\n",
      "Training finished in 2.54s.\n",
      "Epoch 001/50 | Train-Loss 149.1523 | Test-MSE 370.2304 | Regret 0.0060 | Fair-Val 508.3608\n",
      "Epoch 010/50 | Train-Loss 109.8672 | Test-MSE 373.4153 | Regret 0.0050 | Fair-Val 509.4874\n",
      "Epoch 020/50 | Train-Loss 286.6328 | Test-MSE 380.8755 | Regret 0.0153 | Fair-Val 517.9122\n",
      "Epoch 030/50 | Train-Loss 593.2305 | Test-MSE 384.7013 | Regret 0.0285 | Fair-Val 522.4958\n",
      "Epoch 040/50 | Train-Loss 487.1230 | Test-MSE 386.0291 | Regret 0.0232 | Fair-Val 524.1398\n",
      "Epoch 050/50 | Train-Loss 295.9727 | Test-MSE 386.4234 | Regret 0.0142 | Fair-Val 524.6615\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0145 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4174 | σ = 0.0060\n",
      "[            FAIRNESS]  μ = 524.6499 | σ = 0.0116\n",
      "[       TRAINING_TIME]  μ = 2.5588 | σ = 0.0197\n",
      "[              G0_MSE]  μ = 364.9638 | σ = 0.0024\n",
      "[              G1_MSE]  μ = 544.9433 | σ = 0.0320\n",
      "[     G0_DECISION_OBJ]  μ = 27.6426 | σ = 0.0480\n",
      "[     G1_DECISION_OBJ]  μ = 51.1901 | σ = 1.5952\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 78.9233 | Test-MSE 369.5992 | Regret 0.0590 | Fair-Val 508.4144\n",
      "Epoch 010/50 | Train-Loss 115.6484 | Test-MSE 357.3958 | Regret 0.0845 | Fair-Val 497.2656\n",
      "Epoch 020/50 | Train-Loss 130.9624 | Test-MSE 333.0449 | Regret 0.0959 | Fair-Val 465.8534\n",
      "Epoch 030/50 | Train-Loss 136.2502 | Test-MSE 311.3412 | Regret 0.0988 | Fair-Val 419.3470\n",
      "Epoch 040/50 | Train-Loss 135.2443 | Test-MSE 304.6464 | Regret 0.0983 | Fair-Val 361.2239\n",
      "Epoch 050/50 | Train-Loss 134.1367 | Test-MSE 324.1054 | Regret 0.0978 | Fair-Val 307.8857\n",
      "Training finished in 2.64s.\n",
      "Epoch 001/50 | Train-Loss 82.9224 | Test-MSE 369.9268 | Regret 0.0602 | Fair-Val 508.8384\n",
      "Epoch 010/50 | Train-Loss 109.3344 | Test-MSE 357.8008 | Regret 0.0786 | Fair-Val 497.8892\n",
      "Epoch 020/50 | Train-Loss 122.1057 | Test-MSE 331.0451 | Regret 0.0892 | Fair-Val 466.2614\n",
      "Epoch 030/50 | Train-Loss 129.8397 | Test-MSE 304.5727 | Regret 0.0941 | Fair-Val 418.4245\n",
      "Epoch 040/50 | Train-Loss 132.2141 | Test-MSE 292.1766 | Regret 0.0952 | Fair-Val 360.0242\n",
      "Epoch 050/50 | Train-Loss 131.9357 | Test-MSE 303.5035 | Regret 0.0951 | Fair-Val 297.4414\n",
      "Training finished in 2.56s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0964 | σ = 0.0013\n",
      "[                 MSE]  μ = 313.8045 | σ = 10.3010\n",
      "[            FAIRNESS]  μ = 302.6636 | σ = 5.2222\n",
      "[       TRAINING_TIME]  μ = 2.6039 | σ = 0.0393\n",
      "[              G0_MSE]  μ = 302.4423 | σ = 10.3903\n",
      "[              G1_MSE]  μ = 397.7625 | σ = 9.6407\n",
      "[     G0_DECISION_OBJ]  μ = 21.0484 | σ = 0.0193\n",
      "[     G1_DECISION_OBJ]  μ = 33.8340 | σ = 0.2519\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 368.9831 | Regret 0.3398 | Fair-Val 507.6001\n",
      "Epoch 010/50 | Train-Loss 87.1204 | Test-MSE 352.9514 | Regret 0.4233 | Fair-Val 491.5408\n",
      "Epoch 020/50 | Train-Loss 101.3864 | Test-MSE 323.6245 | Regret 0.4992 | Fair-Val 456.4656\n",
      "Epoch 030/50 | Train-Loss 103.7902 | Test-MSE 291.9113 | Regret 0.4985 | Fair-Val 405.1118\n",
      "Epoch 040/50 | Train-Loss 100.7530 | Test-MSE 268.1656 | Regret 0.4841 | Fair-Val 339.9048\n",
      "Epoch 050/50 | Train-Loss 99.3065 | Test-MSE 264.6647 | Regret 0.4766 | Fair-Val 267.5626\n",
      "Training finished in 2.48s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 369.2369 | Regret 0.3472 | Fair-Val 507.9189\n",
      "Epoch 010/50 | Train-Loss 82.6439 | Test-MSE 352.2357 | Regret 0.3927 | Fair-Val 490.3625\n",
      "Epoch 020/50 | Train-Loss 94.3698 | Test-MSE 320.1707 | Regret 0.4697 | Fair-Val 452.9274\n",
      "Epoch 030/50 | Train-Loss 102.0180 | Test-MSE 286.2388 | Regret 0.4962 | Fair-Val 399.3138\n",
      "Epoch 040/50 | Train-Loss 103.0727 | Test-MSE 262.2052 | Regret 0.4905 | Fair-Val 333.0533\n",
      "Epoch 050/50 | Train-Loss 100.8569 | Test-MSE 258.7193 | Regret 0.4781 | Fair-Val 262.3913\n",
      "Training finished in 2.50s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4773 | σ = 0.0007\n",
      "[                 MSE]  μ = 261.6920 | σ = 2.9727\n",
      "[            FAIRNESS]  μ = 264.9769 | σ = 2.5857\n",
      "[       TRAINING_TIME]  μ = 2.4918 | σ = 0.0082\n",
      "[              G0_MSE]  μ = 250.8123 | σ = 2.9817\n",
      "[              G1_MSE]  μ = 342.0845 | σ = 2.9063\n",
      "[     G0_DECISION_OBJ]  μ = 18.3818 | σ = 0.0082\n",
      "[     G1_DECISION_OBJ]  μ = 33.4390 | σ = 1.3831\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3953.8799 | Test-MSE 369.6822 | Regret 0.1205 | Fair-Val 507.7017\n",
      "Epoch 010/50 | Train-Loss 3341.0195 | Test-MSE 374.4593 | Regret 0.1120 | Fair-Val 511.0104\n",
      "Epoch 020/50 | Train-Loss 4739.0889 | Test-MSE 382.4295 | Regret 0.1789 | Fair-Val 519.9126\n",
      "Epoch 030/50 | Train-Loss 7344.4829 | Test-MSE 385.5072 | Regret 0.2948 | Fair-Val 523.4715\n",
      "Epoch 040/50 | Train-Loss 9647.7998 | Test-MSE 386.3705 | Regret 0.3913 | Fair-Val 524.5809\n",
      "Epoch 050/50 | Train-Loss 7680.3735 | Test-MSE 386.5898 | Regret 0.3329 | Fair-Val 524.8821\n",
      "Training finished in 2.62s.\n",
      "Epoch 001/50 | Train-Loss 4051.9204 | Test-MSE 370.2293 | Regret 0.1241 | Fair-Val 508.3653\n",
      "Epoch 010/50 | Train-Loss 3362.4014 | Test-MSE 374.7551 | Regret 0.1102 | Fair-Val 511.2748\n",
      "Epoch 020/50 | Train-Loss 4892.9854 | Test-MSE 382.3448 | Regret 0.1821 | Fair-Val 519.7814\n",
      "Epoch 030/50 | Train-Loss 7302.3428 | Test-MSE 385.4475 | Regret 0.2881 | Fair-Val 523.4105\n",
      "Epoch 040/50 | Train-Loss 9775.2217 | Test-MSE 386.3373 | Regret 0.3991 | Fair-Val 524.5442\n",
      "Epoch 050/50 | Train-Loss 7950.4463 | Test-MSE 386.5736 | Regret 0.4108 | Fair-Val 524.8646\n",
      "Training finished in 2.54s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3718 | σ = 0.0390\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 524.8734 | σ = 0.0087\n",
      "[       TRAINING_TIME]  μ = 2.5770 | σ = 0.0418\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0074\n",
      "[              G1_MSE]  μ = 545.3247 | σ = 0.0124\n",
      "[     G0_DECISION_OBJ]  μ = 27.0095 | σ = 4.6895\n",
      "[     G1_DECISION_OBJ]  μ = 377.6530 | σ = 190.0825\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 681.5478 | Test-MSE 369.6608 | Regret 0.0058 | Fair-Val 507.6636\n",
      "Epoch 010/50 | Train-Loss 646.9650 | Test-MSE 372.3972 | Regret 0.0053 | Fair-Val 508.3057\n",
      "Epoch 020/50 | Train-Loss 880.3593 | Test-MSE 380.2114 | Regret 0.0178 | Fair-Val 517.1793\n",
      "Epoch 030/50 | Train-Loss 1162.6594 | Test-MSE 384.6342 | Regret 0.0289 | Fair-Val 522.4189\n",
      "Epoch 040/50 | Train-Loss 1061.6234 | Test-MSE 385.9742 | Regret 0.0236 | Fair-Val 524.0582\n",
      "Epoch 050/50 | Train-Loss 870.0930 | Test-MSE 386.4202 | Regret 0.0148 | Fair-Val 524.6514\n",
      "Training finished in 2.58s.\n",
      "Epoch 001/50 | Train-Loss 688.2701 | Test-MSE 370.2173 | Regret 0.0060 | Fair-Val 508.3449\n",
      "Epoch 010/50 | Train-Loss 647.4316 | Test-MSE 373.1810 | Regret 0.0050 | Fair-Val 509.1758\n",
      "Epoch 020/50 | Train-Loss 832.0442 | Test-MSE 380.6599 | Regret 0.0151 | Fair-Val 517.6177\n",
      "Epoch 030/50 | Train-Loss 1141.6689 | Test-MSE 384.6658 | Regret 0.0284 | Fair-Val 522.4476\n",
      "Epoch 040/50 | Train-Loss 1037.2656 | Test-MSE 386.0438 | Regret 0.0229 | Fair-Val 524.1604\n",
      "Epoch 050/50 | Train-Loss 847.7223 | Test-MSE 386.4308 | Regret 0.0139 | Fair-Val 524.6727\n",
      "Training finished in 2.55s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0144 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4255 | σ = 0.0053\n",
      "[            FAIRNESS]  μ = 524.6620 | σ = 0.0106\n",
      "[       TRAINING_TIME]  μ = 2.5638 | σ = 0.0138\n",
      "[              G0_MSE]  μ = 364.9718 | σ = 0.0040\n",
      "[              G1_MSE]  μ = 544.9522 | σ = 0.0144\n",
      "[     G0_DECISION_OBJ]  μ = 27.6496 | σ = 0.0384\n",
      "[     G1_DECISION_OBJ]  μ = 51.2027 | σ = 1.6496\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 616.8344 | Test-MSE 369.4432 | Regret 0.0583 | Fair-Val 508.2133\n",
      "Epoch 010/50 | Train-Loss 633.2652 | Test-MSE 356.1486 | Regret 0.0795 | Fair-Val 495.6698\n",
      "Epoch 020/50 | Train-Loss 603.9327 | Test-MSE 327.3419 | Regret 0.0877 | Fair-Val 458.4466\n",
      "Epoch 030/50 | Train-Loss 530.8755 | Test-MSE 293.9179 | Regret 0.0853 | Fair-Val 394.7360\n",
      "Epoch 040/50 | Train-Loss 418.4904 | Test-MSE 268.4575 | Regret 0.0790 | Fair-Val 302.5624\n",
      "Epoch 050/50 | Train-Loss 331.0372 | Test-MSE 269.5360 | Regret 0.0726 | Fair-Val 237.2384\n",
      "Training finished in 2.53s.\n",
      "Epoch 001/50 | Train-Loss 622.0402 | Test-MSE 369.7164 | Regret 0.0593 | Fair-Val 508.5608\n",
      "Epoch 010/50 | Train-Loss 624.4081 | Test-MSE 355.4624 | Regret 0.0719 | Fair-Val 494.7910\n",
      "Epoch 020/50 | Train-Loss 588.5978 | Test-MSE 322.1503 | Regret 0.0793 | Fair-Val 454.4706\n",
      "Epoch 030/50 | Train-Loss 516.7760 | Test-MSE 282.4205 | Regret 0.0795 | Fair-Val 387.1905\n",
      "Epoch 040/50 | Train-Loss 409.2392 | Test-MSE 252.4633 | Regret 0.0756 | Fair-Val 296.3835\n",
      "Epoch 050/50 | Train-Loss 317.3625 | Test-MSE 255.3889 | Regret 0.0712 | Fair-Val 219.0571\n",
      "Training finished in 2.62s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0719 | σ = 0.0007\n",
      "[                 MSE]  μ = 262.4625 | σ = 7.0735\n",
      "[            FAIRNESS]  μ = 228.1478 | σ = 9.0906\n",
      "[       TRAINING_TIME]  μ = 2.5781 | σ = 0.0445\n",
      "[              G0_MSE]  μ = 255.9634 | σ = 7.6713\n",
      "[              G1_MSE]  μ = 310.4863 | σ = 2.6567\n",
      "[     G0_DECISION_OBJ]  μ = 21.1076 | σ = 0.0210\n",
      "[     G1_DECISION_OBJ]  μ = 32.4676 | σ = 0.1848\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 612.5168 | Test-MSE 368.4211 | Regret 0.3254 | Fair-Val 506.7992\n",
      "Epoch 010/50 | Train-Loss 577.2137 | Test-MSE 345.8217 | Regret 0.3335 | Fair-Val 481.2682\n",
      "Epoch 020/50 | Train-Loss 517.9517 | Test-MSE 299.8217 | Regret 0.3698 | Fair-Val 423.6961\n",
      "Epoch 030/50 | Train-Loss 415.9778 | Test-MSE 240.7453 | Regret 0.3515 | Fair-Val 334.0703\n",
      "Epoch 040/50 | Train-Loss 295.8572 | Test-MSE 192.9868 | Regret 0.3337 | Fair-Val 226.6295\n",
      "Epoch 050/50 | Train-Loss 218.8607 | Test-MSE 188.8353 | Regret 0.3288 | Fair-Val 155.0357\n",
      "Training finished in 2.55s.\n",
      "Epoch 001/50 | Train-Loss 618.1407 | Test-MSE 368.7325 | Regret 0.3342 | Fair-Val 507.1826\n",
      "Epoch 010/50 | Train-Loss 573.1943 | Test-MSE 344.2766 | Regret 0.3166 | Fair-Val 478.9482\n",
      "Epoch 020/50 | Train-Loss 503.0676 | Test-MSE 292.8324 | Regret 0.3491 | Fair-Val 414.9620\n",
      "Epoch 030/50 | Train-Loss 402.5359 | Test-MSE 230.6384 | Regret 0.3449 | Fair-Val 321.1063\n",
      "Epoch 040/50 | Train-Loss 293.7167 | Test-MSE 188.7059 | Regret 0.3299 | Fair-Val 221.5115\n",
      "Epoch 050/50 | Train-Loss 227.1893 | Test-MSE 192.2306 | Regret 0.3248 | Fair-Val 159.5770\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3268 | σ = 0.0020\n",
      "[                 MSE]  μ = 190.5330 | σ = 1.6976\n",
      "[            FAIRNESS]  μ = 157.3063 | σ = 2.2706\n",
      "[       TRAINING_TIME]  μ = 2.5676 | σ = 0.0169\n",
      "[              G0_MSE]  μ = 184.9535 | σ = 0.9648\n",
      "[              G1_MSE]  μ = 231.7614 | σ = 7.1128\n",
      "[     G0_DECISION_OBJ]  μ = 18.3872 | σ = 0.0628\n",
      "[     G1_DECISION_OBJ]  μ = 32.4145 | σ = 2.3101\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3416.5906 | Test-MSE 369.6828 | Regret 0.1205 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2802.6633 | Test-MSE 374.4723 | Regret 0.1120 | Fair-Val 0.6054\n",
      "Epoch 020/50 | Train-Loss 4189.7432 | Test-MSE 382.4344 | Regret 0.1789 | Fair-Val 0.5995\n",
      "Epoch 030/50 | Train-Loss 6787.0146 | Test-MSE 385.5090 | Regret 0.2947 | Fair-Val 0.5979\n",
      "Epoch 040/50 | Train-Loss 9090.5029 | Test-MSE 386.3711 | Regret 0.3913 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7129.5654 | Test-MSE 386.5898 | Regret 0.3323 | Fair-Val 0.5977\n",
      "Training finished in 2.58s.\n",
      "Epoch 001/50 | Train-Loss 3513.4246 | Test-MSE 370.2302 | Regret 0.1241 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 2822.9480 | Test-MSE 374.7670 | Regret 0.1102 | Fair-Val 0.6049\n",
      "Epoch 020/50 | Train-Loss 4342.3892 | Test-MSE 382.3508 | Regret 0.1819 | Fair-Val 0.5993\n",
      "Epoch 030/50 | Train-Loss 6752.7881 | Test-MSE 385.4486 | Regret 0.2878 | Fair-Val 0.5978\n",
      "Epoch 040/50 | Train-Loss 9233.2529 | Test-MSE 386.3377 | Regret 0.3988 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 7383.2002 | Test-MSE 386.5737 | Regret 0.4111 | Fair-Val 0.5977\n",
      "Training finished in 2.58s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3717 | σ = 0.0394\n",
      "[                 MSE]  μ = 386.5817 | σ = 0.0081\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.5782 | σ = 0.0020\n",
      "[              G0_MSE]  μ = 365.0988 | σ = 0.0073\n",
      "[              G1_MSE]  μ = 545.3249 | σ = 0.0133\n",
      "[     G0_DECISION_OBJ]  μ = 27.0203 | σ = 4.7743\n",
      "[     G1_DECISION_OBJ]  μ = 379.4149 | σ = 193.2611\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 144.2585 | Test-MSE 369.6758 | Regret 0.0058 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 111.2293 | Test-MSE 372.6392 | Regret 0.0052 | Fair-Val 0.6051\n",
      "Epoch 020/50 | Train-Loss 331.7802 | Test-MSE 380.4272 | Regret 0.0177 | Fair-Val 0.5997\n",
      "Epoch 030/50 | Train-Loss 607.9603 | Test-MSE 384.6749 | Regret 0.0288 | Fair-Val 0.5981\n",
      "Epoch 040/50 | Train-Loss 506.8681 | Test-MSE 385.9738 | Regret 0.0237 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 315.7430 | Test-MSE 386.4113 | Regret 0.0149 | Fair-Val 0.5977\n",
      "Training finished in 2.61s.\n",
      "Epoch 001/50 | Train-Loss 149.7742 | Test-MSE 370.2304 | Regret 0.0060 | Fair-Val 0.6209\n",
      "Epoch 010/50 | Train-Loss 110.4711 | Test-MSE 373.4153 | Regret 0.0050 | Fair-Val 0.6046\n",
      "Epoch 020/50 | Train-Loss 287.2308 | Test-MSE 380.8755 | Regret 0.0153 | Fair-Val 0.5992\n",
      "Epoch 030/50 | Train-Loss 593.8292 | Test-MSE 384.7013 | Regret 0.0285 | Fair-Val 0.5980\n",
      "Epoch 040/50 | Train-Loss 487.7157 | Test-MSE 386.0291 | Regret 0.0232 | Fair-Val 0.5977\n",
      "Epoch 050/50 | Train-Loss 296.5731 | Test-MSE 386.4233 | Regret 0.0142 | Fair-Val 0.5977\n",
      "Training finished in 2.55s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0145 | σ = 0.0004\n",
      "[                 MSE]  μ = 386.4173 | σ = 0.0060\n",
      "[            FAIRNESS]  μ = 0.5977 | σ = 0.0000\n",
      "[       TRAINING_TIME]  μ = 2.5807 | σ = 0.0339\n",
      "[              G0_MSE]  μ = 364.9638 | σ = 0.0024\n",
      "[              G1_MSE]  μ = 544.9434 | σ = 0.0322\n",
      "[     G0_DECISION_OBJ]  μ = 27.6425 | σ = 0.0481\n",
      "[     G1_DECISION_OBJ]  μ = 51.1904 | σ = 1.5955\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 79.5451 | Test-MSE 369.5992 | Regret 0.0590 | Fair-Val 0.6266\n",
      "Epoch 010/50 | Train-Loss 116.3017 | Test-MSE 357.3955 | Regret 0.0845 | Fair-Val 0.6597\n",
      "Epoch 020/50 | Train-Loss 131.6116 | Test-MSE 333.0457 | Regret 0.0959 | Fair-Val 0.6437\n",
      "Epoch 030/50 | Train-Loss 136.8471 | Test-MSE 311.3495 | Regret 0.0988 | Fair-Val 0.5779\n",
      "Epoch 040/50 | Train-Loss 135.7517 | Test-MSE 304.6819 | Regret 0.0983 | Fair-Val 0.4760\n",
      "Epoch 050/50 | Train-Loss 134.5319 | Test-MSE 324.1862 | Regret 0.0978 | Fair-Val 0.3698\n",
      "Training finished in 2.55s.\n",
      "Epoch 001/50 | Train-Loss 83.5442 | Test-MSE 369.9268 | Regret 0.0602 | Fair-Val 0.6264\n",
      "Epoch 010/50 | Train-Loss 109.9849 | Test-MSE 357.8005 | Regret 0.0786 | Fair-Val 0.6604\n",
      "Epoch 020/50 | Train-Loss 122.7631 | Test-MSE 331.0451 | Regret 0.0892 | Fair-Val 0.6587\n",
      "Epoch 030/50 | Train-Loss 130.4441 | Test-MSE 304.5735 | Regret 0.0941 | Fair-Val 0.5959\n",
      "Epoch 040/50 | Train-Loss 132.7158 | Test-MSE 292.1863 | Regret 0.0953 | Fair-Val 0.4876\n",
      "Epoch 050/50 | Train-Loss 132.3121 | Test-MSE 303.5395 | Regret 0.0951 | Fair-Val 0.3700\n",
      "Training finished in 2.52s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0965 | σ = 0.0013\n",
      "[                 MSE]  μ = 313.8628 | σ = 10.3233\n",
      "[            FAIRNESS]  μ = 0.3699 | σ = 0.0001\n",
      "[       TRAINING_TIME]  μ = 2.5327 | σ = 0.0133\n",
      "[              G0_MSE]  μ = 302.5021 | σ = 10.4118\n",
      "[              G1_MSE]  μ = 397.8102 | σ = 9.6692\n",
      "[     G0_DECISION_OBJ]  μ = 21.0487 | σ = 0.0193\n",
      "[     G1_DECISION_OBJ]  μ = 33.8343 | σ = 0.2523\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'closed-form', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 75.2275 | Test-MSE 368.9831 | Regret 0.3398 | Fair-Val 0.6261\n",
      "Epoch 010/50 | Train-Loss 87.7622 | Test-MSE 352.9489 | Regret 0.4233 | Fair-Val 0.6596\n",
      "Epoch 020/50 | Train-Loss 102.0407 | Test-MSE 323.6200 | Regret 0.4992 | Fair-Val 0.6612\n",
      "Epoch 030/50 | Train-Loss 104.4179 | Test-MSE 291.9105 | Regret 0.4986 | Fair-Val 0.6104\n",
      "Epoch 040/50 | Train-Loss 101.2906 | Test-MSE 268.1856 | Regret 0.4842 | Fair-Val 0.5072\n",
      "Epoch 050/50 | Train-Loss 99.7093 | Test-MSE 264.7404 | Regret 0.4768 | Fair-Val 0.3769\n",
      "Training finished in 2.60s.\n",
      "Epoch 001/50 | Train-Loss 79.6448 | Test-MSE 369.2367 | Regret 0.3472 | Fair-Val 0.6259\n",
      "Epoch 010/50 | Train-Loss 83.2817 | Test-MSE 352.2339 | Regret 0.3926 | Fair-Val 0.6555\n",
      "Epoch 020/50 | Train-Loss 95.0301 | Test-MSE 320.1678 | Regret 0.4697 | Fair-Val 0.6679\n",
      "Epoch 030/50 | Train-Loss 102.6413 | Test-MSE 286.2333 | Regret 0.4962 | Fair-Val 0.6153\n",
      "Epoch 040/50 | Train-Loss 103.6052 | Test-MSE 262.2154 | Regret 0.4907 | Fair-Val 0.5056\n",
      "Epoch 050/50 | Train-Loss 101.2571 | Test-MSE 258.7828 | Regret 0.4782 | Fair-Val 0.3715\n",
      "Training finished in 2.57s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.4775 | σ = 0.0007\n",
      "[                 MSE]  μ = 261.7616 | σ = 2.9788\n",
      "[            FAIRNESS]  μ = 0.3742 | σ = 0.0027\n",
      "[       TRAINING_TIME]  μ = 2.5879 | σ = 0.0171\n",
      "[              G0_MSE]  μ = 250.8910 | σ = 2.9904\n",
      "[              G1_MSE]  μ = 342.0882 | σ = 2.8931\n",
      "[     G0_DECISION_OBJ]  μ = 18.3828 | σ = 0.0081\n",
      "[     G1_DECISION_OBJ]  μ = 33.4433 | σ = 1.3908\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3415.9688 | Test-MSE 367.7104 | Regret 0.1155 | Fair-Val 505.4888\n",
      "Epoch 010/50 | Train-Loss 2044.1875 | Test-MSE 353.2028 | Regret 0.0796 | Fair-Val 485.3503\n",
      "Epoch 020/50 | Train-Loss 1394.0293 | Test-MSE 350.2819 | Regret 0.0597 | Fair-Val 480.0713\n",
      "Epoch 030/50 | Train-Loss 689.7168 | Test-MSE 346.1506 | Regret 0.0344 | Fair-Val 472.1028\n",
      "Epoch 040/50 | Train-Loss 289.1016 | Test-MSE 340.3505 | Regret 0.0196 | Fair-Val 461.7527\n",
      "Epoch 050/50 | Train-Loss 236.9004 | Test-MSE 332.2181 | Regret 0.0169 | Fair-Val 450.4261\n",
      "Training finished in 27.59s.\n",
      "Epoch 001/50 | Train-Loss 3512.8027 | Test-MSE 368.0974 | Regret 0.1189 | Fair-Val 505.9565\n",
      "Epoch 010/50 | Train-Loss 2133.3867 | Test-MSE 350.4653 | Regret 0.0841 | Fair-Val 481.9890\n",
      "Epoch 020/50 | Train-Loss 1498.2969 | Test-MSE 344.9782 | Regret 0.0649 | Fair-Val 473.9484\n",
      "Epoch 030/50 | Train-Loss 841.5312 | Test-MSE 340.0007 | Regret 0.0413 | Fair-Val 465.0429\n",
      "Epoch 040/50 | Train-Loss 331.5508 | Test-MSE 339.3988 | Regret 0.0207 | Fair-Val 461.0330\n",
      "Epoch 050/50 | Train-Loss 240.0723 | Test-MSE 333.8560 | Regret 0.0168 | Fair-Val 452.6937\n",
      "Training finished in 27.80s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0169 | σ = 0.0000\n",
      "[                 MSE]  μ = 333.0371 | σ = 0.8189\n",
      "[            FAIRNESS]  μ = 451.5599 | σ = 1.1338\n",
      "[       TRAINING_TIME]  μ = 27.6967 | σ = 0.1043\n",
      "[              G0_MSE]  μ = 313.7988 | σ = 0.7981\n",
      "[              G1_MSE]  μ = 475.1935 | σ = 0.9730\n",
      "[     G0_DECISION_OBJ]  μ = 60.8601 | σ = 0.5214\n",
      "[     G1_DECISION_OBJ]  μ = 115.0959 | σ = 0.9428\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 143.6367 | Test-MSE 369.1213 | Regret 0.0062 | Fair-Val 507.5539\n",
      "Epoch 010/50 | Train-Loss 104.6523 | Test-MSE 359.4040 | Regret 0.0046 | Fair-Val 494.9531\n",
      "Epoch 020/50 | Train-Loss 87.5391 | Test-MSE 354.8450 | Regret 0.0039 | Fair-Val 488.4119\n",
      "Epoch 030/50 | Train-Loss 74.5664 | Test-MSE 351.8091 | Regret 0.0034 | Fair-Val 483.8170\n",
      "Epoch 040/50 | Train-Loss 62.4121 | Test-MSE 351.3908 | Regret 0.0029 | Fair-Val 482.5114\n",
      "Epoch 050/50 | Train-Loss 50.8105 | Test-MSE 347.1885 | Regret 0.0024 | Fair-Val 476.0682\n",
      "Training finished in 48.28s.\n",
      "Epoch 001/50 | Train-Loss 149.1523 | Test-MSE 370.4302 | Regret 0.0061 | Fair-Val 508.6294\n",
      "Epoch 010/50 | Train-Loss 102.2559 | Test-MSE 369.5356 | Regret 0.0044 | Fair-Val 505.7309\n",
      "Epoch 020/50 | Train-Loss 80.8203 | Test-MSE 359.1111 | Regret 0.0036 | Fair-Val 492.1394\n",
      "Epoch 030/50 | Train-Loss 65.7461 | Test-MSE 344.8767 | Regret 0.0030 | Fair-Val 473.4823\n",
      "Epoch 040/50 | Train-Loss 54.4062 | Test-MSE 339.2263 | Regret 0.0026 | Fair-Val 465.6646\n",
      "Epoch 050/50 | Train-Loss 47.4492 | Test-MSE 337.9689 | Regret 0.0023 | Fair-Val 463.3902\n",
      "Training finished in 48.11s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0023 | σ = 0.0001\n",
      "[                 MSE]  μ = 342.5787 | σ = 4.6098\n",
      "[            FAIRNESS]  μ = 469.7292 | σ = 6.3390\n",
      "[       TRAINING_TIME]  μ = 48.1953 | σ = 0.0854\n",
      "[              G0_MSE]  μ = 324.6804 | σ = 4.0592\n",
      "[              G1_MSE]  μ = 474.8340 | σ = 8.6785\n",
      "[     G0_DECISION_OBJ]  μ = 30.5726 | σ = 0.1698\n",
      "[     G1_DECISION_OBJ]  μ = 52.4475 | σ = 0.3794\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 78.9233 | Test-MSE 367.5254 | Regret 0.0475 | Fair-Val 505.2697\n",
      "Epoch 010/50 | Train-Loss 41.1045 | Test-MSE 352.2019 | Regret 0.0295 | Fair-Val 483.7520\n",
      "Epoch 020/50 | Train-Loss 27.2690 | Test-MSE 340.6072 | Regret 0.0220 | Fair-Val 467.4858\n",
      "Epoch 030/50 | Train-Loss 17.3031 | Test-MSE 335.0717 | Regret 0.0156 | Fair-Val 459.1276\n",
      "Epoch 040/50 | Train-Loss 11.7717 | Test-MSE 325.0539 | Regret 0.0117 | Fair-Val 445.1310\n",
      "Epoch 050/50 | Train-Loss 8.7264 | Test-MSE 326.2713 | Regret 0.0092 | Fair-Val 444.8787\n",
      "Training finished in 48.23s.\n",
      "Epoch 001/50 | Train-Loss 82.9224 | Test-MSE 368.2647 | Regret 0.0494 | Fair-Val 506.0984\n",
      "Epoch 010/50 | Train-Loss 44.3149 | Test-MSE 345.0649 | Regret 0.0322 | Fair-Val 475.1792\n",
      "Epoch 020/50 | Train-Loss 28.6765 | Test-MSE 334.2950 | Regret 0.0233 | Fair-Val 459.8874\n",
      "Epoch 030/50 | Train-Loss 19.5533 | Test-MSE 328.3310 | Regret 0.0174 | Fair-Val 450.9614\n",
      "Epoch 040/50 | Train-Loss 14.0190 | Test-MSE 326.1146 | Regret 0.0135 | Fair-Val 447.1089\n",
      "Epoch 050/50 | Train-Loss 10.5828 | Test-MSE 309.3139 | Regret 0.0110 | Fair-Val 423.5012\n",
      "Training finished in 48.26s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0101 | σ = 0.0009\n",
      "[                 MSE]  μ = 317.7926 | σ = 8.4787\n",
      "[            FAIRNESS]  μ = 434.1899 | σ = 10.6888\n",
      "[       TRAINING_TIME]  μ = 48.2490 | σ = 0.0148\n",
      "[              G0_MSE]  μ = 300.4407 | σ = 7.8357\n",
      "[              G1_MSE]  μ = 446.0106 | σ = 13.2296\n",
      "[     G0_DECISION_OBJ]  μ = 19.3136 | σ = 0.0544\n",
      "[     G1_DECISION_OBJ]  μ = 26.5513 | σ = 0.0585\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 74.6058 | Test-MSE 367.6070 | Regret 0.2829 | Fair-Val 505.3412\n",
      "Epoch 010/50 | Train-Loss 35.9731 | Test-MSE 349.5266 | Regret 0.1759 | Fair-Val 480.3135\n",
      "Epoch 020/50 | Train-Loss 22.5636 | Test-MSE 343.4620 | Regret 0.1318 | Fair-Val 470.8025\n",
      "Epoch 030/50 | Train-Loss 13.1557 | Test-MSE 343.6685 | Regret 0.0874 | Fair-Val 469.6504\n",
      "Epoch 040/50 | Train-Loss 7.9965 | Test-MSE 340.8257 | Regret 0.0636 | Fair-Val 463.9208\n",
      "Epoch 050/50 | Train-Loss 6.3787 | Test-MSE 339.9479 | Regret 0.0529 | Fair-Val 461.5259\n",
      "Training finished in 33.43s.\n",
      "Epoch 001/50 | Train-Loss 79.0229 | Test-MSE 367.9788 | Regret 0.2958 | Fair-Val 505.8084\n",
      "Epoch 010/50 | Train-Loss 38.3754 | Test-MSE 348.4821 | Regret 0.1879 | Fair-Val 479.0026\n",
      "Epoch 020/50 | Train-Loss 24.2450 | Test-MSE 340.9009 | Regret 0.1401 | Fair-Val 467.6172\n",
      "Epoch 030/50 | Train-Loss 14.9223 | Test-MSE 340.9932 | Regret 0.0971 | Fair-Val 466.5834\n",
      "Epoch 040/50 | Train-Loss 9.6102 | Test-MSE 339.6258 | Regret 0.0718 | Fair-Val 463.0620\n",
      "Epoch 050/50 | Train-Loss 6.7616 | Test-MSE 338.6022 | Regret 0.0576 | Fair-Val 460.2617\n",
      "Training finished in 32.98s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0552 | σ = 0.0023\n",
      "[                 MSE]  μ = 339.2751 | σ = 0.6729\n",
      "[            FAIRNESS]  μ = 460.8938 | σ = 0.6321\n",
      "[       TRAINING_TIME]  μ = 33.2053 | σ = 0.2263\n",
      "[              G0_MSE]  μ = 320.4697 | σ = 0.6150\n",
      "[              G1_MSE]  μ = 478.2328 | σ = 1.1002\n",
      "[     G0_DECISION_OBJ]  μ = 16.6029 | σ = 0.0044\n",
      "[     G1_DECISION_OBJ]  μ = 21.6067 | σ = 0.0150\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3953.8799 | Test-MSE 367.7047 | Regret 0.1155 | Fair-Val 505.4829\n",
      "Epoch 010/50 | Train-Loss 2567.0815 | Test-MSE 350.9344 | Regret 0.0802 | Fair-Val 482.5413\n",
      "Epoch 020/50 | Train-Loss 1933.3568 | Test-MSE 344.9248 | Regret 0.0616 | Fair-Val 473.5445\n",
      "Epoch 030/50 | Train-Loss 1242.2041 | Test-MSE 339.2037 | Regret 0.0372 | Fair-Val 463.4492\n",
      "Epoch 040/50 | Train-Loss 774.6402 | Test-MSE 330.6731 | Regret 0.0206 | Fair-Val 448.9404\n",
      "Epoch 050/50 | Train-Loss 703.1393 | Test-MSE 321.9910 | Regret 0.0171 | Fair-Val 436.2473\n",
      "Training finished in 27.25s.\n",
      "Epoch 001/50 | Train-Loss 4051.9204 | Test-MSE 368.0918 | Regret 0.1189 | Fair-Val 505.9486\n",
      "Epoch 010/50 | Train-Loss 2663.8845 | Test-MSE 350.2644 | Regret 0.0851 | Fair-Val 481.6581\n",
      "Epoch 020/50 | Train-Loss 2015.7759 | Test-MSE 345.6841 | Regret 0.0656 | Fair-Val 474.6392\n",
      "Epoch 030/50 | Train-Loss 1352.5533 | Test-MSE 342.8569 | Regret 0.0420 | Fair-Val 468.6611\n",
      "Epoch 040/50 | Train-Loss 838.1738 | Test-MSE 341.3644 | Regret 0.0216 | Fair-Val 463.6707\n",
      "Epoch 050/50 | Train-Loss 714.5704 | Test-MSE 332.1675 | Regret 0.0168 | Fair-Val 450.2224\n",
      "Training finished in 27.14s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0169 | σ = 0.0001\n",
      "[                 MSE]  μ = 327.0793 | σ = 5.0883\n",
      "[            FAIRNESS]  μ = 443.2348 | σ = 6.9876\n",
      "[       TRAINING_TIME]  μ = 27.1967 | σ = 0.0571\n",
      "[              G0_MSE]  μ = 308.2131 | σ = 4.9050\n",
      "[              G1_MSE]  μ = 466.4866 | σ = 6.4424\n",
      "[     G0_DECISION_OBJ]  μ = 61.1836 | σ = 0.0156\n",
      "[     G1_DECISION_OBJ]  μ = 115.0043 | σ = 0.4448\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 681.5478 | Test-MSE 367.2614 | Regret 0.0057 | Fair-Val 505.0760\n",
      "Epoch 010/50 | Train-Loss 626.3253 | Test-MSE 363.0159 | Regret 0.0043 | Fair-Val 498.1406\n",
      "Epoch 020/50 | Train-Loss 587.7930 | Test-MSE 355.9619 | Regret 0.0034 | Fair-Val 487.8151\n",
      "Epoch 030/50 | Train-Loss 579.9941 | Test-MSE 362.2079 | Regret 0.0028 | Fair-Val 494.6122\n",
      "Epoch 040/50 | Train-Loss 569.1487 | Test-MSE 364.0675 | Regret 0.0023 | Fair-Val 496.0624\n",
      "Epoch 050/50 | Train-Loss 559.2351 | Test-MSE 363.3641 | Regret 0.0019 | Fair-Val 494.4834\n",
      "Training finished in 47.34s.\n",
      "Epoch 001/50 | Train-Loss 688.2701 | Test-MSE 370.3325 | Regret 0.0061 | Fair-Val 508.5059\n",
      "Epoch 010/50 | Train-Loss 620.0541 | Test-MSE 360.6259 | Regret 0.0043 | Fair-Val 494.7746\n",
      "Epoch 020/50 | Train-Loss 599.6618 | Test-MSE 362.2678 | Regret 0.0035 | Fair-Val 495.5809\n",
      "Epoch 030/50 | Train-Loss 566.7337 | Test-MSE 351.2002 | Regret 0.0029 | Fair-Val 480.3222\n",
      "Epoch 040/50 | Train-Loss 534.5636 | Test-MSE 334.8505 | Regret 0.0027 | Fair-Val 458.1371\n",
      "Epoch 050/50 | Train-Loss 493.0544 | Test-MSE 307.5952 | Regret 0.0027 | Fair-Val 422.0404\n",
      "Training finished in 47.52s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0023 | σ = 0.0004\n",
      "[                 MSE]  μ = 335.4796 | σ = 27.8845\n",
      "[            FAIRNESS]  μ = 458.2619 | σ = 36.2215\n",
      "[       TRAINING_TIME]  μ = 47.4289 | σ = 0.0916\n",
      "[              G0_MSE]  μ = 319.1278 | σ = 25.1609\n",
      "[              G1_MSE]  μ = 456.3080 | σ = 48.0094\n",
      "[     G0_DECISION_OBJ]  μ = 30.9468 | σ = 0.0893\n",
      "[     G1_DECISION_OBJ]  μ = 53.9852 | σ = 0.5975\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 616.8344 | Test-MSE 367.4256 | Regret 0.0476 | Fair-Val 505.1841\n",
      "Epoch 010/50 | Train-Loss 531.7774 | Test-MSE 332.0768 | Regret 0.0340 | Fair-Val 459.3831\n",
      "Epoch 020/50 | Train-Loss 443.3038 | Test-MSE 274.7372 | Regret 0.0346 | Fair-Val 385.6256\n",
      "Epoch 030/50 | Train-Loss 360.0031 | Test-MSE 219.9157 | Regret 0.0359 | Fair-Val 310.0103\n",
      "Epoch 040/50 | Train-Loss 295.3876 | Test-MSE 180.2204 | Regret 0.0358 | Fair-Val 253.6108\n",
      "Epoch 050/50 | Train-Loss 231.3551 | Test-MSE 145.2916 | Regret 0.0361 | Fair-Val 196.4113\n",
      "Training finished in 46.54s.\n",
      "Epoch 001/50 | Train-Loss 622.0402 | Test-MSE 367.9836 | Regret 0.0495 | Fair-Val 505.8148\n",
      "Epoch 010/50 | Train-Loss 534.8118 | Test-MSE 334.7857 | Regret 0.0348 | Fair-Val 461.9479\n",
      "Epoch 020/50 | Train-Loss 449.2767 | Test-MSE 282.5611 | Regret 0.0345 | Fair-Val 394.0432\n",
      "Epoch 030/50 | Train-Loss 370.3390 | Test-MSE 231.1667 | Regret 0.0349 | Fair-Val 324.4016\n",
      "Epoch 040/50 | Train-Loss 305.0146 | Test-MSE 189.0744 | Regret 0.0343 | Fair-Val 267.3841\n",
      "Epoch 050/50 | Train-Loss 244.1181 | Test-MSE 152.0016 | Regret 0.0341 | Fair-Val 212.6819\n",
      "Training finished in 46.72s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0351 | σ = 0.0010\n",
      "[                 MSE]  μ = 148.6466 | σ = 3.3550\n",
      "[            FAIRNESS]  μ = 204.5466 | σ = 8.1353\n",
      "[       TRAINING_TIME]  μ = 46.6333 | σ = 0.0903\n",
      "[              G0_MSE]  μ = 142.8367 | σ = 3.7483\n",
      "[              G1_MSE]  μ = 191.5771 | σ = 0.4490\n",
      "[     G0_DECISION_OBJ]  μ = 20.3225 | σ = 0.1266\n",
      "[     G1_DECISION_OBJ]  μ = 34.6209 | σ = 0.1222\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'mad'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 612.5168 | Test-MSE 367.5231 | Regret 0.2833 | Fair-Val 505.2639\n",
      "Epoch 010/50 | Train-Loss 526.1622 | Test-MSE 331.9506 | Regret 0.2019 | Fair-Val 459.1397\n",
      "Epoch 020/50 | Train-Loss 437.6317 | Test-MSE 273.8434 | Regret 0.2206 | Fair-Val 384.4040\n",
      "Epoch 030/50 | Train-Loss 354.6549 | Test-MSE 218.3264 | Regret 0.2762 | Fair-Val 308.1146\n",
      "Epoch 040/50 | Train-Loss 286.6944 | Test-MSE 175.6366 | Regret 0.3032 | Fair-Val 247.1174\n",
      "Epoch 050/50 | Train-Loss 217.5502 | Test-MSE 139.8183 | Regret 0.3433 | Fair-Val 181.5182\n",
      "Training finished in 32.72s.\n",
      "Epoch 001/50 | Train-Loss 618.1407 | Test-MSE 367.8609 | Regret 0.2963 | Fair-Val 505.6937\n",
      "Epoch 010/50 | Train-Loss 528.6384 | Test-MSE 331.9601 | Regret 0.2183 | Fair-Val 458.8145\n",
      "Epoch 020/50 | Train-Loss 438.7554 | Test-MSE 274.4778 | Regret 0.2441 | Fair-Val 384.6233\n",
      "Epoch 030/50 | Train-Loss 357.6295 | Test-MSE 220.3677 | Regret 0.2945 | Fair-Val 310.4916\n",
      "Epoch 040/50 | Train-Loss 288.8717 | Test-MSE 176.9888 | Regret 0.3016 | Fair-Val 249.3532\n",
      "Epoch 050/50 | Train-Loss 220.8710 | Test-MSE 141.0636 | Regret 0.3332 | Fair-Val 185.7623\n",
      "Training finished in 32.83s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.3382 | σ = 0.0050\n",
      "[                 MSE]  μ = 140.4409 | σ = 0.6227\n",
      "[            FAIRNESS]  μ = 183.6403 | σ = 2.1220\n",
      "[       TRAINING_TIME]  μ = 32.7783 | σ = 0.0542\n",
      "[              G0_MSE]  μ = 134.7895 | σ = 0.6655\n",
      "[              G1_MSE]  μ = 182.2012 | σ = 0.3064\n",
      "[     G0_DECISION_OBJ]  μ = 16.5692 | σ = 0.0041\n",
      "[     G1_DECISION_OBJ]  μ = 75.7395 | σ = 0.6073\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 3416.5906 | Test-MSE 367.7104 | Regret 0.1155 | Fair-Val 0.6222\n",
      "Epoch 010/50 | Train-Loss 2043.8499 | Test-MSE 352.6847 | Regret 0.0796 | Fair-Val 0.6142\n",
      "Epoch 020/50 | Train-Loss 1414.4500 | Test-MSE 349.0280 | Regret 0.0607 | Fair-Val 0.6104\n",
      "Epoch 030/50 | Train-Loss 711.9598 | Test-MSE 346.2029 | Regret 0.0353 | Fair-Val 0.6042\n",
      "Epoch 040/50 | Train-Loss 301.6839 | Test-MSE 342.4666 | Regret 0.0202 | Fair-Val 0.5986\n",
      "Epoch 050/50 | Train-Loss 249.3577 | Test-MSE 336.4994 | Regret 0.0174 | Fair-Val 0.5990\n",
      "Training finished in 27.17s.\n",
      "Epoch 001/50 | Train-Loss 3513.4246 | Test-MSE 368.0974 | Regret 0.1189 | Fair-Val 0.6222\n",
      "Epoch 010/50 | Train-Loss 2125.7075 | Test-MSE 352.1580 | Regret 0.0840 | Fair-Val 0.6139\n",
      "Epoch 020/50 | Train-Loss 1481.1256 | Test-MSE 349.3294 | Regret 0.0644 | Fair-Val 0.6100\n",
      "Epoch 030/50 | Train-Loss 782.4255 | Test-MSE 346.8904 | Regret 0.0386 | Fair-Val 0.6050\n",
      "Epoch 040/50 | Train-Loss 309.7903 | Test-MSE 345.4209 | Regret 0.0195 | Fair-Val 0.5989\n",
      "Epoch 050/50 | Train-Loss 237.9923 | Test-MSE 339.8972 | Regret 0.0165 | Fair-Val 0.5982\n",
      "Training finished in 27.13s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0170 | σ = 0.0005\n",
      "[                 MSE]  μ = 338.1983 | σ = 1.6989\n",
      "[            FAIRNESS]  μ = 0.5986 | σ = 0.0004\n",
      "[       TRAINING_TIME]  μ = 27.1511 | σ = 0.0236\n",
      "[              G0_MSE]  μ = 318.6804 | σ = 1.5937\n",
      "[              G1_MSE]  μ = 482.4207 | σ = 2.4763\n",
      "[     G0_DECISION_OBJ]  μ = 60.7777 | σ = 0.6888\n",
      "[     G1_DECISION_OBJ]  μ = 115.2031 | σ = 1.2761\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 0.8, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 144.2585 | Test-MSE 369.0858 | Regret 0.0061 | Fair-Val 0.6244\n",
      "Epoch 010/50 | Train-Loss 108.2767 | Test-MSE 355.1115 | Regret 0.0047 | Fair-Val 0.6276\n",
      "Epoch 020/50 | Train-Loss 93.7646 | Test-MSE 336.8481 | Regret 0.0042 | Fair-Val 0.6329\n",
      "Epoch 030/50 | Train-Loss 79.8754 | Test-MSE 330.2327 | Regret 0.0036 | Fair-Val 0.6285\n",
      "Epoch 040/50 | Train-Loss 70.2146 | Test-MSE 327.9977 | Regret 0.0033 | Fair-Val 0.6266\n",
      "Epoch 050/50 | Train-Loss 64.5441 | Test-MSE 321.3904 | Regret 0.0030 | Fair-Val 0.6280\n",
      "Training finished in 47.88s.\n",
      "Epoch 001/50 | Train-Loss 149.7742 | Test-MSE 370.4302 | Regret 0.0061 | Fair-Val 0.6211\n",
      "Epoch 010/50 | Train-Loss 100.8422 | Test-MSE 359.1764 | Regret 0.0044 | Fair-Val 0.6176\n",
      "Epoch 020/50 | Train-Loss 82.3584 | Test-MSE 357.8701 | Regret 0.0037 | Fair-Val 0.6140\n",
      "Epoch 030/50 | Train-Loss 67.3941 | Test-MSE 360.3270 | Regret 0.0031 | Fair-Val 0.6108\n",
      "Epoch 040/50 | Train-Loss 52.6661 | Test-MSE 357.5821 | Regret 0.0024 | Fair-Val 0.6072\n",
      "Epoch 050/50 | Train-Loss 44.5006 | Test-MSE 339.0534 | Regret 0.0022 | Fair-Val 0.6044\n",
      "Training finished in 47.51s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0026 | σ = 0.0004\n",
      "[                 MSE]  μ = 330.2219 | σ = 8.8315\n",
      "[            FAIRNESS]  μ = 0.6162 | σ = 0.0118\n",
      "[       TRAINING_TIME]  μ = 47.6979 | σ = 0.1867\n",
      "[              G0_MSE]  μ = 313.0548 | σ = 8.4626\n",
      "[              G1_MSE]  μ = 457.0742 | σ = 11.5570\n",
      "[     G0_DECISION_OBJ]  μ = 30.4439 | σ = 0.4061\n",
      "[     G1_DECISION_OBJ]  μ = 52.3319 | σ = 1.4543\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 1.5, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 79.5451 | Test-MSE 367.5248 | Regret 0.0475 | Fair-Val 0.6223\n",
      "Epoch 010/50 | Train-Loss 43.9422 | Test-MSE 344.1613 | Regret 0.0311 | Fair-Val 0.6190\n",
      "Epoch 020/50 | Train-Loss 30.4668 | Test-MSE 327.6100 | Regret 0.0236 | Fair-Val 0.6149\n",
      "Epoch 030/50 | Train-Loss 22.2192 | Test-MSE 313.7325 | Regret 0.0187 | Fair-Val 0.6153\n",
      "Epoch 040/50 | Train-Loss 14.9429 | Test-MSE 313.3910 | Regret 0.0137 | Fair-Val 0.6114\n",
      "Epoch 050/50 | Train-Loss 10.9596 | Test-MSE 307.6783 | Regret 0.0104 | Fair-Val 0.6100\n",
      "Training finished in 47.35s.\n",
      "Epoch 001/50 | Train-Loss 83.5442 | Test-MSE 368.2647 | Regret 0.0494 | Fair-Val 0.6219\n",
      "Epoch 010/50 | Train-Loss 43.0055 | Test-MSE 354.5529 | Regret 0.0306 | Fair-Val 0.6102\n",
      "Epoch 020/50 | Train-Loss 27.7917 | Test-MSE 349.1666 | Regret 0.0222 | Fair-Val 0.6052\n",
      "Epoch 030/50 | Train-Loss 16.9309 | Test-MSE 349.5036 | Regret 0.0146 | Fair-Val 0.6039\n",
      "Epoch 040/50 | Train-Loss 11.1389 | Test-MSE 350.2031 | Regret 0.0103 | Fair-Val 0.6012\n",
      "Epoch 050/50 | Train-Loss 8.2520 | Test-MSE 344.7473 | Regret 0.0081 | Fair-Val 0.5998\n",
      "Training finished in 47.33s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0093 | σ = 0.0011\n",
      "[                 MSE]  μ = 326.2128 | σ = 18.5345\n",
      "[            FAIRNESS]  μ = 0.6049 | σ = 0.0051\n",
      "[       TRAINING_TIME]  μ = 47.3386 | σ = 0.0124\n",
      "[              G0_MSE]  μ = 308.1300 | σ = 17.3450\n",
      "[              G1_MSE]  μ = 459.8316 | σ = 27.3238\n",
      "[     G0_DECISION_OBJ]  μ = 19.4147 | σ = 0.1289\n",
      "[     G1_DECISION_OBJ]  μ = 26.8550 | σ = 0.1579\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUNNING EXPERIMENT: {'Group': False, 'Grad Method': 'finite-diff', 'Alpha': 2, 'Lambda': 1.0, 'Fairness': 'atkinson'}\n",
      "----------------------------------------------------------------------\n",
      "Epoch 001/50 | Train-Loss 75.2275 | Test-MSE 367.6070 | Regret 0.2829 | Fair-Val 0.6222\n",
      "Epoch 010/50 | Train-Loss 36.5637 | Test-MSE 349.3493 | Regret 0.1759 | Fair-Val 0.6122\n",
      "Epoch 020/50 | Train-Loss 23.1036 | Test-MSE 343.7390 | Regret 0.1314 | Fair-Val 0.6071\n",
      "Epoch 030/50 | Train-Loss 13.7887 | Test-MSE 342.4338 | Regret 0.0877 | Fair-Val 0.6057\n",
      "Epoch 040/50 | Train-Loss 8.6105 | Test-MSE 340.7592 | Regret 0.0644 | Fair-Val 0.6013\n",
      "Epoch 050/50 | Train-Loss 6.9723 | Test-MSE 340.2456 | Regret 0.0530 | Fair-Val 0.5992\n",
      "Training finished in 32.78s.\n",
      "Epoch 001/50 | Train-Loss 79.6448 | Test-MSE 367.9788 | Regret 0.2958 | Fair-Val 0.6223\n",
      "Epoch 010/50 | Train-Loss 39.2088 | Test-MSE 347.8881 | Regret 0.1888 | Fair-Val 0.6134\n",
      "Epoch 020/50 | Train-Loss 25.0922 | Test-MSE 339.2909 | Regret 0.1416 | Fair-Val 0.6083\n",
      "Epoch 030/50 | Train-Loss 15.8780 | Test-MSE 337.8957 | Regret 0.0987 | Fair-Val 0.6073\n",
      "Epoch 040/50 | Train-Loss 10.4313 | Test-MSE 336.9919 | Regret 0.0729 | Fair-Val 0.6031\n",
      "Epoch 050/50 | Train-Loss 7.4715 | Test-MSE 336.6005 | Regret 0.0581 | Fair-Val 0.6002\n",
      "Training finished in 32.86s.\n",
      "\n",
      "============================================================\n",
      "      AVERAGED RESULTS ACROSS ALL TRIALS\n",
      "============================================================\n",
      "[              REGRET]  μ = 0.0556 | σ = 0.0026\n",
      "[                 MSE]  μ = 338.4230 | σ = 1.8226\n",
      "[            FAIRNESS]  μ = 0.5997 | σ = 0.0005\n",
      "[       TRAINING_TIME]  μ = 32.8200 | σ = 0.0405\n",
      "[              G0_MSE]  μ = 319.6783 | σ = 1.6795\n",
      "[              G1_MSE]  μ = 476.9326 | σ = 2.8796\n",
      "[     G0_DECISION_OBJ]  μ = 16.6297 | σ = 0.0168\n",
      "[     G1_DECISION_OBJ]  μ = 21.6343 | σ = 0.0213\n",
      "[     G0_TRUE_BENEFIT]  μ = 11.9597 | σ = 0.0000\n",
      "[     G1_TRUE_BENEFIT]  μ = 16.2677 | σ = 0.0000\n",
      "\n",
      "==========================================================================================\n",
      "                           GRID SEARCH COMPLETE\n",
      "==========================================================================================\n",
      "    Group  Grad Method  Alpha  Lambda  Fairness  G0_decision_obj  G0_decision_obj_std      G0_mse  G0_mse_std  G0_true_benefit  G0_true_benefit_std  G1_decision_obj  G1_decision_obj_std      G1_mse  G1_mse_std  G1_true_benefit  G1_true_benefit_std    fairness  fairness_std         mse    mse_std    regret  regret_std  training_time  training_time_std\n",
      "0    True  closed-form    0.5     0.0       mad        42.885602             0.381066  328.561310    0.647247        11.959665                  0.0       326.179104             3.364223  473.356598    0.620758        16.267719                  0.0   72.397644      0.013245  345.820892   0.644104  0.033816    0.000394     274.621546          13.972836\n",
      "1    True  closed-form    0.8     0.0       mad        19.420700             0.000598  335.580627    0.110535        11.959665                  0.0       162.440094             0.022816  493.023468    0.166870        16.267719                  0.0   78.721420      0.028168  354.347839   0.117249  0.000651    0.000001     264.772162           0.580686\n",
      "2    True  closed-form    1.5     0.0       mad        16.792012             0.002661  321.008957    0.064896        11.959665                  0.0        55.175333             0.067195  481.816803    0.090820        16.267719                  0.0   80.403923      0.077858  340.177216   0.046326  0.003886    0.000069     274.797599           9.432766\n",
      "3    True  closed-form    2.0     0.0       mad        16.595218             0.012964  321.211945    0.970337        11.959665                  0.0        21.607846             0.029735  479.428482    1.489456        16.267719                  0.0   79.108269      0.259560  340.071350   1.032196  0.054595    0.002242     294.297873           3.509423\n",
      "4    True  closed-form    0.5     1.0       mad        37.017497             0.339698  252.937981    0.097527        11.959665                  0.0       254.405521             1.552983  320.991211    0.966736        16.267719                  0.0   34.026615      0.434605  261.049896   0.201141  0.102945    0.001905     331.209196          43.209460\n",
      "5    True  closed-form    0.8     1.0       mad        20.024442             0.032839  253.122360    0.203430        11.959665                  0.0       166.441474             0.469258  321.102310    0.834793        16.267719                  0.0   33.989975      0.315681  261.225571   0.278702  0.002046    0.000063     387.478635          19.519375\n",
      "6    True  closed-form    1.5     1.0       mad        18.165037             0.018044  240.237396    0.913879        11.959665                  0.0        60.672982             0.036672  304.411377    1.968811        16.267719                  0.0   32.086990      0.527466  247.886940   1.039635  0.026125    0.000396     394.932016          45.030531\n",
      "7    True  closed-form    2.0     1.0       mad        17.427934             0.028964  231.503967    0.936157        11.959665                  0.0        22.861863             1.979074  284.153809    0.693207        16.267719                  0.0   26.324921      0.121475  237.779831   0.907227  0.129690    0.004618     259.516278           1.544331\n",
      "8    True  closed-form    0.5     1.0  atkinson        42.369655             0.037111  318.592682    0.668488        11.959665                  0.0       328.402565             2.183663  445.839783    0.117889        16.267719                  0.0    0.003355      0.000050  333.760513   0.574753  0.036726    0.000284     257.182714           0.117035\n",
      "9    True  closed-form    0.8     1.0  atkinson        19.545886             0.008996  284.665588    0.998535        11.959665                  0.0       166.687751             0.194213  359.925644    1.655502        16.267719                  0.0    0.001576      0.000015  293.636581   1.076859  0.001236    0.000009     258.328434           0.117207\n",
      "10   True  closed-form    1.5     1.0  atkinson        16.794573             0.002382  320.952057    0.098572        11.959665                  0.0        55.169916             0.068523  481.437317    0.031006        16.267719                  0.0    0.005005      0.000010  340.081909   0.083130  0.003886    0.000066     258.494168           0.440194\n",
      "11   True  closed-form    2.0     1.0  atkinson        16.595815             0.012455  321.213776    0.974792        11.959665                  0.0        21.608447             0.029068  479.401855    1.489044        16.267719                  0.0    0.004873      0.000002  340.069809   1.036118  0.054593    0.002246     261.851391           1.185793\n",
      "12   True  finite-diff    0.5     0.0       mad        45.696191             0.339362  316.930832    1.130142        11.959665                  0.0       262.815521             2.448009  480.004089    1.283569        16.267719                  0.0   81.536629      0.076714  336.369171   1.148438  0.009046    0.000047      82.230106           0.001729\n",
      "13   True  finite-diff    0.8     0.0       mad        21.094724             0.013952  320.789398    7.526611        11.959665                  0.0       171.836513             0.008226  480.882126   10.298019        16.267719                  0.0   80.046364      1.385704  339.872482   7.856979  0.000191    0.000002     142.157507           0.074044\n",
      "14   True  finite-diff    1.5     0.0       mad        16.796529             0.008838  318.608658    3.402023        11.959665                  0.0        55.173669             0.014700  478.014328    5.261826        16.267719                  0.0   79.702835      0.929901  337.609818   3.623703  0.003959    0.000134     143.161553           0.215291\n",
      "15   True  finite-diff    2.0     0.0       mad        16.624645             0.015326  322.640442    0.110168        11.959665                  0.0        21.617902             0.032105  481.644211    0.030167        16.267719                  0.0   79.501884      0.040001  341.593689   0.100647  0.055019    0.001653      98.586610           0.169780\n",
      "16   True  finite-diff    0.5     1.0       mad        34.119008             0.173976  218.676117    2.509567        11.959665                  0.0       249.645119             0.014838  278.204742    2.649323        16.267719                  0.0   29.764313      0.069878  225.771950   2.526253  0.039385    0.000287      82.519408           0.312963\n",
      "17   True  finite-diff    0.8     1.0       mad              NaN                  NaN         NaN         NaN        11.959665                  0.0              NaN                  NaN         NaN         NaN        16.267719                  0.0         NaN           NaN         NaN        NaN       NaN         NaN     131.120757           0.212160\n",
      "18   True  finite-diff    1.5     1.0       mad        18.164503             0.009834  240.558472    0.643341        11.959665                  0.0        60.625740             0.491879  304.670578    1.823929        16.267719                  0.0   32.056053      0.590294  248.200638   0.784088  0.026018    0.000125     142.827264           0.170143\n",
      "19   True  finite-diff    2.0     1.0       mad        17.448669             0.015326  231.442726    1.761421        11.959665                  0.0        22.606911             1.540396  284.221985    1.800629        16.267719                  0.0   26.389629      0.019604  237.734024   1.766098  0.129936    0.003492     100.362417           0.291839\n",
      "20   True  finite-diff    0.5     1.0  atkinson        45.654282             0.197665  316.517288    2.256058        11.959665                  0.0       262.441905             3.058609  479.518448    2.839401        16.267719                  0.0    0.005272      0.000033  335.947021   2.325592  0.009029    0.000010      82.205243           0.292571\n",
      "21   True  finite-diff    0.8     1.0  atkinson        20.920451             0.105124  327.265015    7.693390        11.959665                  0.0       170.817202             0.021091  482.818222   13.440170        16.267719                  0.0    0.004575      0.000109  345.806946   8.378387  0.000210    0.000020     142.039248           0.886154\n",
      "22   True  finite-diff    1.5     1.0  atkinson        16.824349             0.011203  320.254288    2.477066        11.959665                  0.0        55.193770             0.125284  480.448563    2.981033        16.267719                  0.0    0.005009      0.000040  339.349426   2.537140  0.003988    0.000090     142.919753           0.037468\n",
      "23   True  finite-diff    2.0     1.0  atkinson        16.630424             0.014119  323.449203    0.234451        11.959665                  0.0        21.577875             0.032359  482.530182    0.514557        16.267719                  0.0    0.004862      0.000009  342.411682   0.267822  0.054825    0.002085      98.496365           0.305569\n",
      "24  False  closed-form    0.5     0.0       mad        27.020298             4.774341  365.098801    0.007339        11.959665                  0.0       379.414795           193.261154  545.324860    0.013275        16.267719                  0.0  524.873383      0.008759  386.581741   0.008072  0.371726    0.039384       2.535680           0.005022\n",
      "25  False  closed-form    0.8     0.0       mad        27.642559             0.048028  364.963821    0.002426        11.959665                  0.0        51.190063             1.595219  544.943298    0.032043        16.267719                  0.0  524.649872      0.011627  386.417358   0.006012  0.014549    0.000363       2.558804           0.019719\n",
      "26  False  closed-form    1.5     0.0       mad        21.048405             0.019268  302.442307   10.390305        11.959665                  0.0        33.833996             0.251884  397.762512    9.640747        16.267719                  0.0  302.663559      5.222153  313.804459  10.300980  0.096445    0.001309       2.603944           0.039312\n",
      "27  False  closed-form    2.0     0.0       mad        18.381802             0.008153  250.812347    2.981705        11.959665                  0.0        33.439049             1.383116  342.084488    2.906326        16.267719                  0.0  264.976929      2.585663  261.692001   2.972733  0.477345    0.000734       2.491812           0.008226\n",
      "28  False  closed-form    0.5     1.0       mad        27.009521             4.689495  365.098755    0.007446        11.959665                  0.0       377.653015           190.082520  545.324707    0.012390        16.267719                  0.0  524.873352      0.008728  386.581696   0.008057  0.371811    0.038960       2.577030           0.041822\n",
      "29  False  closed-form    0.8     1.0       mad        27.649586             0.038444  364.971848    0.004044        11.959665                  0.0        51.202721             1.649641  544.952240    0.014435        16.267719                  0.0  524.662048      0.010620  386.425507   0.005280  0.014370    0.000426       2.563785           0.013844\n",
      "30  False  closed-form    1.5     1.0       mad        21.107574             0.020972  255.963371    7.671272        11.959665                  0.0        32.467621             0.184771  310.486252    2.656693        16.267719                  0.0  228.147766      9.090637  262.462479   7.073532  0.071927    0.000692       2.578070           0.044526\n",
      "31  False  closed-form    2.0     1.0       mad        18.387234             0.062823  184.953506    0.964783        11.959665                  0.0        32.414486             2.310066  231.761375    7.112831        16.267719                  0.0  157.306343      2.270638  190.532982   1.697639  0.326789    0.001970       2.567561           0.016870\n",
      "32  False  closed-form    0.5     1.0  atkinson        27.020294             4.774338  365.098801    0.007339        11.959665                  0.0       379.414856           193.261093  545.324860    0.013275        16.267719                  0.0    0.597660      0.000005  386.581741   0.008072  0.371726    0.039384       2.578232           0.001975\n",
      "33  False  closed-form    0.8     1.0  atkinson        27.642513             0.048074  364.963806    0.002411        11.959665                  0.0        51.190369             1.595514  544.943420    0.032166        16.267719                  0.0    0.597663      0.000006  386.417343   0.005997  0.014549    0.000363       2.580668           0.033908\n",
      "34  False  closed-form    1.5     1.0  atkinson        21.048674             0.019276  302.502090   10.411819        11.959665                  0.0        33.834305             0.252300  397.810226    9.669205        16.267719                  0.0    0.369924      0.000108  313.862823  10.323334  0.096457    0.001316       2.532691           0.013315\n",
      "35  False  closed-form    2.0     1.0  atkinson        18.382778             0.008149  250.890953    2.990410        11.959665                  0.0        33.443314             1.390825  342.088150    2.893051        16.267719                  0.0    0.374190      0.002711  261.761642   2.978806  0.477495    0.000716       2.587912           0.017083\n",
      "36  False  finite-diff    0.5     0.0       mad        60.860062             0.521421  313.798813    0.798080        11.959665                  0.0       115.095940             0.942757  475.193542    0.973022        16.267719                  0.0  451.559875      1.133789  333.037079   0.818939  0.016853    0.000048      27.696736           0.104324\n",
      "37  False  finite-diff    0.8     0.0       mad        30.572609             0.169820  324.680405    4.059250        11.959665                  0.0        52.447464             0.379442  474.834000    8.678482        16.267719                  0.0  469.729218      6.339020  342.578705   4.609833  0.002309    0.000051      48.195258           0.085395\n",
      "38  False  finite-diff    1.5     0.0       mad        19.313580             0.054379  300.440689    7.835739        11.959665                  0.0        26.551258             0.058515  446.010605   13.229630        16.267719                  0.0  434.189926     10.688766  317.792633   8.478699  0.010092    0.000887      48.248995           0.014812\n",
      "39  False  finite-diff    2.0     0.0       mad        16.602900             0.004362  320.469681    0.615005        11.959665                  0.0        21.606749             0.015003  478.232773    1.100174        16.267719                  0.0  460.893814      0.632126  339.275055   0.672852  0.055237    0.002325      33.205332           0.226325\n",
      "40  False  finite-diff    0.5     1.0       mad        61.183563             0.015585  308.213089    4.905014        11.959665                  0.0       115.004333             0.444820  466.486603    6.442444        16.267719                  0.0  443.234848      6.987595  327.079269   5.088272  0.016931    0.000137      27.196665           0.057140\n",
      "41  False  finite-diff    0.8     1.0       mad        30.946777             0.089279  319.127777   25.160950        11.959665                  0.0        53.985229             0.597458  456.307999   48.009445        16.267719                  0.0  458.261917     36.221481  335.479645  27.884491  0.002301    0.000400      47.428917           0.091623\n",
      "42  False  finite-diff    1.5     1.0       mad        20.322514             0.126605  142.836723    3.748299        11.959665                  0.0        34.620934             0.122175  191.577141    0.448982        16.267719                  0.0  204.546577      8.135323  148.646591   3.355026  0.035089    0.001019      46.633286           0.090254\n",
      "43  False  finite-diff    2.0     1.0       mad        16.569235             0.004082  134.789452    0.665474        11.959665                  0.0        75.739487             0.607315  182.201241    0.306389        16.267719                  0.0  183.640297      2.122047  140.440941   0.622673  0.338224    0.005040      32.778311           0.054214\n",
      "44  False  finite-diff    0.5     1.0  atkinson        60.777687             0.688784  318.680405    1.593704        11.959665                  0.0       115.203064             1.276150  482.420700    2.476334        16.267719                  0.0    0.598605      0.000386  338.198257   1.698898  0.016989    0.000457      27.151094           0.023568\n",
      "45  False  finite-diff    0.8     1.0  atkinson        30.443924             0.406112  313.054825    8.462631        11.959665                  0.0        52.331898             1.454327  457.074203   11.556992        16.267719                  0.0    0.616213      0.011800  330.221924   8.831482  0.002625    0.000416      47.697885           0.186656\n",
      "46  False  finite-diff    1.5     1.0  atkinson        19.414701             0.128948  308.130005   17.345032        11.959665                  0.0        26.854992             0.157941  459.831604   27.323822        16.267719                  0.0    0.604894      0.005123  326.212814  18.534500  0.009272    0.001123      47.338640           0.012407\n",
      "47  False  finite-diff    2.0     1.0  atkinson        16.629665             0.016752  319.678314    1.679535        11.959665                  0.0        21.634274             0.021319  476.932648    2.879578        16.267719                  0.0    0.599662      0.000497  338.423035   1.822571  0.055586    0.002555      32.820024           0.040500\n",
      "\n",
      "--- LaTeX Table Output ---\n",
      "\\begin{table}\n",
      "\\caption{Averaged Experimental Results Across Different Parameters.}\n",
      "\\label{tab:avg_exp_results_expanded}\n",
      "\\begin{tabular}{rlrrlrrrrrrrrrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "Group & Grad Method & Alpha & Lambda & Fairness & G0_decision_obj & G0_decision_obj_std & G0_mse & G0_mse_std & G0_true_benefit & G0_true_benefit_std & G1_decision_obj & G1_decision_obj_std & G1_mse & G1_mse_std & G1_true_benefit & G1_true_benefit_std & fairness & fairness_std & mse & mse_std & regret & regret_std & training_time & training_time_std \\\\\n",
      "\\midrule\n",
      "True & closed-form & 0.5000 & 0.0000 & mad & 42.8856 & 0.3811 & 328.5613 & 0.6472 & 11.9597 & 0.0000 & 326.1791 & 3.3642 & 473.3566 & 0.6208 & 16.2677 & 0.0000 & 72.3976 & 0.0132 & 345.8209 & 0.6441 & 0.0338 & 0.0004 & 274.6215 & 13.9728 \\\\\n",
      "True & closed-form & 0.8000 & 0.0000 & mad & 19.4207 & 0.0006 & 335.5806 & 0.1105 & 11.9597 & 0.0000 & 162.4401 & 0.0228 & 493.0235 & 0.1669 & 16.2677 & 0.0000 & 78.7214 & 0.0282 & 354.3478 & 0.1172 & 0.0007 & 0.0000 & 264.7722 & 0.5807 \\\\\n",
      "True & closed-form & 1.5000 & 0.0000 & mad & 16.7920 & 0.0027 & 321.0090 & 0.0649 & 11.9597 & 0.0000 & 55.1753 & 0.0672 & 481.8168 & 0.0908 & 16.2677 & 0.0000 & 80.4039 & 0.0779 & 340.1772 & 0.0463 & 0.0039 & 0.0001 & 274.7976 & 9.4328 \\\\\n",
      "True & closed-form & 2.0000 & 0.0000 & mad & 16.5952 & 0.0130 & 321.2119 & 0.9703 & 11.9597 & 0.0000 & 21.6078 & 0.0297 & 479.4285 & 1.4895 & 16.2677 & 0.0000 & 79.1083 & 0.2596 & 340.0714 & 1.0322 & 0.0546 & 0.0022 & 294.2979 & 3.5094 \\\\\n",
      "True & closed-form & 0.5000 & 1.0000 & mad & 37.0175 & 0.3397 & 252.9380 & 0.0975 & 11.9597 & 0.0000 & 254.4055 & 1.5530 & 320.9912 & 0.9667 & 16.2677 & 0.0000 & 34.0266 & 0.4346 & 261.0499 & 0.2011 & 0.1029 & 0.0019 & 331.2092 & 43.2095 \\\\\n",
      "True & closed-form & 0.8000 & 1.0000 & mad & 20.0244 & 0.0328 & 253.1224 & 0.2034 & 11.9597 & 0.0000 & 166.4415 & 0.4693 & 321.1023 & 0.8348 & 16.2677 & 0.0000 & 33.9900 & 0.3157 & 261.2256 & 0.2787 & 0.0020 & 0.0001 & 387.4786 & 19.5194 \\\\\n",
      "True & closed-form & 1.5000 & 1.0000 & mad & 18.1650 & 0.0180 & 240.2374 & 0.9139 & 11.9597 & 0.0000 & 60.6730 & 0.0367 & 304.4114 & 1.9688 & 16.2677 & 0.0000 & 32.0870 & 0.5275 & 247.8869 & 1.0396 & 0.0261 & 0.0004 & 394.9320 & 45.0305 \\\\\n",
      "True & closed-form & 2.0000 & 1.0000 & mad & 17.4279 & 0.0290 & 231.5040 & 0.9362 & 11.9597 & 0.0000 & 22.8619 & 1.9791 & 284.1538 & 0.6932 & 16.2677 & 0.0000 & 26.3249 & 0.1215 & 237.7798 & 0.9072 & 0.1297 & 0.0046 & 259.5163 & 1.5443 \\\\\n",
      "True & closed-form & 0.5000 & 1.0000 & atkinson & 42.3697 & 0.0371 & 318.5927 & 0.6685 & 11.9597 & 0.0000 & 328.4026 & 2.1837 & 445.8398 & 0.1179 & 16.2677 & 0.0000 & 0.0034 & 0.0001 & 333.7605 & 0.5748 & 0.0367 & 0.0003 & 257.1827 & 0.1170 \\\\\n",
      "True & closed-form & 0.8000 & 1.0000 & atkinson & 19.5459 & 0.0090 & 284.6656 & 0.9985 & 11.9597 & 0.0000 & 166.6878 & 0.1942 & 359.9256 & 1.6555 & 16.2677 & 0.0000 & 0.0016 & 0.0000 & 293.6366 & 1.0769 & 0.0012 & 0.0000 & 258.3284 & 0.1172 \\\\\n",
      "True & closed-form & 1.5000 & 1.0000 & atkinson & 16.7946 & 0.0024 & 320.9521 & 0.0986 & 11.9597 & 0.0000 & 55.1699 & 0.0685 & 481.4373 & 0.0310 & 16.2677 & 0.0000 & 0.0050 & 0.0000 & 340.0819 & 0.0831 & 0.0039 & 0.0001 & 258.4942 & 0.4402 \\\\\n",
      "True & closed-form & 2.0000 & 1.0000 & atkinson & 16.5958 & 0.0125 & 321.2138 & 0.9748 & 11.9597 & 0.0000 & 21.6084 & 0.0291 & 479.4019 & 1.4890 & 16.2677 & 0.0000 & 0.0049 & 0.0000 & 340.0698 & 1.0361 & 0.0546 & 0.0022 & 261.8514 & 1.1858 \\\\\n",
      "True & finite-diff & 0.5000 & 0.0000 & mad & 45.6962 & 0.3394 & 316.9308 & 1.1301 & 11.9597 & 0.0000 & 262.8155 & 2.4480 & 480.0041 & 1.2836 & 16.2677 & 0.0000 & 81.5366 & 0.0767 & 336.3692 & 1.1484 & 0.0090 & 0.0000 & 82.2301 & 0.0017 \\\\\n",
      "True & finite-diff & 0.8000 & 0.0000 & mad & 21.0947 & 0.0140 & 320.7894 & 7.5266 & 11.9597 & 0.0000 & 171.8365 & 0.0082 & 480.8821 & 10.2980 & 16.2677 & 0.0000 & 80.0464 & 1.3857 & 339.8725 & 7.8570 & 0.0002 & 0.0000 & 142.1575 & 0.0740 \\\\\n",
      "True & finite-diff & 1.5000 & 0.0000 & mad & 16.7965 & 0.0088 & 318.6087 & 3.4020 & 11.9597 & 0.0000 & 55.1737 & 0.0147 & 478.0143 & 5.2618 & 16.2677 & 0.0000 & 79.7028 & 0.9299 & 337.6098 & 3.6237 & 0.0040 & 0.0001 & 143.1616 & 0.2153 \\\\\n",
      "True & finite-diff & 2.0000 & 0.0000 & mad & 16.6246 & 0.0153 & 322.6404 & 0.1102 & 11.9597 & 0.0000 & 21.6179 & 0.0321 & 481.6442 & 0.0302 & 16.2677 & 0.0000 & 79.5019 & 0.0400 & 341.5937 & 0.1006 & 0.0550 & 0.0017 & 98.5866 & 0.1698 \\\\\n",
      "True & finite-diff & 0.5000 & 1.0000 & mad & 34.1190 & 0.1740 & 218.6761 & 2.5096 & 11.9597 & 0.0000 & 249.6451 & 0.0148 & 278.2047 & 2.6493 & 16.2677 & 0.0000 & 29.7643 & 0.0699 & 225.7719 & 2.5263 & 0.0394 & 0.0003 & 82.5194 & 0.3130 \\\\\n",
      "True & finite-diff & 0.8000 & 1.0000 & mad & NaN & NaN & NaN & NaN & 11.9597 & 0.0000 & NaN & NaN & NaN & NaN & 16.2677 & 0.0000 & NaN & NaN & NaN & NaN & NaN & NaN & 131.1208 & 0.2122 \\\\\n",
      "True & finite-diff & 1.5000 & 1.0000 & mad & 18.1645 & 0.0098 & 240.5585 & 0.6433 & 11.9597 & 0.0000 & 60.6257 & 0.4919 & 304.6706 & 1.8239 & 16.2677 & 0.0000 & 32.0561 & 0.5903 & 248.2006 & 0.7841 & 0.0260 & 0.0001 & 142.8273 & 0.1701 \\\\\n",
      "True & finite-diff & 2.0000 & 1.0000 & mad & 17.4487 & 0.0153 & 231.4427 & 1.7614 & 11.9597 & 0.0000 & 22.6069 & 1.5404 & 284.2220 & 1.8006 & 16.2677 & 0.0000 & 26.3896 & 0.0196 & 237.7340 & 1.7661 & 0.1299 & 0.0035 & 100.3624 & 0.2918 \\\\\n",
      "True & finite-diff & 0.5000 & 1.0000 & atkinson & 45.6543 & 0.1977 & 316.5173 & 2.2561 & 11.9597 & 0.0000 & 262.4419 & 3.0586 & 479.5184 & 2.8394 & 16.2677 & 0.0000 & 0.0053 & 0.0000 & 335.9470 & 2.3256 & 0.0090 & 0.0000 & 82.2052 & 0.2926 \\\\\n",
      "True & finite-diff & 0.8000 & 1.0000 & atkinson & 20.9205 & 0.1051 & 327.2650 & 7.6934 & 11.9597 & 0.0000 & 170.8172 & 0.0211 & 482.8182 & 13.4402 & 16.2677 & 0.0000 & 0.0046 & 0.0001 & 345.8069 & 8.3784 & 0.0002 & 0.0000 & 142.0392 & 0.8862 \\\\\n",
      "True & finite-diff & 1.5000 & 1.0000 & atkinson & 16.8243 & 0.0112 & 320.2543 & 2.4771 & 11.9597 & 0.0000 & 55.1938 & 0.1253 & 480.4486 & 2.9810 & 16.2677 & 0.0000 & 0.0050 & 0.0000 & 339.3494 & 2.5371 & 0.0040 & 0.0001 & 142.9198 & 0.0375 \\\\\n",
      "True & finite-diff & 2.0000 & 1.0000 & atkinson & 16.6304 & 0.0141 & 323.4492 & 0.2345 & 11.9597 & 0.0000 & 21.5779 & 0.0324 & 482.5302 & 0.5146 & 16.2677 & 0.0000 & 0.0049 & 0.0000 & 342.4117 & 0.2678 & 0.0548 & 0.0021 & 98.4964 & 0.3056 \\\\\n",
      "False & closed-form & 0.5000 & 0.0000 & mad & 27.0203 & 4.7743 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.4148 & 193.2612 & 545.3249 & 0.0133 & 16.2677 & 0.0000 & 524.8734 & 0.0088 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.5357 & 0.0050 \\\\\n",
      "False & closed-form & 0.8000 & 0.0000 & mad & 27.6426 & 0.0480 & 364.9638 & 0.0024 & 11.9597 & 0.0000 & 51.1901 & 1.5952 & 544.9433 & 0.0320 & 16.2677 & 0.0000 & 524.6499 & 0.0116 & 386.4174 & 0.0060 & 0.0145 & 0.0004 & 2.5588 & 0.0197 \\\\\n",
      "False & closed-form & 1.5000 & 0.0000 & mad & 21.0484 & 0.0193 & 302.4423 & 10.3903 & 11.9597 & 0.0000 & 33.8340 & 0.2519 & 397.7625 & 9.6407 & 16.2677 & 0.0000 & 302.6636 & 5.2222 & 313.8045 & 10.3010 & 0.0964 & 0.0013 & 2.6039 & 0.0393 \\\\\n",
      "False & closed-form & 2.0000 & 0.0000 & mad & 18.3818 & 0.0082 & 250.8123 & 2.9817 & 11.9597 & 0.0000 & 33.4390 & 1.3831 & 342.0845 & 2.9063 & 16.2677 & 0.0000 & 264.9769 & 2.5857 & 261.6920 & 2.9727 & 0.4773 & 0.0007 & 2.4918 & 0.0082 \\\\\n",
      "False & closed-form & 0.5000 & 1.0000 & mad & 27.0095 & 4.6895 & 365.0988 & 0.0074 & 11.9597 & 0.0000 & 377.6530 & 190.0825 & 545.3247 & 0.0124 & 16.2677 & 0.0000 & 524.8734 & 0.0087 & 386.5817 & 0.0081 & 0.3718 & 0.0390 & 2.5770 & 0.0418 \\\\\n",
      "False & closed-form & 0.8000 & 1.0000 & mad & 27.6496 & 0.0384 & 364.9718 & 0.0040 & 11.9597 & 0.0000 & 51.2027 & 1.6496 & 544.9522 & 0.0144 & 16.2677 & 0.0000 & 524.6620 & 0.0106 & 386.4255 & 0.0053 & 0.0144 & 0.0004 & 2.5638 & 0.0138 \\\\\n",
      "False & closed-form & 1.5000 & 1.0000 & mad & 21.1076 & 0.0210 & 255.9634 & 7.6713 & 11.9597 & 0.0000 & 32.4676 & 0.1848 & 310.4863 & 2.6567 & 16.2677 & 0.0000 & 228.1478 & 9.0906 & 262.4625 & 7.0735 & 0.0719 & 0.0007 & 2.5781 & 0.0445 \\\\\n",
      "False & closed-form & 2.0000 & 1.0000 & mad & 18.3872 & 0.0628 & 184.9535 & 0.9648 & 11.9597 & 0.0000 & 32.4145 & 2.3101 & 231.7614 & 7.1128 & 16.2677 & 0.0000 & 157.3063 & 2.2706 & 190.5330 & 1.6976 & 0.3268 & 0.0020 & 2.5676 & 0.0169 \\\\\n",
      "False & closed-form & 0.5000 & 1.0000 & atkinson & 27.0203 & 4.7743 & 365.0988 & 0.0073 & 11.9597 & 0.0000 & 379.4149 & 193.2611 & 545.3249 & 0.0133 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.5817 & 0.0081 & 0.3717 & 0.0394 & 2.5782 & 0.0020 \\\\\n",
      "False & closed-form & 0.8000 & 1.0000 & atkinson & 27.6425 & 0.0481 & 364.9638 & 0.0024 & 11.9597 & 0.0000 & 51.1904 & 1.5955 & 544.9434 & 0.0322 & 16.2677 & 0.0000 & 0.5977 & 0.0000 & 386.4173 & 0.0060 & 0.0145 & 0.0004 & 2.5807 & 0.0339 \\\\\n",
      "False & closed-form & 1.5000 & 1.0000 & atkinson & 21.0487 & 0.0193 & 302.5021 & 10.4118 & 11.9597 & 0.0000 & 33.8343 & 0.2523 & 397.8102 & 9.6692 & 16.2677 & 0.0000 & 0.3699 & 0.0001 & 313.8628 & 10.3233 & 0.0965 & 0.0013 & 2.5327 & 0.0133 \\\\\n",
      "False & closed-form & 2.0000 & 1.0000 & atkinson & 18.3828 & 0.0081 & 250.8910 & 2.9904 & 11.9597 & 0.0000 & 33.4433 & 1.3908 & 342.0882 & 2.8931 & 16.2677 & 0.0000 & 0.3742 & 0.0027 & 261.7616 & 2.9788 & 0.4775 & 0.0007 & 2.5879 & 0.0171 \\\\\n",
      "False & finite-diff & 0.5000 & 0.0000 & mad & 60.8601 & 0.5214 & 313.7988 & 0.7981 & 11.9597 & 0.0000 & 115.0959 & 0.9428 & 475.1935 & 0.9730 & 16.2677 & 0.0000 & 451.5599 & 1.1338 & 333.0371 & 0.8189 & 0.0169 & 0.0000 & 27.6967 & 0.1043 \\\\\n",
      "False & finite-diff & 0.8000 & 0.0000 & mad & 30.5726 & 0.1698 & 324.6804 & 4.0592 & 11.9597 & 0.0000 & 52.4475 & 0.3794 & 474.8340 & 8.6785 & 16.2677 & 0.0000 & 469.7292 & 6.3390 & 342.5787 & 4.6098 & 0.0023 & 0.0001 & 48.1953 & 0.0854 \\\\\n",
      "False & finite-diff & 1.5000 & 0.0000 & mad & 19.3136 & 0.0544 & 300.4407 & 7.8357 & 11.9597 & 0.0000 & 26.5513 & 0.0585 & 446.0106 & 13.2296 & 16.2677 & 0.0000 & 434.1899 & 10.6888 & 317.7926 & 8.4787 & 0.0101 & 0.0009 & 48.2490 & 0.0148 \\\\\n",
      "False & finite-diff & 2.0000 & 0.0000 & mad & 16.6029 & 0.0044 & 320.4697 & 0.6150 & 11.9597 & 0.0000 & 21.6067 & 0.0150 & 478.2328 & 1.1002 & 16.2677 & 0.0000 & 460.8938 & 0.6321 & 339.2751 & 0.6729 & 0.0552 & 0.0023 & 33.2053 & 0.2263 \\\\\n",
      "False & finite-diff & 0.5000 & 1.0000 & mad & 61.1836 & 0.0156 & 308.2131 & 4.9050 & 11.9597 & 0.0000 & 115.0043 & 0.4448 & 466.4866 & 6.4424 & 16.2677 & 0.0000 & 443.2348 & 6.9876 & 327.0793 & 5.0883 & 0.0169 & 0.0001 & 27.1967 & 0.0571 \\\\\n",
      "False & finite-diff & 0.8000 & 1.0000 & mad & 30.9468 & 0.0893 & 319.1278 & 25.1609 & 11.9597 & 0.0000 & 53.9852 & 0.5975 & 456.3080 & 48.0094 & 16.2677 & 0.0000 & 458.2619 & 36.2215 & 335.4796 & 27.8845 & 0.0023 & 0.0004 & 47.4289 & 0.0916 \\\\\n",
      "False & finite-diff & 1.5000 & 1.0000 & mad & 20.3225 & 0.1266 & 142.8367 & 3.7483 & 11.9597 & 0.0000 & 34.6209 & 0.1222 & 191.5771 & 0.4490 & 16.2677 & 0.0000 & 204.5466 & 8.1353 & 148.6466 & 3.3550 & 0.0351 & 0.0010 & 46.6333 & 0.0903 \\\\\n",
      "False & finite-diff & 2.0000 & 1.0000 & mad & 16.5692 & 0.0041 & 134.7895 & 0.6655 & 11.9597 & 0.0000 & 75.7395 & 0.6073 & 182.2012 & 0.3064 & 16.2677 & 0.0000 & 183.6403 & 2.1220 & 140.4409 & 0.6227 & 0.3382 & 0.0050 & 32.7783 & 0.0542 \\\\\n",
      "False & finite-diff & 0.5000 & 1.0000 & atkinson & 60.7777 & 0.6888 & 318.6804 & 1.5937 & 11.9597 & 0.0000 & 115.2031 & 1.2761 & 482.4207 & 2.4763 & 16.2677 & 0.0000 & 0.5986 & 0.0004 & 338.1983 & 1.6989 & 0.0170 & 0.0005 & 27.1511 & 0.0236 \\\\\n",
      "False & finite-diff & 0.8000 & 1.0000 & atkinson & 30.4439 & 0.4061 & 313.0548 & 8.4626 & 11.9597 & 0.0000 & 52.3319 & 1.4543 & 457.0742 & 11.5570 & 16.2677 & 0.0000 & 0.6162 & 0.0118 & 330.2219 & 8.8315 & 0.0026 & 0.0004 & 47.6979 & 0.1867 \\\\\n",
      "False & finite-diff & 1.5000 & 1.0000 & atkinson & 19.4147 & 0.1289 & 308.1300 & 17.3450 & 11.9597 & 0.0000 & 26.8550 & 0.1579 & 459.8316 & 27.3238 & 16.2677 & 0.0000 & 0.6049 & 0.0051 & 326.2128 & 18.5345 & 0.0093 & 0.0011 & 47.3386 & 0.0124 \\\\\n",
      "False & finite-diff & 2.0000 & 1.0000 & atkinson & 16.6297 & 0.0168 & 319.6783 & 1.6795 & 11.9597 & 0.0000 & 21.6343 & 0.0213 & 476.9326 & 2.8796 & 16.2677 & 0.0000 & 0.5997 & 0.0005 & 338.4230 & 1.8226 & 0.0556 & 0.0026 & 32.8200 & 0.0405 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hyperparameter Grid Definition ---\n",
    "alphas = [0.5, 0.8, 1.5, 2]\n",
    "fairness_lambdas = [0, 1.0]\n",
    "group_settings = [True, False]\n",
    "grad_methods = ['closed-form','finite-diff'] # New parameter\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  GRID-SEARCH HARNESS  (only the inner loop changed)\n",
    "# ---------------------------------------------------------------------\n",
    "results_list = []\n",
    "\n",
    "for group in group_settings:\n",
    "    fairness_types = ['mad', 'atkinson']           # keep acc_parity only if λ>0\n",
    "    for grad_method in grad_methods:\n",
    "        for lam in fairness_lambdas:\n",
    "            for fairness in fairness_types:\n",
    "                if lam == 0 and fairness != fairness_types[0]:\n",
    "                    continue                       # skip unattainable combos\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    run_params = {\n",
    "                        'Group': group,\n",
    "                        'Grad Method': grad_method,\n",
    "                        'Alpha': alpha,\n",
    "                        'Lambda': lam,\n",
    "                        'Fairness': fairness\n",
    "                    }\n",
    "                    print(\"\\n\" + \"-\"*70)\n",
    "                    print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                    print(\"-\"*70)\n",
    "\n",
    "                    train_args = dict(\n",
    "                        X_train=feats_train, y_train=b_train, race_train=race_train,\n",
    "                        cost_train=cost_train, gainF_train=gainF_train,\n",
    "                        X_test=feats_test,  y_test=b_test,  race_test=race_test,\n",
    "                        cost_test=cost_test, gainF_test=gainF_test,\n",
    "                        model_class=FairRiskPredictor,\n",
    "                        input_dim=feats_train.shape[1],\n",
    "                        alpha=alpha, Q=Q,\n",
    "                        lambda_fair=lam, fairness_type=fairness,\n",
    "                        group=group, grad_method=grad_method,\n",
    "                        num_epochs=50, lr=0.003\n",
    "                    )\n",
    "\n",
    "                    avg_results = train_many_trials_regret(\n",
    "                        n_trials=2, **train_args)\n",
    "\n",
    "                    # ---------------- build DataFrame row ------------\n",
    "                    row = run_params.copy()\n",
    "                    row.update(avg_results)          # every metric goes in\n",
    "                    results_list.append(row)\n",
    "\n",
    "# ---------------- DataFrame & LaTeX dump -----------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Put the hyper-parameters first; everything else follows automatically\n",
    "hp_cols = ['Group', 'Grad Method', 'Alpha', 'Lambda', 'Fairness']\n",
    "other_cols = sorted([c for c in results_df.columns if c not in hp_cols])\n",
    "results_df = results_df[hp_cols + other_cols]\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Averaged Experimental Results Across Different Parameters.\",\n",
    "    label=\"tab:avg_exp_results_expanded\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                           GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None,\n",
    "                       'display.width', 1200):\n",
    "    print(results_df)\n",
    "\n",
    "print(\"\\n--- LaTeX Table Output ---\")\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9440c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"20250702results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da20a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Grad Method</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Fairness</th>\n",
       "      <th>G0_decision_obj</th>\n",
       "      <th>G0_decision_obj_std</th>\n",
       "      <th>G0_mse</th>\n",
       "      <th>G0_mse_std</th>\n",
       "      <th>G0_true_benefit</th>\n",
       "      <th>...</th>\n",
       "      <th>G1_true_benefit</th>\n",
       "      <th>G1_true_benefit_std</th>\n",
       "      <th>fairness</th>\n",
       "      <th>fairness_std</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_std</th>\n",
       "      <th>regret</th>\n",
       "      <th>regret_std</th>\n",
       "      <th>training_time</th>\n",
       "      <th>training_time_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>42.885602</td>\n",
       "      <td>0.381066</td>\n",
       "      <td>328.561310</td>\n",
       "      <td>0.647247</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.397644</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>345.820892</td>\n",
       "      <td>0.644104</td>\n",
       "      <td>0.033816</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>274.621546</td>\n",
       "      <td>13.972836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>19.420700</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>335.580627</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.721420</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>354.347839</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>264.772162</td>\n",
       "      <td>0.580686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.792012</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>321.008957</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.403923</td>\n",
       "      <td>0.077858</td>\n",
       "      <td>340.177216</td>\n",
       "      <td>0.046326</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>274.797599</td>\n",
       "      <td>9.432766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.595218</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>321.211945</td>\n",
       "      <td>0.970337</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.108269</td>\n",
       "      <td>0.259560</td>\n",
       "      <td>340.071350</td>\n",
       "      <td>1.032196</td>\n",
       "      <td>0.054595</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>294.297873</td>\n",
       "      <td>3.509423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>37.017497</td>\n",
       "      <td>0.339698</td>\n",
       "      <td>252.937981</td>\n",
       "      <td>0.097527</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.026615</td>\n",
       "      <td>0.434605</td>\n",
       "      <td>261.049896</td>\n",
       "      <td>0.201141</td>\n",
       "      <td>0.102945</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>331.209196</td>\n",
       "      <td>43.209460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>20.024442</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>253.122360</td>\n",
       "      <td>0.203430</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.989975</td>\n",
       "      <td>0.315681</td>\n",
       "      <td>261.225571</td>\n",
       "      <td>0.278702</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>387.478635</td>\n",
       "      <td>19.519375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>18.165037</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>240.237396</td>\n",
       "      <td>0.913879</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.086990</td>\n",
       "      <td>0.527466</td>\n",
       "      <td>247.886940</td>\n",
       "      <td>1.039635</td>\n",
       "      <td>0.026125</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>394.932016</td>\n",
       "      <td>45.030531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>17.427934</td>\n",
       "      <td>0.028964</td>\n",
       "      <td>231.503967</td>\n",
       "      <td>0.936157</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.324921</td>\n",
       "      <td>0.121475</td>\n",
       "      <td>237.779831</td>\n",
       "      <td>0.907227</td>\n",
       "      <td>0.129690</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>259.516278</td>\n",
       "      <td>1.544331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>42.369655</td>\n",
       "      <td>0.037111</td>\n",
       "      <td>318.592682</td>\n",
       "      <td>0.668488</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>333.760513</td>\n",
       "      <td>0.574753</td>\n",
       "      <td>0.036726</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>257.182714</td>\n",
       "      <td>0.117035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>19.545886</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>284.665588</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>293.636581</td>\n",
       "      <td>1.076859</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>258.328434</td>\n",
       "      <td>0.117207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>16.794573</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>320.952057</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>340.081909</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>258.494168</td>\n",
       "      <td>0.440194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>16.595815</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>321.213776</td>\n",
       "      <td>0.974792</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>340.069809</td>\n",
       "      <td>1.036118</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>261.851391</td>\n",
       "      <td>1.185793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>45.696191</td>\n",
       "      <td>0.339362</td>\n",
       "      <td>316.930832</td>\n",
       "      <td>1.130142</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.536629</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>336.369171</td>\n",
       "      <td>1.148438</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>82.230106</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>21.094724</td>\n",
       "      <td>0.013952</td>\n",
       "      <td>320.789398</td>\n",
       "      <td>7.526611</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.046364</td>\n",
       "      <td>1.385704</td>\n",
       "      <td>339.872482</td>\n",
       "      <td>7.856979</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>142.157507</td>\n",
       "      <td>0.074044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.796529</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>318.608658</td>\n",
       "      <td>3.402023</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.702835</td>\n",
       "      <td>0.929901</td>\n",
       "      <td>337.609818</td>\n",
       "      <td>3.623703</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>143.161553</td>\n",
       "      <td>0.215291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.624645</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>322.640442</td>\n",
       "      <td>0.110168</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.501884</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>341.593689</td>\n",
       "      <td>0.100647</td>\n",
       "      <td>0.055019</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>98.586610</td>\n",
       "      <td>0.169780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>34.119008</td>\n",
       "      <td>0.173976</td>\n",
       "      <td>218.676117</td>\n",
       "      <td>2.509567</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.764313</td>\n",
       "      <td>0.069878</td>\n",
       "      <td>225.771950</td>\n",
       "      <td>2.526253</td>\n",
       "      <td>0.039385</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>82.519408</td>\n",
       "      <td>0.312963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.120757</td>\n",
       "      <td>0.212160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>18.164503</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>240.558472</td>\n",
       "      <td>0.643341</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.056053</td>\n",
       "      <td>0.590294</td>\n",
       "      <td>248.200638</td>\n",
       "      <td>0.784088</td>\n",
       "      <td>0.026018</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>142.827264</td>\n",
       "      <td>0.170143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>17.448669</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>231.442726</td>\n",
       "      <td>1.761421</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.389629</td>\n",
       "      <td>0.019604</td>\n",
       "      <td>237.734024</td>\n",
       "      <td>1.766098</td>\n",
       "      <td>0.129936</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>100.362417</td>\n",
       "      <td>0.291839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>45.654282</td>\n",
       "      <td>0.197665</td>\n",
       "      <td>316.517288</td>\n",
       "      <td>2.256058</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>335.947021</td>\n",
       "      <td>2.325592</td>\n",
       "      <td>0.009029</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>82.205243</td>\n",
       "      <td>0.292571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>20.920451</td>\n",
       "      <td>0.105124</td>\n",
       "      <td>327.265015</td>\n",
       "      <td>7.693390</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>345.806946</td>\n",
       "      <td>8.378387</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>142.039248</td>\n",
       "      <td>0.886154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>16.824349</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>320.254288</td>\n",
       "      <td>2.477066</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>339.349426</td>\n",
       "      <td>2.537140</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>142.919753</td>\n",
       "      <td>0.037468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>True</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>16.630424</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>323.449203</td>\n",
       "      <td>0.234451</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>342.411682</td>\n",
       "      <td>0.267822</td>\n",
       "      <td>0.054825</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>98.496365</td>\n",
       "      <td>0.305569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>27.020298</td>\n",
       "      <td>4.774341</td>\n",
       "      <td>365.098801</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.873383</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>386.581741</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.371726</td>\n",
       "      <td>0.039384</td>\n",
       "      <td>2.535680</td>\n",
       "      <td>0.005022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>27.642559</td>\n",
       "      <td>0.048028</td>\n",
       "      <td>364.963821</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.649872</td>\n",
       "      <td>0.011627</td>\n",
       "      <td>386.417358</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>2.558804</td>\n",
       "      <td>0.019719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>21.048405</td>\n",
       "      <td>0.019268</td>\n",
       "      <td>302.442307</td>\n",
       "      <td>10.390305</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.663559</td>\n",
       "      <td>5.222153</td>\n",
       "      <td>313.804459</td>\n",
       "      <td>10.300980</td>\n",
       "      <td>0.096445</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>2.603944</td>\n",
       "      <td>0.039312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>18.381802</td>\n",
       "      <td>0.008153</td>\n",
       "      <td>250.812347</td>\n",
       "      <td>2.981705</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.976929</td>\n",
       "      <td>2.585663</td>\n",
       "      <td>261.692001</td>\n",
       "      <td>2.972733</td>\n",
       "      <td>0.477345</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>2.491812</td>\n",
       "      <td>0.008226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>27.009521</td>\n",
       "      <td>4.689495</td>\n",
       "      <td>365.098755</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.873352</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>386.581696</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.371811</td>\n",
       "      <td>0.038960</td>\n",
       "      <td>2.577030</td>\n",
       "      <td>0.041822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>27.649586</td>\n",
       "      <td>0.038444</td>\n",
       "      <td>364.971848</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.662048</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>386.425507</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>2.563785</td>\n",
       "      <td>0.013844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>21.107574</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>255.963371</td>\n",
       "      <td>7.671272</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.147766</td>\n",
       "      <td>9.090637</td>\n",
       "      <td>262.462479</td>\n",
       "      <td>7.073532</td>\n",
       "      <td>0.071927</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>2.578070</td>\n",
       "      <td>0.044526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>18.387234</td>\n",
       "      <td>0.062823</td>\n",
       "      <td>184.953506</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.306343</td>\n",
       "      <td>2.270638</td>\n",
       "      <td>190.532982</td>\n",
       "      <td>1.697639</td>\n",
       "      <td>0.326789</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>2.567561</td>\n",
       "      <td>0.016870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>27.020294</td>\n",
       "      <td>4.774338</td>\n",
       "      <td>365.098801</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597660</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>386.581741</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.371726</td>\n",
       "      <td>0.039384</td>\n",
       "      <td>2.578232</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>27.642513</td>\n",
       "      <td>0.048074</td>\n",
       "      <td>364.963806</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597663</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>386.417343</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>2.580668</td>\n",
       "      <td>0.033908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>21.048674</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>302.502090</td>\n",
       "      <td>10.411819</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369924</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>313.862823</td>\n",
       "      <td>10.323334</td>\n",
       "      <td>0.096457</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>2.532691</td>\n",
       "      <td>0.013315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>False</td>\n",
       "      <td>closed-form</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>18.382778</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>250.890953</td>\n",
       "      <td>2.990410</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374190</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>261.761642</td>\n",
       "      <td>2.978806</td>\n",
       "      <td>0.477495</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>2.587912</td>\n",
       "      <td>0.017083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>60.860062</td>\n",
       "      <td>0.521421</td>\n",
       "      <td>313.798813</td>\n",
       "      <td>0.798080</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>451.559875</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>333.037079</td>\n",
       "      <td>0.818939</td>\n",
       "      <td>0.016853</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>27.696736</td>\n",
       "      <td>0.104324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>30.572609</td>\n",
       "      <td>0.169820</td>\n",
       "      <td>324.680405</td>\n",
       "      <td>4.059250</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469.729218</td>\n",
       "      <td>6.339020</td>\n",
       "      <td>342.578705</td>\n",
       "      <td>4.609833</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>48.195258</td>\n",
       "      <td>0.085395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>19.313580</td>\n",
       "      <td>0.054379</td>\n",
       "      <td>300.440689</td>\n",
       "      <td>7.835739</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.189926</td>\n",
       "      <td>10.688766</td>\n",
       "      <td>317.792633</td>\n",
       "      <td>8.478699</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>48.248995</td>\n",
       "      <td>0.014812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.602900</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>320.469681</td>\n",
       "      <td>0.615005</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>460.893814</td>\n",
       "      <td>0.632126</td>\n",
       "      <td>339.275055</td>\n",
       "      <td>0.672852</td>\n",
       "      <td>0.055237</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>33.205332</td>\n",
       "      <td>0.226325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>61.183563</td>\n",
       "      <td>0.015585</td>\n",
       "      <td>308.213089</td>\n",
       "      <td>4.905014</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.234848</td>\n",
       "      <td>6.987595</td>\n",
       "      <td>327.079269</td>\n",
       "      <td>5.088272</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>27.196665</td>\n",
       "      <td>0.057140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>30.946777</td>\n",
       "      <td>0.089279</td>\n",
       "      <td>319.127777</td>\n",
       "      <td>25.160950</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.261917</td>\n",
       "      <td>36.221481</td>\n",
       "      <td>335.479645</td>\n",
       "      <td>27.884491</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>47.428917</td>\n",
       "      <td>0.091623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>20.322514</td>\n",
       "      <td>0.126605</td>\n",
       "      <td>142.836723</td>\n",
       "      <td>3.748299</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>204.546577</td>\n",
       "      <td>8.135323</td>\n",
       "      <td>148.646591</td>\n",
       "      <td>3.355026</td>\n",
       "      <td>0.035089</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>46.633286</td>\n",
       "      <td>0.090254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>16.569235</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>134.789452</td>\n",
       "      <td>0.665474</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.640297</td>\n",
       "      <td>2.122047</td>\n",
       "      <td>140.440941</td>\n",
       "      <td>0.622673</td>\n",
       "      <td>0.338224</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>32.778311</td>\n",
       "      <td>0.054214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>60.777687</td>\n",
       "      <td>0.688784</td>\n",
       "      <td>318.680405</td>\n",
       "      <td>1.593704</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598605</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>338.198257</td>\n",
       "      <td>1.698898</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>27.151094</td>\n",
       "      <td>0.023568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>30.443924</td>\n",
       "      <td>0.406112</td>\n",
       "      <td>313.054825</td>\n",
       "      <td>8.462631</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616213</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>330.221924</td>\n",
       "      <td>8.831482</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>47.697885</td>\n",
       "      <td>0.186656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>19.414701</td>\n",
       "      <td>0.128948</td>\n",
       "      <td>308.130005</td>\n",
       "      <td>17.345032</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604894</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>326.212814</td>\n",
       "      <td>18.534500</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>47.338640</td>\n",
       "      <td>0.012407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>False</td>\n",
       "      <td>finite-diff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>16.629665</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>319.678314</td>\n",
       "      <td>1.679535</td>\n",
       "      <td>11.959665</td>\n",
       "      <td>...</td>\n",
       "      <td>16.267719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599662</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>338.423035</td>\n",
       "      <td>1.822571</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>32.820024</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group  Grad Method  Alpha  Lambda  Fairness  G0_decision_obj  \\\n",
       "0    True  closed-form    0.5     0.0       mad        42.885602   \n",
       "1    True  closed-form    0.8     0.0       mad        19.420700   \n",
       "2    True  closed-form    1.5     0.0       mad        16.792012   \n",
       "3    True  closed-form    2.0     0.0       mad        16.595218   \n",
       "4    True  closed-form    0.5     1.0       mad        37.017497   \n",
       "5    True  closed-form    0.8     1.0       mad        20.024442   \n",
       "6    True  closed-form    1.5     1.0       mad        18.165037   \n",
       "7    True  closed-form    2.0     1.0       mad        17.427934   \n",
       "8    True  closed-form    0.5     1.0  atkinson        42.369655   \n",
       "9    True  closed-form    0.8     1.0  atkinson        19.545886   \n",
       "10   True  closed-form    1.5     1.0  atkinson        16.794573   \n",
       "11   True  closed-form    2.0     1.0  atkinson        16.595815   \n",
       "12   True  finite-diff    0.5     0.0       mad        45.696191   \n",
       "13   True  finite-diff    0.8     0.0       mad        21.094724   \n",
       "14   True  finite-diff    1.5     0.0       mad        16.796529   \n",
       "15   True  finite-diff    2.0     0.0       mad        16.624645   \n",
       "16   True  finite-diff    0.5     1.0       mad        34.119008   \n",
       "17   True  finite-diff    0.8     1.0       mad              NaN   \n",
       "18   True  finite-diff    1.5     1.0       mad        18.164503   \n",
       "19   True  finite-diff    2.0     1.0       mad        17.448669   \n",
       "20   True  finite-diff    0.5     1.0  atkinson        45.654282   \n",
       "21   True  finite-diff    0.8     1.0  atkinson        20.920451   \n",
       "22   True  finite-diff    1.5     1.0  atkinson        16.824349   \n",
       "23   True  finite-diff    2.0     1.0  atkinson        16.630424   \n",
       "24  False  closed-form    0.5     0.0       mad        27.020298   \n",
       "25  False  closed-form    0.8     0.0       mad        27.642559   \n",
       "26  False  closed-form    1.5     0.0       mad        21.048405   \n",
       "27  False  closed-form    2.0     0.0       mad        18.381802   \n",
       "28  False  closed-form    0.5     1.0       mad        27.009521   \n",
       "29  False  closed-form    0.8     1.0       mad        27.649586   \n",
       "30  False  closed-form    1.5     1.0       mad        21.107574   \n",
       "31  False  closed-form    2.0     1.0       mad        18.387234   \n",
       "32  False  closed-form    0.5     1.0  atkinson        27.020294   \n",
       "33  False  closed-form    0.8     1.0  atkinson        27.642513   \n",
       "34  False  closed-form    1.5     1.0  atkinson        21.048674   \n",
       "35  False  closed-form    2.0     1.0  atkinson        18.382778   \n",
       "36  False  finite-diff    0.5     0.0       mad        60.860062   \n",
       "37  False  finite-diff    0.8     0.0       mad        30.572609   \n",
       "38  False  finite-diff    1.5     0.0       mad        19.313580   \n",
       "39  False  finite-diff    2.0     0.0       mad        16.602900   \n",
       "40  False  finite-diff    0.5     1.0       mad        61.183563   \n",
       "41  False  finite-diff    0.8     1.0       mad        30.946777   \n",
       "42  False  finite-diff    1.5     1.0       mad        20.322514   \n",
       "43  False  finite-diff    2.0     1.0       mad        16.569235   \n",
       "44  False  finite-diff    0.5     1.0  atkinson        60.777687   \n",
       "45  False  finite-diff    0.8     1.0  atkinson        30.443924   \n",
       "46  False  finite-diff    1.5     1.0  atkinson        19.414701   \n",
       "47  False  finite-diff    2.0     1.0  atkinson        16.629665   \n",
       "\n",
       "    G0_decision_obj_std      G0_mse  G0_mse_std  G0_true_benefit  ...  \\\n",
       "0              0.381066  328.561310    0.647247        11.959665  ...   \n",
       "1              0.000598  335.580627    0.110535        11.959665  ...   \n",
       "2              0.002661  321.008957    0.064896        11.959665  ...   \n",
       "3              0.012964  321.211945    0.970337        11.959665  ...   \n",
       "4              0.339698  252.937981    0.097527        11.959665  ...   \n",
       "5              0.032839  253.122360    0.203430        11.959665  ...   \n",
       "6              0.018044  240.237396    0.913879        11.959665  ...   \n",
       "7              0.028964  231.503967    0.936157        11.959665  ...   \n",
       "8              0.037111  318.592682    0.668488        11.959665  ...   \n",
       "9              0.008996  284.665588    0.998535        11.959665  ...   \n",
       "10             0.002382  320.952057    0.098572        11.959665  ...   \n",
       "11             0.012455  321.213776    0.974792        11.959665  ...   \n",
       "12             0.339362  316.930832    1.130142        11.959665  ...   \n",
       "13             0.013952  320.789398    7.526611        11.959665  ...   \n",
       "14             0.008838  318.608658    3.402023        11.959665  ...   \n",
       "15             0.015326  322.640442    0.110168        11.959665  ...   \n",
       "16             0.173976  218.676117    2.509567        11.959665  ...   \n",
       "17                  NaN         NaN         NaN        11.959665  ...   \n",
       "18             0.009834  240.558472    0.643341        11.959665  ...   \n",
       "19             0.015326  231.442726    1.761421        11.959665  ...   \n",
       "20             0.197665  316.517288    2.256058        11.959665  ...   \n",
       "21             0.105124  327.265015    7.693390        11.959665  ...   \n",
       "22             0.011203  320.254288    2.477066        11.959665  ...   \n",
       "23             0.014119  323.449203    0.234451        11.959665  ...   \n",
       "24             4.774341  365.098801    0.007339        11.959665  ...   \n",
       "25             0.048028  364.963821    0.002426        11.959665  ...   \n",
       "26             0.019268  302.442307   10.390305        11.959665  ...   \n",
       "27             0.008153  250.812347    2.981705        11.959665  ...   \n",
       "28             4.689495  365.098755    0.007446        11.959665  ...   \n",
       "29             0.038444  364.971848    0.004044        11.959665  ...   \n",
       "30             0.020972  255.963371    7.671272        11.959665  ...   \n",
       "31             0.062823  184.953506    0.964783        11.959665  ...   \n",
       "32             4.774338  365.098801    0.007339        11.959665  ...   \n",
       "33             0.048074  364.963806    0.002411        11.959665  ...   \n",
       "34             0.019276  302.502090   10.411819        11.959665  ...   \n",
       "35             0.008149  250.890953    2.990410        11.959665  ...   \n",
       "36             0.521421  313.798813    0.798080        11.959665  ...   \n",
       "37             0.169820  324.680405    4.059250        11.959665  ...   \n",
       "38             0.054379  300.440689    7.835739        11.959665  ...   \n",
       "39             0.004362  320.469681    0.615005        11.959665  ...   \n",
       "40             0.015585  308.213089    4.905014        11.959665  ...   \n",
       "41             0.089279  319.127777   25.160950        11.959665  ...   \n",
       "42             0.126605  142.836723    3.748299        11.959665  ...   \n",
       "43             0.004082  134.789452    0.665474        11.959665  ...   \n",
       "44             0.688784  318.680405    1.593704        11.959665  ...   \n",
       "45             0.406112  313.054825    8.462631        11.959665  ...   \n",
       "46             0.128948  308.130005   17.345032        11.959665  ...   \n",
       "47             0.016752  319.678314    1.679535        11.959665  ...   \n",
       "\n",
       "    G1_true_benefit  G1_true_benefit_std    fairness  fairness_std  \\\n",
       "0         16.267719                  0.0   72.397644      0.013245   \n",
       "1         16.267719                  0.0   78.721420      0.028168   \n",
       "2         16.267719                  0.0   80.403923      0.077858   \n",
       "3         16.267719                  0.0   79.108269      0.259560   \n",
       "4         16.267719                  0.0   34.026615      0.434605   \n",
       "5         16.267719                  0.0   33.989975      0.315681   \n",
       "6         16.267719                  0.0   32.086990      0.527466   \n",
       "7         16.267719                  0.0   26.324921      0.121475   \n",
       "8         16.267719                  0.0    0.003355      0.000050   \n",
       "9         16.267719                  0.0    0.001576      0.000015   \n",
       "10        16.267719                  0.0    0.005005      0.000010   \n",
       "11        16.267719                  0.0    0.004873      0.000002   \n",
       "12        16.267719                  0.0   81.536629      0.076714   \n",
       "13        16.267719                  0.0   80.046364      1.385704   \n",
       "14        16.267719                  0.0   79.702835      0.929901   \n",
       "15        16.267719                  0.0   79.501884      0.040001   \n",
       "16        16.267719                  0.0   29.764313      0.069878   \n",
       "17        16.267719                  0.0         NaN           NaN   \n",
       "18        16.267719                  0.0   32.056053      0.590294   \n",
       "19        16.267719                  0.0   26.389629      0.019604   \n",
       "20        16.267719                  0.0    0.005272      0.000033   \n",
       "21        16.267719                  0.0    0.004575      0.000109   \n",
       "22        16.267719                  0.0    0.005009      0.000040   \n",
       "23        16.267719                  0.0    0.004862      0.000009   \n",
       "24        16.267719                  0.0  524.873383      0.008759   \n",
       "25        16.267719                  0.0  524.649872      0.011627   \n",
       "26        16.267719                  0.0  302.663559      5.222153   \n",
       "27        16.267719                  0.0  264.976929      2.585663   \n",
       "28        16.267719                  0.0  524.873352      0.008728   \n",
       "29        16.267719                  0.0  524.662048      0.010620   \n",
       "30        16.267719                  0.0  228.147766      9.090637   \n",
       "31        16.267719                  0.0  157.306343      2.270638   \n",
       "32        16.267719                  0.0    0.597660      0.000005   \n",
       "33        16.267719                  0.0    0.597663      0.000006   \n",
       "34        16.267719                  0.0    0.369924      0.000108   \n",
       "35        16.267719                  0.0    0.374190      0.002711   \n",
       "36        16.267719                  0.0  451.559875      1.133789   \n",
       "37        16.267719                  0.0  469.729218      6.339020   \n",
       "38        16.267719                  0.0  434.189926     10.688766   \n",
       "39        16.267719                  0.0  460.893814      0.632126   \n",
       "40        16.267719                  0.0  443.234848      6.987595   \n",
       "41        16.267719                  0.0  458.261917     36.221481   \n",
       "42        16.267719                  0.0  204.546577      8.135323   \n",
       "43        16.267719                  0.0  183.640297      2.122047   \n",
       "44        16.267719                  0.0    0.598605      0.000386   \n",
       "45        16.267719                  0.0    0.616213      0.011800   \n",
       "46        16.267719                  0.0    0.604894      0.005123   \n",
       "47        16.267719                  0.0    0.599662      0.000497   \n",
       "\n",
       "           mse    mse_std    regret  regret_std  training_time  \\\n",
       "0   345.820892   0.644104  0.033816    0.000394     274.621546   \n",
       "1   354.347839   0.117249  0.000651    0.000001     264.772162   \n",
       "2   340.177216   0.046326  0.003886    0.000069     274.797599   \n",
       "3   340.071350   1.032196  0.054595    0.002242     294.297873   \n",
       "4   261.049896   0.201141  0.102945    0.001905     331.209196   \n",
       "5   261.225571   0.278702  0.002046    0.000063     387.478635   \n",
       "6   247.886940   1.039635  0.026125    0.000396     394.932016   \n",
       "7   237.779831   0.907227  0.129690    0.004618     259.516278   \n",
       "8   333.760513   0.574753  0.036726    0.000284     257.182714   \n",
       "9   293.636581   1.076859  0.001236    0.000009     258.328434   \n",
       "10  340.081909   0.083130  0.003886    0.000066     258.494168   \n",
       "11  340.069809   1.036118  0.054593    0.002246     261.851391   \n",
       "12  336.369171   1.148438  0.009046    0.000047      82.230106   \n",
       "13  339.872482   7.856979  0.000191    0.000002     142.157507   \n",
       "14  337.609818   3.623703  0.003959    0.000134     143.161553   \n",
       "15  341.593689   0.100647  0.055019    0.001653      98.586610   \n",
       "16  225.771950   2.526253  0.039385    0.000287      82.519408   \n",
       "17         NaN        NaN       NaN         NaN     131.120757   \n",
       "18  248.200638   0.784088  0.026018    0.000125     142.827264   \n",
       "19  237.734024   1.766098  0.129936    0.003492     100.362417   \n",
       "20  335.947021   2.325592  0.009029    0.000010      82.205243   \n",
       "21  345.806946   8.378387  0.000210    0.000020     142.039248   \n",
       "22  339.349426   2.537140  0.003988    0.000090     142.919753   \n",
       "23  342.411682   0.267822  0.054825    0.002085      98.496365   \n",
       "24  386.581741   0.008072  0.371726    0.039384       2.535680   \n",
       "25  386.417358   0.006012  0.014549    0.000363       2.558804   \n",
       "26  313.804459  10.300980  0.096445    0.001309       2.603944   \n",
       "27  261.692001   2.972733  0.477345    0.000734       2.491812   \n",
       "28  386.581696   0.008057  0.371811    0.038960       2.577030   \n",
       "29  386.425507   0.005280  0.014370    0.000426       2.563785   \n",
       "30  262.462479   7.073532  0.071927    0.000692       2.578070   \n",
       "31  190.532982   1.697639  0.326789    0.001970       2.567561   \n",
       "32  386.581741   0.008072  0.371726    0.039384       2.578232   \n",
       "33  386.417343   0.005997  0.014549    0.000363       2.580668   \n",
       "34  313.862823  10.323334  0.096457    0.001316       2.532691   \n",
       "35  261.761642   2.978806  0.477495    0.000716       2.587912   \n",
       "36  333.037079   0.818939  0.016853    0.000048      27.696736   \n",
       "37  342.578705   4.609833  0.002309    0.000051      48.195258   \n",
       "38  317.792633   8.478699  0.010092    0.000887      48.248995   \n",
       "39  339.275055   0.672852  0.055237    0.002325      33.205332   \n",
       "40  327.079269   5.088272  0.016931    0.000137      27.196665   \n",
       "41  335.479645  27.884491  0.002301    0.000400      47.428917   \n",
       "42  148.646591   3.355026  0.035089    0.001019      46.633286   \n",
       "43  140.440941   0.622673  0.338224    0.005040      32.778311   \n",
       "44  338.198257   1.698898  0.016989    0.000457      27.151094   \n",
       "45  330.221924   8.831482  0.002625    0.000416      47.697885   \n",
       "46  326.212814  18.534500  0.009272    0.001123      47.338640   \n",
       "47  338.423035   1.822571  0.055586    0.002555      32.820024   \n",
       "\n",
       "    training_time_std  \n",
       "0           13.972836  \n",
       "1            0.580686  \n",
       "2            9.432766  \n",
       "3            3.509423  \n",
       "4           43.209460  \n",
       "5           19.519375  \n",
       "6           45.030531  \n",
       "7            1.544331  \n",
       "8            0.117035  \n",
       "9            0.117207  \n",
       "10           0.440194  \n",
       "11           1.185793  \n",
       "12           0.001729  \n",
       "13           0.074044  \n",
       "14           0.215291  \n",
       "15           0.169780  \n",
       "16           0.312963  \n",
       "17           0.212160  \n",
       "18           0.170143  \n",
       "19           0.291839  \n",
       "20           0.292571  \n",
       "21           0.886154  \n",
       "22           0.037468  \n",
       "23           0.305569  \n",
       "24           0.005022  \n",
       "25           0.019719  \n",
       "26           0.039312  \n",
       "27           0.008226  \n",
       "28           0.041822  \n",
       "29           0.013844  \n",
       "30           0.044526  \n",
       "31           0.016870  \n",
       "32           0.001975  \n",
       "33           0.033908  \n",
       "34           0.013315  \n",
       "35           0.017083  \n",
       "36           0.104324  \n",
       "37           0.085395  \n",
       "38           0.014812  \n",
       "39           0.226325  \n",
       "40           0.057140  \n",
       "41           0.091623  \n",
       "42           0.090254  \n",
       "43           0.054214  \n",
       "44           0.023568  \n",
       "45           0.186656  \n",
       "46           0.012407  \n",
       "47           0.040500  \n",
       "\n",
       "[48 rows x 25 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85beff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/50 | Train-Loss 100.8280 | Test-MSE 365.0521 | Regret 0.0011 | Fair-Val 85.3123\n",
      "Epoch 010/50 | Train-Loss 91.1712 | Test-MSE 357.5446 | Regret 0.0010 | Fair-Val 80.5979\n",
      "Epoch 020/50 | Train-Loss 77.4437 | Test-MSE 345.8405 | Regret 0.0010 | Fair-Val 73.3753\n",
      "Epoch 030/50 | Train-Loss 60.1280 | Test-MSE 330.9709 | Regret 0.0011 | Fair-Val 64.2276\n",
      "Epoch 040/50 | Train-Loss 40.4944 | Test-MSE 314.1836 | Regret 0.0012 | Fair-Val 54.4745\n",
      "Epoch 050/50 | Train-Loss 17.5547 | Test-MSE 297.6054 | Regret 0.0015 | Fair-Val 45.4820\n",
      "Training finished in 143.59s.\n"
     ]
    }
   ],
   "source": [
    "# Fix the NaN values in the DataFrame\n",
    "hyperparams = {\n",
    "    \"alpha\": 0.8,\n",
    "    \"Q\": 2500,\n",
    "    \"lambda_fair\": 1,\n",
    "    \"fairness_type\": \"mad\",   \n",
    "    \"group\": True,            # Set to True for group fairness, False for individual\n",
    "    \"grad_method\": \"finite-diff\",\n",
    "    \"num_epochs\": 50,        \n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": len(b_train),\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "final_model, logs = train_model_regret(\n",
    "    X_train=feats_train, y_train=b_train, race_train=race_train, cost_train=cost_train, gainF_train=gainF_train,\n",
    "    X_test=feats_test, y_test=b_test, race_test=race_test, cost_test=cost_test, gainF_test=gainF_test,\n",
    "    model_class=FairRiskPredictor,\n",
    "    input_dim=feats_train.shape[1],\n",
    "    **hyperparams\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
