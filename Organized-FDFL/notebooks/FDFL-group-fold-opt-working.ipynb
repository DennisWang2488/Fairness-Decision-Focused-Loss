{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cvxpy as cp\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add custom paths\n",
    "sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\MyRepo\\\\fold-opt-package\\\\fold_opt')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GMRES import *\n",
    "from fold_opt import *\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "from src.utils.myOptimization import (\n",
    "    AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, solve_coupled_group_grad,\n",
    "    compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss, mean_abs_dev\n",
    "from src.utils.features import get_all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coupled_group_obj_torch(d, b, group_idx, alpha, beta=None):\n",
    "    \"\"\"\n",
    "    Calculates the objective value for the new alpha=beta formulation using torch tensors.\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are torch tensors\n",
    "    if not torch.is_tensor(d):\n",
    "        d = torch.tensor(d, dtype=torch.float32)\n",
    "    if not torch.is_tensor(b):\n",
    "        b = torch.tensor(b, dtype=torch.float32)\n",
    "    if not torch.is_tensor(group_idx):\n",
    "        group_idx = torch.tensor(group_idx, dtype=torch.float32)\n",
    "\n",
    "    d = d.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    group_idx = group_idx.reshape(-1)\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    y = b * d + epsilon\n",
    "    unique_groups = torch.unique(group_idx)\n",
    "    g_k_values = torch.zeros(len(unique_groups), dtype=torch.float32, device=y.device)\n",
    "\n",
    "    for i, k in enumerate(unique_groups):\n",
    "        members_mask = (group_idx == k)\n",
    "        y_k = y[members_mask]\n",
    "        if 0 < alpha < 1:\n",
    "            g_k_values[i] = torch.sum(y_k ** (1 - alpha)) / (1 - alpha)\n",
    "        elif alpha > 1:\n",
    "            g_k_values[i] = (alpha - 1) / torch.sum(y_k ** (1 - alpha))\n",
    "        elif abs(alpha - 1.0) < epsilon:\n",
    "            g_k_values[i] = torch.sum(torch.log(y_k))\n",
    "        else:  # alpha <= 0\n",
    "            g_k_values[i] = torch.sum(y_k)\n",
    "\n",
    "    if alpha == float('inf') or str(alpha).lower() == 'inf':\n",
    "        objective_value = torch.min(g_k_values)\n",
    "    elif abs(alpha - 1.0) < epsilon:\n",
    "        objective_value = torch.sum(torch.log(g_k_values + epsilon))\n",
    "    elif abs(alpha - 0.0) < epsilon:\n",
    "        objective_value = torch.sum(g_k_values)\n",
    "    else:\n",
    "        f_alpha_values = (g_k_values ** (1 - alpha)) / (1 - alpha)\n",
    "        objective_value = torch.sum(f_alpha_values)\n",
    "\n",
    "    return objective_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, Q = 1.5, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "\n",
    "df = pd.read_csv('E:\\\\User\\\\Stevens\\\\MyRepo\\\\Organized-FDFL\\\\src\\\\data\\\\data.csv')\n",
    "\n",
    "df = df.sample(n=2000, random_state=1)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'risk_score_t', 'program_enrolled_t', 'cost_t', 'cost_avoidable_t', 'race', 'dem_female', 'gagne_sum_tm1', 'gagne_sum_t', \n",
    "    'risk_score_percentile', 'screening_eligible', 'avoidable_cost_mapped', 'propensity_score', 'g_binary', \n",
    "    'g_continuous', 'utility_binary', 'utility_continuous'\n",
    "]\n",
    "# for race 0 is white, 1 is black\n",
    "df_stat = df[columns_to_keep]\n",
    "df_feature = df[[col for col in df.columns if col not in columns_to_keep]]\n",
    "\n",
    "# ---------- basic 1-D helpers ----------\n",
    "def as_1d(a, dtype=np.float32):\n",
    "    a = np.asarray(a, dtype=dtype).reshape(-1)   # (N,)\n",
    "    if a.ndim != 1:\n",
    "        raise ValueError(f\"expect 1-D, got {a.shape}\")\n",
    "    return a\n",
    "\n",
    "# transform the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# risk   = as_1d(df['risk_score_t']) * 100\n",
    "risk = np.array(df['benefit']) * 100.0  # scale to [0,100]\n",
    "risk = np.maximum(risk,1)         # or whatever the true column is\n",
    "gainF  = np.ones_like(risk, dtype=np.float32)\n",
    "cost   = as_1d(df['cost_t_capped']) * 10.0\n",
    "cost   = np.maximum(cost, 1)              # keep strictly positive\n",
    "race   = as_1d(df['race'])  # keep as int\n",
    "\n",
    "feats  = scaler.fit_transform(df[get_all_features(df)]).astype(np.float32)   # (N,p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    def __init__(self, optmodel, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "        self.optmodel = optmodel\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol = self.optmodel(self.risk, self.cost, self.race, Q=Q, alpha=alpha)\n",
    "        obj = compute_coupled_group_obj_torch(sol, self.risk, self.race, alpha=alpha)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol = torch.from_numpy(sol).float()\n",
    "        self.obj = torch.tensor(obj).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return ( self.feats[idx],\n",
    "                self.risk[idx],\n",
    "                self.gainF[idx],\n",
    "                self.cost[idx],\n",
    "                self.race[idx],\n",
    "                self.sol[idx],    # or store per-item solutions; see note ▼\n",
    "                self.obj )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer with batch normalization\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1000\n",
      "Test size: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FairRiskPredictor(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=152, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training parameters\n",
    "\n",
    "optmodel = solve_coupled_group_alpha\n",
    "\n",
    "feats_np  = np.asarray(feats)              # 2-D OK\n",
    "gainF_np  = to_numpy_1d(gainF)\n",
    "risk_np   = to_numpy_1d(risk)\n",
    "cost_np   = to_numpy_1d(cost)\n",
    "race_np   = to_numpy_1d(df['race'].values)\n",
    "\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, risk_train, risk_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    feats, gainF, risk, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(optmodel, feats_train, risk_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(optmodel, feats_test, risk_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "# Create dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "predmodel.to(DEVICE)\n",
    "# save the initial model\n",
    "# torch.save(predmodel.state_dict(), 'initial_model.pth')\n",
    "# load the initial model\n",
    "\n",
    "# self.sol is (N,) – __getitem__ returns a scalar component;\n",
    "# DataLoader stacks to (B,), which is exactly what the training loop expects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Fair Obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_fair_individual(u, alpha):\n",
    "    if alpha == 1:\n",
    "        return torch.log(u).sum(-1)\n",
    "    elif alpha == 0:\n",
    "        return u.sum(-1)\n",
    "    elif alpha == 'inf':\n",
    "        return u.min(-1).values\n",
    "    return (u**(1-alpha)/(1-alpha)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_optimization(gainF, risk, cost, alpha, Q):\n",
    "    gainF = gainF.detach().cpu().numpy() if isinstance(gainF, torch.Tensor) else gainF\n",
    "    risk = risk.detach().cpu().numpy() if isinstance(risk, torch.Tensor) else risk\n",
    "    cost = cost.detach().cpu().numpy() if isinstance(cost, torch.Tensor) else cost\n",
    "\n",
    "    risk = risk.clip(0.001)\n",
    "    gainF, risk, cost = gainF.flatten(), risk.flatten(), cost.flatten()\n",
    "    d = cp.Variable(risk.shape, nonneg=True)\n",
    "\n",
    "    if gainF.shape != risk.shape or risk.shape != cost.shape:\n",
    "        raise ValueError(\"Dimensions of gainF, risk, and cost do not match\")\n",
    "\n",
    "    utils = cp.multiply(cp.multiply(gainF, risk), d)\n",
    "    constraints = [d >= 0, cp.sum(cost * d) <= Q]\n",
    "\n",
    "    if alpha == 'inf':\n",
    "        t = cp.Variable()\n",
    "        objective = cp.Maximize(t)\n",
    "        constraints.append(utils >= t)\n",
    "    elif alpha == 1:\n",
    "        objective = cp.Maximize(cp.sum(cp.log(utils)))\n",
    "    elif alpha == 0:\n",
    "        objective = cp.Maximize(cp.sum(utils))\n",
    "    else:\n",
    "        objective = cp.Maximize(cp.sum(utils**(1-alpha)) / (1-alpha))\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.MOSEK, verbose=False, warm_start=True, mosek_params={'MSK_IPAR_LOG': 1})\n",
    "\n",
    "    if problem.status != 'optimal':\n",
    "        print(f\"Warning: Problem status is {problem.status}\")\n",
    "\n",
    "    optimal_decision = d.value\n",
    "    optimal_value = alpha_fair_individual(optimal_decision * gainF * risk, alpha)\n",
    "\n",
    "    return optimal_decision, optimal_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_budget(x, cost, Q, max_iter=100):\n",
    "    \"\"\"\n",
    "    x : (B,n)   or (n,)   –– internally promoted to (B,n)\n",
    "    cost : (n,) positive\n",
    "    Q : scalar or length‑B tensor\n",
    "    \"\"\"\n",
    "    batched = x.dim() == 2\n",
    "    if not batched:                       # (n,)  →  (1,n)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    B, n = x.shape\n",
    "    cost = cost.to(x)\n",
    "    Q    = torch.as_tensor(Q, dtype=x.dtype, device=x.device).reshape(-1, 1)  # (B,1)\n",
    "\n",
    "    d    = x.clamp(min=0.)                # enforce non‑neg\n",
    "    viol = (d @ cost) > Q.squeeze(1)      # which rows violate the budget?\n",
    "\n",
    "    if viol.any():\n",
    "        dv, Qv = d[viol], Q[viol]\n",
    "        lam_lo = torch.zeros_like(Qv.squeeze(1))\n",
    "        lam_hi = (dv / cost).max(1).values   # upper bound for λ⋆\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            trial   = (dv - lam_mid[:, None] * cost).clamp(min=0.)\n",
    "            too_big = (trial @ cost) > Qv.squeeze(1)\n",
    "            lam_lo[too_big] = lam_mid[too_big]\n",
    "            lam_hi[~too_big]= lam_mid[~too_big]\n",
    "\n",
    "        d[viol] = (dv - lam_hi[:, None] * cost).clamp(min=0.)\n",
    "\n",
    "    return d if batched else d.squeeze(0)   # restore original rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvxpylayer Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n",
    "# --- 1. Define the Differentiable Projection Layer ---\n",
    "def get_differentiable_projection(n, cost_np, Q_val):\n",
    "    \"\"\"\n",
    "    Creates a differentiable projection layer using CvxpyLayer.\n",
    "    This layer solves the projection problem:\n",
    "        minimize   ||d - z||_2^2\n",
    "        subject to cost^T * d <= Q\n",
    "                   d >= 0\n",
    "    \"\"\"\n",
    "    # Define CVXPY variables and parameters\n",
    "    d_var = cp.Variable(n)\n",
    "    z_param = cp.Parameter(n)\n",
    "    objective = cp.Minimize(cp.sum_squares(d_var - z_param))\n",
    "    constraints = [cost_np @ d_var <= Q_val, d_var >= 0]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    return CvxpyLayer(problem, parameters=[z_param], variables=[d_var])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold-Opt Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_step_cvxpylayer(r, d, g, race, cost, Q, alpha, lr, projection_layer, group=False):\n",
    "    \"\"\"\n",
    "    Performs one PGD step using the CvxpyLayer for projection.\n",
    "    \"\"\"\n",
    "    # Temporarily enable gradients for the internal gradient calculation\n",
    "    with torch.enable_grad():\n",
    "        # The input 'd' comes from the solver and has no grad history.\n",
    "        # We clone it and require gradients to trace the PGD update.\n",
    "        d_clone = d.clone().requires_grad_(True)\n",
    "    \n",
    "        if not group:\n",
    "            obj = alpha_fair_individual(d_clone * r * g, alpha=alpha)\n",
    "        else:\n",
    "            obj = compute_coupled_group_obj_torch(d_clone, r, race, alpha=alpha)\n",
    "        \n",
    "        # Compute gradient of the objective w.r.t. the decision `d_clone`\n",
    "        # This now happens within the `enable_grad` context.\n",
    "        grad_d, = torch.autograd.grad(obj, d_clone, create_graph=True)\n",
    "\n",
    "    # Perform the gradient ascent step\n",
    "    unprojected_d = d + lr * grad_d\n",
    "    \n",
    "    # Project back to the feasible set using the differentiable layer\n",
    "    projected_d, = projection_layer(unprojected_d)\n",
    "    \n",
    "    return projected_d\n",
    "\n",
    "def make_foldopt_layer_cvxpylayer(g, cost, race, alpha, Q, group, lr=5e-3, n_fixedpt=50, backprop_rule='FPI'):\n",
    "    \"\"\"\n",
    "    Factory function to create a FoldOptLayer with a differentiable PGD update.\n",
    "    \"\"\"\n",
    "    # Detach tensors that are not parameters of the prediction model but are used in the optimization.\n",
    "    # This prevents trying to compute gradients with respect to them.\n",
    "    g_detached = g.detach()\n",
    "    cost_detached = cost.detach()\n",
    "    race_detached = race.detach()\n",
    "\n",
    "    # --- Create the differentiable projection layer once ---\n",
    "    n_vars = cost.shape[0]\n",
    "    cost_np = cost_detached.cpu().numpy()\n",
    "    Q_val = Q.item() if isinstance(Q, torch.Tensor) else Q\n",
    "    projection_layer = get_differentiable_projection(n_vars, cost_np, Q_val)\n",
    "\n",
    "    # --- Solver function (no gradients needed) ---\n",
    "    def solver_fn(r_b): # r_b is batched, expected shape (B, n)\n",
    "        # Assuming the solver can only handle one instance at a time.\n",
    "        # If your solver is batched, you can simplify this.\n",
    "        d_list = []\n",
    "        for r_single in r_b:\n",
    "            r_np = r_single.detach().cpu().numpy()\n",
    "            cost_np_local = cost_detached.cpu().numpy()\n",
    "            race_np_local = race_detached.cpu().numpy()\n",
    "            \n",
    "            if group:\n",
    "                d_np = solve_coupled_group_alpha(r_np, cost_np_local, race_np_local, Q=Q_val, alpha=alpha)\n",
    "            else:\n",
    "                d_np,_ = solve_closed_form(g_detached.cpu().numpy(), r_np, cost_np_local, Q=Q_val, alpha=alpha)\n",
    "                # d_np, _ = solve_optimization(g_detached.cpu().numpy(), r_np, cost_np, alpha, Q)\n",
    "\n",
    "            \n",
    "            d_list.append(torch.from_numpy(d_np).to(r_b.device, r_b.dtype))\n",
    "        return torch.stack(d_list)\n",
    "\n",
    "\n",
    "    # --- Differentiable update function (closure) ---\n",
    "    def update_fn(r, d_star):\n",
    "        # r and d_star are batched\n",
    "        g_b = g_detached.expand_as(r)\n",
    "        \n",
    "        # Call the PGD step with the captured projection_layer and group flag\n",
    "        return pgd_step_cvxpylayer(r, d_star, g_b, race_detached, cost_detached, Q, alpha, lr, projection_layer, group)\n",
    "\n",
    "    return FoldOptLayer(solver_fn, update_fn, n_iter=n_fixedpt, backprop_rule=backprop_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_differentiable_dfl(\n",
    "    predmodel,\n",
    "    feats_train, risk_train, gainF_train, cost_train, race_train,\n",
    "    alpha, Q,\n",
    "    group=True,\n",
    "    num_epochs=50,\n",
    "    lr_pred=1e-3,\n",
    "    pgd_lr=5e-3,\n",
    "    n_fixedpt=50,\n",
    "    lambda_fairness=0.0,\n",
    "    fairness_type='atkinson'\n",
    "):\n",
    "    device = next(predmodel.parameters()).device\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr_pred)\n",
    "    \n",
    "    feats = torch.from_numpy(feats_train).to(device)\n",
    "    risk = torch.from_numpy(risk_train).to(device)\n",
    "    gainF = torch.from_numpy(gainF_train).to(device)\n",
    "    cost = torch.from_numpy(cost_train).to(device)\n",
    "    race = torch.from_numpy(race_train).to(device)\n",
    "\n",
    "    # --- Pre-calculate optimal solution (ground truth) ---\n",
    "    if group:\n",
    "        opt_d_np = solve_coupled_group_alpha(risk.cpu().numpy(), cost.cpu().numpy(), race.cpu().numpy(), Q, alpha)\n",
    "    else:\n",
    "        opt_d_np,_ = solve_closed_form(gainF.cpu().numpy(), risk.cpu().numpy(), cost.cpu().numpy(), Q, alpha)\n",
    "    opt_d = torch.from_numpy(opt_d_np).to(device)\n",
    "    \n",
    "    # --- CORRECTED: Calculate optimal objective ---\n",
    "    # The function already aggregates over groups, so no loop is needed.\n",
    "    if group:\n",
    "        opt_obj = compute_coupled_group_obj_torch(opt_d, risk, race, alpha)\n",
    "    else:\n",
    "        opt_obj = alpha_fair_individual(opt_d * risk * gainF, alpha)\n",
    "\n",
    "    # --- Build the FoldOpt layer with the 'group' flag ---\n",
    "    differentiable_opt_layer = make_foldopt_layer_cvxpylayer(\n",
    "        gainF, cost, race, alpha, Q, group, lr=pgd_lr, n_fixedpt=n_fixedpt\n",
    "    )\n",
    "\n",
    "    logs = {\"loss\": [], \"regret\": [], \"fairness\": []}\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        predmodel.train()\n",
    "        \n",
    "        pred_risk = predmodel(feats).clamp(min=1)\n",
    "        d_pred = differentiable_opt_layer(pred_risk.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        # --- Loss Calculation ---\n",
    "        if group:\n",
    "            # Use d_pred here to keep the computation graph intact\n",
    "            pred_obj = compute_coupled_group_obj_torch(d_pred, risk, race, alpha)\n",
    "        else:\n",
    "            pred_obj = alpha_fair_individual(d_pred * risk * gainF, alpha)\n",
    "\n",
    "        regret = (opt_obj - pred_obj) / (torch.abs(opt_obj) + 1e-8)\n",
    "        \n",
    "        # --- Fairness Penalty ---\n",
    "        fairness_penalty = torch.tensor(0.0, device=device)\n",
    "        if lambda_fairness > 0:\n",
    "            mode = 'between' if group else 'individual'\n",
    "            if fairness_type == 'atkinson':\n",
    "                fairness_penalty = atkinson_loss(pred_risk, risk, race, beta=0.5, mode=mode)\n",
    "            elif fairness_type == 'mad':\n",
    "                fairness_penalty = mean_abs_dev(pred_risk, risk, race, mode=mode)\n",
    "\n",
    "        loss = regret + lambda_fairness * fairness_penalty\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(predmodel.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        logs[\"loss\"].append(loss.item())\n",
    "        logs[\"regret\"].append(regret.item())\n",
    "        logs[\"fairness\"].append(fairness_penalty.item())\n",
    "            \n",
    "    # --- Final Evaluation for Detailed Logging ---\n",
    "    predmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        final_pred_risk = predmodel(feats).clamp(min=1e-4)\n",
    "        final_d_pred = differentiable_opt_layer(final_pred_risk.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        eval_logs = {}\n",
    "        unique_groups = torch.unique(race).cpu().numpy()\n",
    "        for g in unique_groups:\n",
    "            mask = (race == g)\n",
    "            if mask.sum() == 0: continue\n",
    "            eval_logs[f'G{int(g)}_mse'] = (final_pred_risk[mask] - risk[mask]).pow(2).mean().item()\n",
    "            group_utility = (final_d_pred[mask] * risk[mask] * gainF[mask])\n",
    "            eval_logs[f'G{int(g)}_decision_obj'] = alpha_fair_individual(group_utility, alpha).item()\n",
    "            eval_logs[f'G{int(g)}_true_benefit'] = risk[mask].mean().item()\n",
    "\n",
    "    return predmodel, logs, eval_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dfl_experiment_grid(\n",
    "    feats, risk, gainF, cost, race,\n",
    "    alpha, Q,\n",
    "    n_trials=3,\n",
    "    base_seed=2025,\n",
    "    group_settings=[True, False],\n",
    "    lambda_grid=[0.0, 1.0],\n",
    "    fairness_type_grid=['atkinson', 'mad'],\n",
    "    num_epochs=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs multiple random-split trials for each hyperparameter in the grid.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    \n",
    "    for group in group_settings:\n",
    "        for fairness_type in fairness_type_grid:\n",
    "            for lam in lambda_grid:\n",
    "                if lam == 0 and fairness_type != fairness_type_grid[0]:\n",
    "                    continue\n",
    "\n",
    "                trial_metrics = defaultdict(list)\n",
    "                run_params = f\"Group={group}, Lambda={lam}, Fairness={fairness_type}\"\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(f\"RUNNING EXPERIMENT: {run_params}\")\n",
    "                print(\"=\"*70)\n",
    "\n",
    "                for t in range(n_trials):\n",
    "                    print(f\"--- Trial {t+1}/{n_trials} ---\")\n",
    "                    seed = base_seed + t\n",
    "                    torch.manual_seed(seed)\n",
    "                    np.random.seed(seed)\n",
    "                    \n",
    "                    (feats_tr, _, risk_tr, _, gainF_tr, _, cost_tr, _, race_tr, _) = train_test_split(\n",
    "                        feats, risk, gainF, cost, race, test_size=0.5, random_state=seed\n",
    "                    )\n",
    "                    \n",
    "                    predictor = FairRiskPredictor(input_dim=feats_tr.shape[1]).to(DEVICE)\n",
    "                    \n",
    "                    _, logs, eval_logs = train_model_differentiable_dfl(\n",
    "                        predictor,\n",
    "                        feats_tr, risk_tr, gainF_tr, cost_tr, race_tr,\n",
    "                        alpha=alpha, Q=Q,\n",
    "                        group=group,\n",
    "                        lambda_fairness=lam,\n",
    "                        fairness_type=fairness_type,\n",
    "                        num_epochs=num_epochs\n",
    "                    )\n",
    "                    \n",
    "                    trial_metrics['final_loss'].append(logs['loss'][-1])\n",
    "                    trial_metrics['final_regret'].append(logs['regret'][-1])\n",
    "                    trial_metrics['final_fairness_penalty'].append(logs['fairness'][-1])\n",
    "                    for key, val in eval_logs.items():\n",
    "                        trial_metrics[key].append(val)\n",
    "\n",
    "                aggregated_row = {'Group': group, 'Lambda': lam, 'FairnessType': fairness_type}\n",
    "                for key, values in trial_metrics.items():\n",
    "                    aggregated_row[f'{key}_mean'] = np.mean(values)\n",
    "                    aggregated_row[f'{key}_std'] = np.std(values)\n",
    "                results_list.append(aggregated_row)\n",
    "\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================== Running for alpha=0.5 ==============================\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=0.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=mad\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "============================== Running for alpha=0.8 ==============================\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=0.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=mad\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "============================== Running for alpha=1.5 ==============================\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=0.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=mad\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "============================== Running for alpha=2.0 ==============================\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=0.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=atkinson\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENT: Group=True, Lambda=1.0, Fairness=mad\n",
      "======================================================================\n",
      "--- Trial 1/1 ---\n",
      "\n",
      "==========================================================================================\n",
      "                              GRID SEARCH COMPLETE\n",
      "==========================================================================================\n",
      "    Alpha  Group  Lambda FairnessType  G0_decision_obj_mean  G0_decision_obj_std  G0_mse_mean  G0_mse_std  G0_true_benefit_mean  G0_true_benefit_std  G1_decision_obj_mean  G1_decision_obj_std  G1_mse_mean  G1_mse_std  G1_true_benefit_mean  G1_true_benefit_std  final_fairness_penalty_mean  final_fairness_penalty_std  final_loss_mean  final_loss_std  final_regret_mean  final_regret_std\n",
      "0  0.5000   True  0.0000     atkinson             2863.0992               0.0000     321.2228      0.0000               11.3732               0.0000              863.3023               0.0000     550.8012      0.0000               15.8555               0.0000                       0.0000                      0.0000           0.0743          0.0000             0.0743            0.0000\n",
      "1  0.5000   True  1.0000     atkinson             2795.7079               0.0000     325.2783      0.0000               11.3732               0.0000              884.9172               0.0000     519.6854      0.0000               15.8555               0.0000                       0.0066                      0.0000           0.0829          0.0000             0.0764            0.0000\n",
      "2  0.5000   True  1.0000          mad             2782.4558               0.0000     328.7175      0.0000               11.3732               0.0000              873.3598               0.0000     530.6681      0.0000               15.8555               0.0000                     101.4687                      0.0000         101.5475          0.0000             0.0788            0.0000\n",
      "3  0.8000   True  0.0000     atkinson             4915.7824               0.0000     328.0870      0.0000               11.3732               0.0000              918.5801               0.0000     560.2274      0.0000               15.8555               0.0000                       0.0000                      0.0000           0.0016          0.0000             0.0016            0.0000\n",
      "4  0.8000   True  1.0000     atkinson             4909.6247               0.0000     332.4139      0.0000               11.3732               0.0000              917.6747               0.0000     526.9977      0.0000               15.8555               0.0000                       0.0064                      0.0000           0.0081          0.0000             0.0017            0.0000\n",
      "5  0.8000   True  1.0000          mad             4911.8326               0.0000     328.7184      0.0000               11.3732               0.0000              917.4042               0.0000     530.6697      0.0000               15.8555               0.0000                     101.4690                      0.0000         101.4706          0.0000             0.0016            0.0000\n",
      "6  1.5000   True  0.0000     atkinson            -1557.2838               0.0000     314.4874      0.0000               11.3732               0.0000             -115.2302               0.0000     529.3991      0.0000               15.8555               0.0000                       0.0000                      0.0000           0.0274          0.0000             0.0274            0.0000\n",
      "7  1.5000   True  1.0000     atkinson            -1560.6669               0.0000     323.8518      0.0000               11.3732               0.0000             -115.9992               0.0000     518.1095      0.0000               15.8555               0.0000                       0.0066                      0.0000           0.0354          0.0000             0.0288            0.0000\n",
      "8  1.5000   True  1.0000          mad            -1567.7728               0.0000     328.7169      0.0000               11.3732               0.0000             -116.5111               0.0000     530.6664      0.0000               15.8555               0.0000                     101.4680                      0.0000         101.4984          0.0000             0.0304            0.0000\n",
      "9  2.0000   True  0.0000     atkinson             -845.5312               0.0000     345.9083      0.0000               11.3732               0.0000              -88.8584               0.0000     606.8408      0.0000               15.8555               0.0000                       0.0000                      0.0000           0.4028          0.0000             0.4028            0.0000\n",
      "10 2.0000   True  1.0000     atkinson             -845.9872               0.0000     345.9462      0.0000               11.3732               0.0000              -88.8919               0.0000     606.4213      0.0000               15.8555               0.0000                       0.0097                      0.0000           0.4123          0.0000             0.4026            0.0000\n",
      "11 2.0000   True  1.0000          mad             -850.2546               0.0000     344.5851      0.0000               11.3732               0.0000              -89.0065               0.0000     593.5618      0.0000               15.8555               0.0000                     123.0935                      0.0000         123.5002          0.0000             0.4066            0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Set Parameters ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "ALPHA_GRID = [0.5, 0.8, 1.5, 2.0]\n",
    "Q_BUDGET = 600\n",
    "\n",
    "all_results = []\n",
    "for alpha_val in ALPHA_GRID:\n",
    "    print(f\"\\n{'='*30} Running for alpha={alpha_val} {'='*30}\")\n",
    "    results_df = run_dfl_experiment_grid(\n",
    "        feats, risk, gainF, cost, race,\n",
    "        alpha=alpha_val,\n",
    "        Q=Q_BUDGET,\n",
    "        n_trials=1,\n",
    "        group_settings=[True],\n",
    "        lambda_grid=[0.0, 1.0],\n",
    "        fairness_type_grid=['atkinson', 'mad'],\n",
    "        num_epochs=30\n",
    "    )\n",
    "    results_df['Alpha'] = alpha_val\n",
    "    all_results.append(results_df)\n",
    "\n",
    "# Concatenate all results\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# --- 4. Print Final Results Table ---\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" \" * 30 + \"GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*90)\n",
    "# Reorder columns for better readability\n",
    "hp_cols = ['Alpha', 'Group', 'Lambda', 'FairnessType']\n",
    "metric_cols = sorted([c for c in final_results_df.columns if c not in hp_cols])\n",
    "final_results_df = final_results_df[hp_cols + metric_cols]\n",
    "\n",
    "with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None,\n",
    "                       'display.width', 1200,\n",
    "                       'display.float_format', '{:.4f}'.format):\n",
    "    print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Group</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>FairnessType</th>\n",
       "      <th>final_regret_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.074304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.076369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.078836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.001671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.027419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.030363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.402790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>atkinson</td>\n",
       "      <td>0.402606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>0.406633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alpha  Group  Lambda FairnessType  final_regret_mean\n",
       "0     0.5   True     0.0     atkinson           0.074304\n",
       "1     0.5   True     1.0     atkinson           0.076369\n",
       "2     0.5   True     1.0          mad           0.078836\n",
       "3     0.8   True     0.0     atkinson           0.001636\n",
       "4     0.8   True     1.0     atkinson           0.001671\n",
       "5     0.8   True     1.0          mad           0.001642\n",
       "6     1.5   True     0.0     atkinson           0.027419\n",
       "7     1.5   True     1.0     atkinson           0.028800\n",
       "8     1.5   True     1.0          mad           0.030363\n",
       "9     2.0   True     0.0     atkinson           0.402790\n",
       "10    2.0   True     1.0     atkinson           0.402606\n",
       "11    2.0   True     1.0          mad           0.406633"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_df[['Alpha', 'Group', 'Lambda', 'FairnessType', 'final_regret_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv('fold-opt-n2000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
