{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold-opt Layer with PGD\n",
    "1. Define PGD update step with analytical projection\n",
    "2. Define solver `solve_closed_form`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\MyRepo\\\\FDFL\\\\helper')\n",
    "# sys.path.insert(0, 'E:\\\\User\\\\Stevens\\\\Code\\\\Fold-opt\\\\fold_opt')\n",
    "from myutil import *\n",
    "from GMRES import *\n",
    "from fold_opt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- little utilities ---------------------------------\n",
    "def to_col(x):              # (…,n)  ->  (…,n,1)\n",
    "    return x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "def from_col(x):            # (…,n,1) -> (…,n)\n",
    "    return x.squeeze(-1)\n",
    "\n",
    "def from_numpy_torch(x):  # (n,) -> (1,n) -> (1,n,1)\n",
    "    return torch.from_numpy(x).float().unsqueeze(0).unsqueeze(-1)\n",
    "def to_torch_numpy(x):    # (1,n,1) -> (1,n) -> (n,)\n",
    "    return x.squeeze(0).squeeze(-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------------------------------------------------------------\n",
    " 1.  Projection onto ${d \\geq 0 , c^ {\\top} d \\leq Q}$\n",
    " \n",
    "     Formula:  $\\Pi(d) = [d - λ^* \\cdot c]_+$  with\n",
    "                $λ^* \\geq 0$ chosen s.t. $c^{\\top}\\Pi(d)=Q$   (if budget violated)\n",
    " -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import cvxpy as cp\n",
    "\n",
    "# # --- Your analytical projection (as given) ---\n",
    "# def proj_budget(x, cost, Q, max_iter=60):\n",
    "#     batched = x.dim() == 2\n",
    "#     if not batched:\n",
    "#         x = x.unsqueeze(0)\n",
    "#     B, n = x.shape\n",
    "#     cost = cost.to(x)\n",
    "#     Q = torch.as_tensor(Q, dtype=x.dtype, device=x.device).reshape(-1, 1)\n",
    "\n",
    "#     d = x.clamp(min=0.)\n",
    "#     viol = (d @ cost) > Q.squeeze(1)\n",
    "\n",
    "#     if viol.any():\n",
    "#         dv, Qv = d[viol], Q[viol]\n",
    "#         lam_lo = torch.zeros_like(Qv.squeeze(1))\n",
    "#         lam_hi = (dv / cost).max(1).values\n",
    "\n",
    "#         for _ in range(max_iter):\n",
    "#             lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "#             trial = (dv - lam_mid[:, None] * cost).clamp(min=0.)\n",
    "#             too_big = (trial @ cost) > Qv.squeeze(1)\n",
    "#             lam_lo[too_big]  = lam_mid[too_big]\n",
    "#             lam_hi[~too_big] = lam_mid[~too_big]\n",
    "\n",
    "#         d[viol] = (dv - lam_hi[:, None] * cost).clamp(min=0.)\n",
    "\n",
    "#     return d if batched else d.squeeze(0)\n",
    "\n",
    "# # --- Solver-based projection via CVXPY ---\n",
    "# def cvx_proj(x_np, cost_np, Q):\n",
    "#     n = x_np.shape[0]\n",
    "#     d = cp.Variable(n)\n",
    "#     obj = cp.Minimize( cp.sum_squares(d - x_np) )\n",
    "#     cons = [d >= 0, cost_np.T @ d <= Q]\n",
    "#     prob = cp.Problem(obj, cons)\n",
    "#     _ = prob.solve(solver=cp.OSQP, warm_start=True)\n",
    "#     return d.value\n",
    "\n",
    "# # --- Timing routine ---\n",
    "# def benchmark(sizes, repetitions=5):\n",
    "#     results = []\n",
    "#     for n in sizes:\n",
    "#         # generate random problem\n",
    "#         x   = torch.randn(n)\n",
    "#         cost= torch.rand(n) + 0.1\n",
    "#         Q   = float( n * cost.mean() * 0.5 )\n",
    "\n",
    "#         # warm‐up\n",
    "#         _ = proj_budget(x, cost, Q)\n",
    "#         _ = cvx_proj(x.numpy(), cost.numpy(), Q)\n",
    "\n",
    "#         # time analytical\n",
    "#         t0 = time.perf_counter()\n",
    "#         for _ in range(repetitions):\n",
    "#             _ = proj_budget(x, cost, Q)\n",
    "#         t_custom = (time.perf_counter() - t0) / repetitions\n",
    "\n",
    "#         # time solver\n",
    "#         t0 = time.perf_counter()\n",
    "#         for _ in range(repetitions):\n",
    "#             _ = cvx_proj(x.numpy(), cost.numpy(), Q)\n",
    "#         t_solver = (time.perf_counter() - t0) / repetitions\n",
    "\n",
    "#         results.append((n, t_custom, t_solver))\n",
    "\n",
    "#     # print results\n",
    "#     print(f\"{'n':>8} | {'custom (s)':>12} | {'solver (s)':>12}\")\n",
    "#     print(\"-\" * 38)\n",
    "#     for n, tc, ts in results:\n",
    "#         print(f\"{n:8d} | {tc:12.6f} | {ts:12.6f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     sizes = [100, 1_000, 5_000, 10_000]\n",
    "#     benchmark(sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_budget(x, cost, Q, max_iter=60):\n",
    "    \"\"\"\n",
    "    x : (B,n)   or (n,)   –– internally promoted to (B,n)\n",
    "    cost : (n,) positive\n",
    "    Q : scalar or length‑B tensor\n",
    "    \"\"\"\n",
    "    batched = x.dim() == 2\n",
    "    if not batched:                       # (n,)  →  (1,n)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    B, n = x.shape\n",
    "    cost = cost.to(x)\n",
    "    Q    = torch.as_tensor(Q, dtype=x.dtype, device=x.device).reshape(-1, 1)  # (B,1)\n",
    "\n",
    "    d    = x.clamp(min=0.)                # enforce non‑neg\n",
    "    viol = (d @ cost) > Q.squeeze(1)      # which rows violate the budget?\n",
    "\n",
    "    if viol.any():\n",
    "        dv, Qv = d[viol], Q[viol]\n",
    "        lam_lo = torch.zeros_like(Qv.squeeze(1))\n",
    "        lam_hi = (dv / cost).max(1).values   # upper bound for λ⋆\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            trial   = (dv - lam_mid[:, None] * cost).clamp(min=0.)\n",
    "            too_big = (trial @ cost) > Qv.squeeze(1)\n",
    "            lam_lo[too_big] = lam_mid[too_big]\n",
    "            lam_hi[~too_big]= lam_mid[~too_big]\n",
    "\n",
    "        d[viol] = (dv - lam_hi[:, None] * cost).clamp(min=0.)\n",
    "\n",
    "    return d if batched else d.squeeze(0)   # restore original rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------------------------------------------------------------\n",
    "###  One PGD update that *is differentiable* wrt both r & d\n",
    " -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_fair(u, alpha):\n",
    "    if alpha == 1:\n",
    "        return torch.log(u).sum(-1)\n",
    "    elif alpha == 0:\n",
    "        return u.sum(-1)\n",
    "    elif alpha == 'inf':\n",
    "        return u.min(-1).values\n",
    "    return (u.pow(1-alpha)/(1-alpha)).sum(-1)\n",
    "\n",
    "def pgd_step(r, d, g, cost, Q, alpha, lr):\n",
    "    d = d.clone().requires_grad_(True)\n",
    "    obj     = alpha_fair(d * r * g, alpha).sum()\n",
    "    grad_d, = torch.autograd.grad(obj, d, create_graph=True)\n",
    "    return proj_budget(d + lr * grad_d, cost, Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solver_torch(r, g, cost, alpha, Q):\n",
    "    if r.dim() == 1:                      # (n,) → (1,n) before looping\n",
    "        r = r.unsqueeze(0)\n",
    "    out = []\n",
    "    for r_i in r:\n",
    "        d_np, _ = solve_closed_form(g.cpu().numpy(),\n",
    "                                    r_i.detach().cpu().numpy(),\n",
    "                                    cost.cpu().numpy(),\n",
    "                                    alpha, Q)\n",
    "        out.append(torch.as_tensor(d_np, dtype=r.dtype, device=r.device))\n",
    "    return torch.stack(out)               # (B,n) even if B=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------------------------------------------------------\n",
    " ###  Fold‑Opt layer\n",
    " ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_foldopt_layer(g, cost, alpha, Q,\n",
    "                       lr=1e-2, n_fixedpt=200, rule='GMRES'):\n",
    "    g    = g.detach()\n",
    "    cost = cost.detach()\n",
    "\n",
    "    # -------- solver: no gradients flow ----------------------------\n",
    "    def solver_fn(r):\n",
    "        return closed_form_solver_torch(r, g, cost, alpha, Q)\n",
    "\n",
    "    # -------- one differentiable PGD step --------------------------\n",
    "    def update_fn(r, x_star, *_):\n",
    "        # promote to (B,n) if needed\n",
    "        if r.dim() == 1:       r = r.unsqueeze(0)\n",
    "        if x_star.dim() == 1:  x_star = x_star.unsqueeze(0)\n",
    "\n",
    "        g_b = g.expand_as(r) if g.dim() == 1 else g\n",
    "        return pgd_step(r, x_star, g_b, cost, Q, alpha, lr)  # (B,n)\n",
    "\n",
    "    return FoldOptLayer(solver_fn, update_fn,\n",
    "                        n_iter=n_fixedpt, backprop_rule=rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r     = torch.tensor([1.,2.,3.,4.,5.], requires_grad=True)\n",
    "print(r.shape)\n",
    "r = r.unsqueeze(0)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "d (no‑batch) : tensor([[0.6189, 0.4376, 0.3573, 0.3094, 0.2768]],\n",
      "       grad_fn=<FixedPtDiffGMRESBackward>)\n",
      "∂d/∂r, shape  : tensor([ 0.0239, -0.1094, -0.0596, -0.0387, -0.0277]) torch.Size([5])\n",
      "closed VJP: tensor([-7.4506e-09, -5.5879e-09, -7.4506e-09,  1.8626e-09, -1.8626e-09])\n",
      "fold‑opt VJP: tensor([ 0.0239, -0.1094, -0.0596, -0.0387, -0.0277])\n",
      "inf norm diff: tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # ---------- single input vector --------------------------------\n",
    "    r     = torch.tensor([1.,2.,3.,4.,5.], requires_grad=True)\n",
    "    g     = torch.ones_like(r)\n",
    "    cost  = torch.ones_like(r)\n",
    "    alpha = 2\n",
    "    Q     = 2.0\n",
    "\n",
    "    r_b   = r.unsqueeze(0)\n",
    "    print(r_b.shape)       # (1,5)\n",
    "\n",
    "    layer = make_foldopt_layer(g, cost, alpha, Q, lr=1e-2)\n",
    "\n",
    "    d = layer(r_b)                # d is (1,5)\n",
    "    d.sum().backward()   # OK: shapes now match inside GMRES\n",
    "\n",
    "    print(\"d (no‑batch) :\", d)\n",
    "    print(\"∂d/∂r, shape  :\", r.grad, r.grad.shape)   # -> (5,)\n",
    "\n",
    "    J = compute_gradient_closed_form(g, r.detach(), cost, alpha, Q)   # now J is 5×5 = ∂d/∂r\n",
    "    v = torch.ones(5)                                                 # same upstream as d.sum()\n",
    "    vjp_closed = (torch.tensor(J.T) @ v).detach()                                   # shape (5,)\n",
    "\n",
    "\n",
    "\n",
    "    r.grad.zero_()\n",
    "    d = layer(r.unsqueeze(0))\n",
    "    d.sum().backward()                 # uses the same “ones” upstream\n",
    "    vjp_foldopt = r.grad               # shape (5,)\n",
    "    print(\"closed VJP:\", vjp_closed)\n",
    "    print(\"fold‑opt VJP:\", vjp_foldopt)\n",
    "    print(\"inf norm diff:\", (vjp_closed - vjp_foldopt).abs().max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed VJP: tensor([-0.3008,  0.0117,  0.0853, -0.0198,  0.0201])\n",
      "fold‑opt VJP: tensor([-0.4898,  0.0321,  0.1297, -0.0220,  0.0300])\n",
      "cosine similarity: 0.9993108510971069\n"
     ]
    }
   ],
   "source": [
    "# 1) closed‑form VJP\n",
    "J        = torch.tensor(compute_gradient_closed_form(g, r.detach(), cost, alpha, Q))  # (5×5)\n",
    "v        = torch.randn(5)   # your test upstream gradient\n",
    "vjp_cl   = J.T @ v          # (5,)\n",
    "\n",
    "# 2) fold‑opt VJP\n",
    "r.grad = None\n",
    "d = layer(r.unsqueeze(0)).squeeze(0)\n",
    "d.backward(v)               # use v instead of ones\n",
    "vjp_fo = r.grad\n",
    "print(\"closed VJP:\", vjp_cl)\n",
    "print(\"fold‑opt VJP:\", vjp_fo)\n",
    "\n",
    "# 3) cosine\n",
    "cos = torch.dot(vjp_cl, vjp_fo) / (vjp_cl.norm()*vjp_fo.norm()+1e-12)\n",
    "print(\"cosine similarity:\", cos.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_solver(c):\n",
    "#     return torch.clamp(c, min=0.0)\n",
    "\n",
    "# def my_update_step(c, x):\n",
    "#     alpha = 0.1\n",
    "#     grad  = x - c\n",
    "#     x_new = torch.clamp(x - alpha*grad, min=0.0)\n",
    "#     return x_new\n",
    "\n",
    "# fold_layer = FoldOptLayer(\n",
    "#                 solver      = my_solver,\n",
    "#                 update_step = my_update_step,\n",
    "#                 n_iter      = 20,\n",
    "#                 backprop_rule='Jacobianx')\n",
    "\n",
    "# c      = torch.tensor([[1.0], [ 2.0]], requires_grad=True)   # (B=2, n=1)\n",
    "# target = torch.tensor([[ 1], [ 1]])\n",
    "\n",
    "# x_star = fold_layer(c)\n",
    "# print(\"x* from FoldOptLayer:\", x_star.squeeze().tolist())     # → [0.0, 2.0]\n",
    "\n",
    "# loss = 0.5 * torch.sum((x_star - target) ** 2)\n",
    "# loss.backward()\n",
    "\n",
    "# print(\"Grad wrt c:\", c.grad.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4898,  0.0321,  0.1297, -0.0220,  0.0300])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_l_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# column sum of cl_grad\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m grad_l_d , cl_grad\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e7\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_l_d' is not defined"
     ]
    }
   ],
   "source": [
    "# column sum of cl_grad\n",
    "grad_l_d , cl_grad.sum(0) * 1e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed‑form VJP: tensor([-7.4506e-09, -2.2352e-08, -6.7055e-08,  2.9802e-08, -8.1956e-08])\n",
      "fold‑opt  VJP: tensor([ 0.0239, -0.1094, -0.0596, -0.0387, -0.0277])\n",
      "difference ∞‑norm: tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "J = torch.tensor(compute_gradient_closed_form(r, g, cost, alpha, Q))  # shape (5,5)\n",
    "vjp_closed = J.sum(dim=0)    # shape (5,)\n",
    "print(\"closed‑form VJP:\", vjp_closed)\n",
    "print(\"fold‑opt  VJP:\", r.grad)  # from your backward()\n",
    "print(\"difference ∞‑norm:\", (vjp_closed - r.grad).abs().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim = 0.48399463295936584\n"
     ]
    }
   ],
   "source": [
    "v1 = vjp_closed\n",
    "v2 = r.grad  # your fold‑opt VJP\n",
    "cos_sim = torch.dot(v1, v2) / (v1.norm() * v2.norm() + 1e-12)\n",
    "print(\"cosine sim =\", cos_sim.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
