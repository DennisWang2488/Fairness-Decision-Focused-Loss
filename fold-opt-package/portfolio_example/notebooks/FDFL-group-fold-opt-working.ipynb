{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "# Add custom paths\n",
    "sys.path.insert(0, '/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/FDFL/helper')\n",
    "sys.path.insert(0, '/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/fold-opt-package/fold_opt')\n",
    "\n",
    "from myutil import *\n",
    "from features import get_all_features\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GMRES import *\n",
    "from fold_opt import *\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "from src.utils.myOptimization import (\n",
    "    solveGroupProblem, closed_form_group_alpha, AlphaFairnesstorch,\n",
    "    solveIndProblem, solve_closed_form, solve_coupled_group_alpha, solve_coupled_group_grad,\n",
    "    compute_coupled_group_obj\n",
    ")\n",
    "from src.utils.myPrediction import generate_random_features, customPredictionModel\n",
    "from src.utils.plots import visLearningCurve\n",
    "from src.fairness.cal_fair_penalty import atkinson_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Alpha & Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, Q = 1.5, 100\n",
    "beta = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_1d(x):\n",
    "    \"\"\"Return a 1-D NumPy array; error if the length is not > 1.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    assert x.ndim == 1, f\"expected 1-D, got shape {x.shape}\"\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/dennis/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/Organized-FDFL/src/data/data.csv')\n",
    "\n",
    "df = df.sample(n=50, random_state=42)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'risk_score_t', 'program_enrolled_t', 'cost_t', 'cost_avoidable_t', 'race', 'dem_female', 'gagne_sum_tm1', 'gagne_sum_t', \n",
    "    'risk_score_percentile', 'screening_eligible', 'avoidable_cost_mapped', 'propensity_score', 'g_binary', \n",
    "    'g_continuous', 'utility_binary', 'utility_continuous'\n",
    "]\n",
    "# for race 0 is white, 1 is black\n",
    "df_stat = df[columns_to_keep]\n",
    "df_feature = df[[col for col in df.columns if col not in columns_to_keep]]\n",
    "\n",
    "# ---------- basic 1-D helpers ----------\n",
    "def as_1d(a, dtype=np.float32):\n",
    "    a = np.asarray(a, dtype=dtype).reshape(-1)   # (N,)\n",
    "    if a.ndim != 1:\n",
    "        raise ValueError(f\"expect 1-D, got {a.shape}\")\n",
    "    return a\n",
    "\n",
    "# transform the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "risk   = as_1d(df['risk_score_t']) * 100\n",
    "risk = np.maximum(risk,0.1)         # or whatever the true column is\n",
    "gainF  = np.ones_like(risk, dtype=np.float32)\n",
    "cost   = as_1d(df['cost_t_capped']) * 10.0\n",
    "cost   = np.maximum(cost, 0.1)              # keep strictly positive\n",
    "race   = as_1d(df['race'], dtype=np.int64)  # keep as int\n",
    "\n",
    "feats  = scaler.fit_transform(df[get_all_features(df)]).astype(np.float32)   # (N,p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    def __init__(self, optmodel, feats, risk, gainF, cost, race, alpha=alpha, Q=Q):\n",
    "        # Store as numpy arrays for now\n",
    "        self.feats = feats\n",
    "        self.risk = risk\n",
    "        self.gainF = gainF\n",
    "        self.cost = cost\n",
    "        self.race = race\n",
    "        self.optmodel = optmodel\n",
    "\n",
    "        # Call optmodel (expects numpy arrays)\n",
    "        sol = self.optmodel(self.risk, self.cost, self.race, Q=Q, alpha=alpha, beta=beta)\n",
    "        obj = compute_coupled_group_obj(sol, self.risk, self.race, alpha=alpha, beta=beta)\n",
    "\n",
    "        # Convert everything to torch tensors for storage\n",
    "        self.feats = torch.from_numpy(self.feats).float()\n",
    "        self.risk = torch.from_numpy(self.risk).float()\n",
    "        self.gainF = torch.from_numpy(self.gainF).float()\n",
    "        self.cost = torch.from_numpy(self.cost).float()\n",
    "        self.race = torch.from_numpy(self.race).float()\n",
    "        self.sol = torch.from_numpy(sol).float()\n",
    "        self.obj = torch.tensor(obj).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.feats, self.risk, self.gainF, self.cost, self.race, self.sol, self.obj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return ( self.feats[idx],\n",
    "                self.risk[idx],\n",
    "                self.gainF[idx],\n",
    "                self.cost[idx],\n",
    "                self.race[idx],\n",
    "                self.sol[idx],    # or store per-item solutions; see note ▼\n",
    "                self.obj )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRiskPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer with batch normalization\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25\n",
      "Test size: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FairRiskPredictor(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=152, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training parameters\n",
    "\n",
    "optmodel = solve_coupled_group_alpha\n",
    "\n",
    "feats_np  = np.asarray(feats)              # 2-D OK\n",
    "gainF_np  = to_numpy_1d(gainF)\n",
    "risk_np   = to_numpy_1d(risk)\n",
    "cost_np   = to_numpy_1d(cost)\n",
    "race_np   = to_numpy_1d(df['race'].values)\n",
    "\n",
    "\n",
    "# Perform train-test split\n",
    "feats_train, feats_test, gainF_train, gainF_test, risk_train, risk_test, cost_train, cost_test, race_train, race_test = train_test_split(\n",
    "    feats, gainF, risk, cost, df['race'].values, test_size=0.5, random_state=2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {feats_train.shape[0]}\")\n",
    "print(f\"Test size: {feats_test.shape[0]}\")\n",
    "\n",
    "dataset_train = optDataset(optmodel, feats_train, risk_train, gainF_train, cost_train, race_train, alpha=alpha, Q=Q)\n",
    "dataset_test = optDataset(optmodel, feats_test, risk_test, gainF_test, cost_test, race_test, alpha=alpha, Q=Q)\n",
    "\n",
    "# Create dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_train), shuffle=False)\n",
    "\n",
    "predmodel = FairRiskPredictor(feats_train.shape[1])\n",
    "predmodel.to(device)\n",
    "# save the initial model\n",
    "# torch.save(predmodel.state_dict(), 'initial_model.pth')\n",
    "# load the initial model\n",
    "\n",
    "# self.sol is (N,) – __getitem__ returns a scalar component;\n",
    "# DataLoader stacks to (B,), which is exactly what the training loop expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_budget(x, cost, Q, max_iter=100):\n",
    "    \"\"\"\n",
    "    x : (B,n)   or (n,)   –– internally promoted to (B,n)\n",
    "    cost : (n,) positive\n",
    "    Q : scalar or length‑B tensor\n",
    "    \"\"\"\n",
    "    batched = x.dim() == 2\n",
    "    if not batched:                       # (n,)  →  (1,n)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    B, n = x.shape\n",
    "    cost = cost.to(x)\n",
    "    Q    = torch.as_tensor(Q, dtype=x.dtype, device=x.device).reshape(-1, 1)  # (B,1)\n",
    "\n",
    "    d    = x.clamp(min=0.)                # enforce non‑neg\n",
    "    viol = (d @ cost) > Q.squeeze(1)      # which rows violate the budget?\n",
    "\n",
    "    if viol.any():\n",
    "        dv, Qv = d[viol], Q[viol]\n",
    "        lam_lo = torch.zeros_like(Qv.squeeze(1))\n",
    "        lam_hi = (dv / cost).max(1).values   # upper bound for λ⋆\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            trial   = (dv - lam_mid[:, None] * cost).clamp(min=0.)\n",
    "            too_big = (trial @ cost) > Qv.squeeze(1)\n",
    "            lam_lo[too_big] = lam_mid[too_big]\n",
    "            lam_hi[~too_big]= lam_mid[~too_big]\n",
    "\n",
    "        d[viol] = (dv - lam_hi[:, None] * cost).clamp(min=0.)\n",
    "\n",
    "    return d if batched else d.squeeze(0)   # restore original rank\n",
    "\n",
    "\n",
    "def alpha_fair(u, alpha):\n",
    "    if alpha == 1:\n",
    "        return torch.log(u).sum(-1)\n",
    "    elif alpha == 0:\n",
    "        return u.sum(-1)\n",
    "    elif alpha == 'inf':\n",
    "        return u.min(-1).values\n",
    "    return (u.pow(1-alpha)/(1-alpha)).sum(-1)\n",
    "\n",
    "def pgd_step(r, d, g, cost, Q, alpha, lr):\n",
    "    d = d.clone().requires_grad_(True)\n",
    "    obj     = alpha_fair(d * r * g, alpha).sum()\n",
    "    grad_d, = torch.autograd.grad(obj, d, create_graph=True)\n",
    "    return proj_budget(d + lr * grad_d, cost, Q)\n",
    "\n",
    "def closed_form_solver_torch(r, cost, group_idx, alpha, Q, beta):\n",
    "\n",
    "    # assert cost.dim() == 1,  f\"cost shape expected (n,), got {cost.shape}\"\n",
    "    # assert race.shape == cost.shape, f\"race {group_idx.shape} vs cost {cost.shape}\"\n",
    "\n",
    "    if r.dim() == 1:\n",
    "        r = r.unsqueeze(0)\n",
    "    out = []\n",
    "    for r_i in r:\n",
    "        d_np = solve_coupled_group_alpha(\n",
    "                    r_i.detach().cpu().numpy(),\n",
    "                    cost.detach().cpu().numpy(),\n",
    "                    group_idx.detach().cpu().numpy(),\n",
    "                    Q=Q, alpha=alpha, beta=beta)\n",
    "        out.append(torch.as_tensor(d_np, dtype=r.dtype, device=r.device))\n",
    "    return torch.stack(out)\n",
    "              # (B,n) even if B=1\n",
    "\n",
    "def make_foldopt_layer(g, cost, group, alpha, Q,\n",
    "                       lr=5e-3, n_fixedpt=400, rule='GMRES'):\n",
    "    g    = g.detach() # gainF\n",
    "    cost = cost.detach() # cost\n",
    "\n",
    "    # -------- solver: no gradients flow ----------------------------\n",
    "    def solver_fn(r):\n",
    "        return closed_form_solver_torch(r, cost, group, alpha, Q, beta)\n",
    "\n",
    "    # -------- one differentiable PGD step --------------------------\n",
    "    def update_fn(r, x_star, *_):\n",
    "        # promote to (B,n) if needed\n",
    "        if r.dim() == 1:       r = r.unsqueeze(0)\n",
    "        if x_star.dim() == 1:  x_star = x_star.unsqueeze(0)\n",
    "\n",
    "        g_b = g.expand_as(r) if g.dim() == 1 else g\n",
    "        return pgd_step(r, x_star, g_b, cost, Q, alpha, lr)  # (B,n)\n",
    "\n",
    "    return FoldOptLayer(solver_fn, update_fn,\n",
    "                        n_iter=n_fixedpt, backprop_rule=rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# assume make_foldopt_layer and alpha_fair are already in scope from your fold-opt code\n",
    "# from fold_opt import make_foldopt_layer, alpha_fair\n",
    "\n",
    "def trainFairModelFoldOpt(\n",
    "    predmodel,\n",
    "    loader_train,\n",
    "    loader_test,\n",
    "    alpha,\n",
    "    Q,\n",
    "    lambda_fairness=0.1,\n",
    "    num_epochs=10,\n",
    "    lr_pred=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    pgd_lr=1e-2,\n",
    "    n_fixedpt=200,\n",
    "    backprop_rule=\"GMRES\"\n",
    "):\n",
    "    device = next(predmodel.parameters()).device\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr_pred, weight_decay=weight_decay)\n",
    "\n",
    "    logs = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_mse\": [],\n",
    "        \"train_fair\": [],\n",
    "        \"train_regret\": []\n",
    "    }\n",
    "\n",
    "    predmodel.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "                # --- pull the one-and-only batch (the entire dataset) ---\n",
    "        feats, risk, gainF, cost, race, opt_d, opt_obj = next(iter(loader_train))\n",
    "\n",
    "        # move to device\n",
    "        feats   = feats.to(device)           # (N,p)\n",
    "        risk    = risk.to(device)            # (N,)\n",
    "        gainF   = gainF.to(device)           # (N,)\n",
    "        cost    = cost.to(device)            # (N,)\n",
    "        race    = race.to(device)            # (N,)\n",
    "        opt_d   = opt_d.to(device)           # (N,)\n",
    "        opt_obj = opt_obj[0].to(device)      # scalar (all identical, keep one)\n",
    "\n",
    "        # --- forward pass: predict risk ---\n",
    "        pred_risk = predmodel(feats).clamp(min=1e-3)  # (n,)\n",
    "\n",
    "        # --- build a Fold-Opt layer for this batch ---\n",
    "        #    it will map r_batch (1,n) → d_pred (1,n)\n",
    "        # fold-opt layer needs 1-D input; it returns (1,N)\n",
    "        d_pred = make_foldopt_layer(\n",
    "                    gainF, cost, race, alpha, Q,\n",
    "                    lr=pgd_lr, n_fixedpt=n_fixedpt, rule=backprop_rule\n",
    "                )(pred_risk.unsqueeze(0)).squeeze(0) \n",
    "        # d_pred = proj_budget(d_pred, cost, Q, max_iter=500).clamp(min=0.)\n",
    "\n",
    "        # used = cost.to(d_pred.device) @ d_pred\n",
    "        # if used > Q + 1e-6:\n",
    "        #     print(f\"[WARN] budget overshoot: {used - Q:.2e}\")\n",
    "        # assert used <= Q + 1e-6\n",
    "\n",
    "        # --- compute regret loss via alpha‐fairness ---\n",
    "        u_pred    = d_pred * risk * gainF               # (n,)\n",
    "        pred_obj  = alpha_fair(u_pred.unsqueeze(0), alpha)       # scalar\n",
    "\n",
    "        regret_l1 = (opt_obj - pred_obj) / (opt_obj.abs() + 1e-7)  # (1,)\n",
    "\n",
    "        # --- fairness penalty: difference in MSE across race groups ---\n",
    "        m0 = (pred_risk[race == 0] - risk[race == 0]).pow(2).mean() if (race==0).any() else torch.tensor(0., device=device)\n",
    "        m1 = (pred_risk[race == 1] - risk[race == 1]).pow(2).mean() if (race==1).any() else torch.tensor(0., device=device)\n",
    "        fair_reg = torch.abs(m0 - m1)\n",
    "\n",
    "        # --- total loss & backward ---\n",
    "        loss = regret_l1 + lambda_fairness * fair_reg\n",
    "        loss.to(device)  # move to device\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- compute simple MSE for logging ---\n",
    "        mse_train = (pred_risk - risk).pow(2).mean()\n",
    "\n",
    "        # --- log everything ---\n",
    "        logs[\"train_loss\"].append(loss.item())\n",
    "        logs[\"train_mse\"].append(mse_train.item())\n",
    "        logs[\"train_fair\"].append(fair_reg.item())\n",
    "        logs[\"train_regret\"].append(regret_l1.item())\n",
    "\n",
    "        # (optional) print progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch:2d} | Loss={loss.item():.4f} | MSE={mse_train.item():.4f} | Fair={fair_reg.item():.4f} | Regret={regret_l1.item():.4f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "    predmodel.eval()\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_multiple_trials_foldopt(\n",
    "    feats,\n",
    "    gainF,\n",
    "    risk,\n",
    "    cost,\n",
    "    df,\n",
    "    optmodel,\n",
    "    n_trials=10,\n",
    "    test_size=0.5,\n",
    "    lambda_fairness=0.0,\n",
    "    num_epochs=50,\n",
    "    lr_pred=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    pgd_lr=1e-2,\n",
    "    n_fixedpt=200,\n",
    "    backprop_rule=\"GMRES\",\n",
    "    alpha=2,\n",
    "    Q=1000,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs multiple random-split trials of fold-opt DFL training, collects \n",
    "    TRAINING curves, and plots mean ± 1 std over trials for Loss, Regret, \n",
    "    MSE, and Fairness.\n",
    "    \"\"\"\n",
    "    all_loss   = []\n",
    "    all_regret = []\n",
    "    all_mse    = []\n",
    "    all_fair   = []\n",
    "    times      = []\n",
    "\n",
    "    start_all = time.time()\n",
    "    for t in range(n_trials):\n",
    "        # print(f\"[Trial {t+1}/{n_trials}]\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1) random train/test split\n",
    "        split = train_test_split(\n",
    "            feats, gainF, risk, cost, df['race'].values,\n",
    "            test_size=test_size, random_state=t\n",
    "        )\n",
    "        feats_tr, feats_te, g_tr, g_te, r_tr, r_te, c_tr, c_te, race_tr, race_te = split\n",
    "\n",
    "        # 2) build datasets / loaders\n",
    "        ds_tr = optDataset(optmodel, feats_tr, r_tr, g_tr, c_tr, race_tr, alpha=alpha, Q=Q)\n",
    "        loader_tr = DataLoader(ds_tr, batch_size=len(r_tr), shuffle=False)\n",
    "\n",
    "        # 3) init predictor\n",
    "        pred = FairRiskPredictor(feats_tr.shape[1]).to(device)\n",
    "\n",
    "        # 4) train fold-opt\n",
    "        logs = trainFairModelFoldOpt(\n",
    "            predmodel=pred,\n",
    "            loader_train=loader_tr,\n",
    "            loader_test=None,            # ignored by this function\n",
    "            alpha=alpha,\n",
    "            Q=Q,\n",
    "            lambda_fairness=lambda_fairness,\n",
    "            num_epochs=num_epochs,\n",
    "            lr_pred=lr_pred,\n",
    "            weight_decay=weight_decay,\n",
    "            pgd_lr=pgd_lr,\n",
    "            n_fixedpt=n_fixedpt,\n",
    "            backprop_rule=backprop_rule\n",
    "        )\n",
    "\n",
    "        all_loss.append(logs[\"train_loss\"])\n",
    "        all_regret.append(logs[\"train_regret\"])\n",
    "        all_mse.append(logs[\"train_mse\"])\n",
    "        all_fair.append(logs[\"train_fair\"])\n",
    "\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "    total_time = time.time() - start_all\n",
    "    mean_time, std_time = np.mean(times), np.std(times)\n",
    "    print(f\"\\nAll {n_trials} trials done in {total_time:.1f}s  (avg {mean_time:.1f}±{std_time:.1f}s each)\")\n",
    "\n",
    "    # to arrays: shape (n_trials, num_epochs)\n",
    "    all_loss   = np.array(all_loss)\n",
    "    all_regret = np.array(all_regret)\n",
    "    all_mse    = np.array(all_mse)\n",
    "    all_fair   = np.array(all_fair)\n",
    "    epochs     = np.arange(all_loss.shape[1])\n",
    "\n",
    "    # compute mean ± std\n",
    "    m_loss,   s_loss   = all_loss.mean(0),   all_loss.std(0)\n",
    "    m_regret, s_regret = all_regret.mean(0), all_regret.std(0)\n",
    "    m_mse,    s_mse    = all_mse.mean(0),    all_mse.std(0)\n",
    "    m_fair,   s_fair   = all_fair.mean(0),   all_fair.std(0)\n",
    "\n",
    "    # --- plot ---\n",
    "    fig, axes = plt.subplots(1,4, figsize=(24,4))\n",
    "    titles = [\"Training Loss\",\"Training Regret\",\"Training MSE\",\"Training Fairness\"]\n",
    "    means  = [m_loss, m_regret, m_mse, m_fair]\n",
    "    stds   = [s_loss, s_regret, s_mse, s_fair]\n",
    "    colors = [\"C0\",\"C1\",\"C2\",\"C3\"]\n",
    "\n",
    "    for ax, title, mean, std, col in zip(axes, titles, means, stds, colors):\n",
    "        ax.plot(epochs, mean,   color=col, lw=2)\n",
    "        ax.fill_between(epochs,\n",
    "                        mean - std,\n",
    "                        mean + std,\n",
    "                        color=col, alpha=0.2)\n",
    "        ax.set_title(f\"{title}\\n(avg of {n_trials} trials)\", fontsize=14)\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"mean_train_loss\":     m_loss,\n",
    "        \"std_train_loss\":      s_loss,\n",
    "        \"mean_train_regret\":   m_regret,\n",
    "        \"std_train_regret\":    s_regret,\n",
    "        \"mean_train_mse\":      m_mse,\n",
    "        \"std_train_mse\":       s_mse,\n",
    "        \"mean_train_fair\":     m_fair,\n",
    "        \"std_train_fair\":      s_fair,\n",
    "        \"times\":               times,\n",
    "        \"total_time\":          total_time,\n",
    "        \"mean_time\":           mean_time,\n",
    "        \"std_time\":            std_time,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss=-0.9853 | MSE=384103.7188 | Fair=384103.7188 | Regret=-0.9853 | 0.5s\n",
      "Epoch  2 | Loss=-0.9853 | MSE=384112.4688 | Fair=384112.4688 | Regret=-0.9853 | 0.4s\n",
      "Epoch  3 | Loss=-0.9853 | MSE=384135.2812 | Fair=384135.2812 | Regret=-0.9853 | 0.4s\n",
      "Epoch  4 | Loss=-0.9854 | MSE=384135.6875 | Fair=384135.6875 | Regret=-0.9854 | 0.4s\n",
      "Epoch  5 | Loss=-0.9852 | MSE=384158.0000 | Fair=384158.0000 | Regret=-0.9852 | 0.4s\n",
      "Epoch  6 | Loss=-0.9853 | MSE=384171.2500 | Fair=384171.2500 | Regret=-0.9853 | 0.4s\n",
      "Epoch  7 | Loss=-0.9852 | MSE=384191.1250 | Fair=384191.1250 | Regret=-0.9852 | 0.4s\n",
      "Epoch  8 | Loss=-0.9852 | MSE=384208.4375 | Fair=384208.4375 | Regret=-0.9852 | 0.4s\n",
      "Epoch  9 | Loss=-0.9851 | MSE=384219.1875 | Fair=384219.1875 | Regret=-0.9851 | 0.4s\n",
      "Epoch 10 | Loss=-0.9851 | MSE=384236.5625 | Fair=384236.5625 | Regret=-0.9851 | 0.4s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multiple_trials_foldopt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgainF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrisk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolve_coupled_group_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_fairness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgd_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fixedpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprop_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGMRES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 59\u001b[0m, in \u001b[0;36mrun_multiple_trials_foldopt\u001b[0;34m(feats, gainF, risk, cost, df, optmodel, n_trials, test_size, lambda_fairness, num_epochs, lr_pred, weight_decay, pgd_lr, n_fixedpt, backprop_rule, alpha, Q, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m pred \u001b[38;5;241m=\u001b[39m FairRiskPredictor(feats_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 4) train fold-opt\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainFairModelFoldOpt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# ignored by this function\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_fairness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_fairness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgd_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpgd_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fixedpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fixedpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprop_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackprop_rule\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m all_loss\u001b[38;5;241m.\u001b[39mappend(logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     75\u001b[0m all_regret\u001b[38;5;241m.\u001b[39mappend(logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_regret\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 80\u001b[0m, in \u001b[0;36mtrainFairModelFoldOpt\u001b[0;34m(predmodel, loader_train, loader_test, alpha, Q, lambda_fairness, num_epochs, lr_pred, weight_decay, pgd_lr, n_fixedpt, backprop_rule)\u001b[0m\n\u001b[1;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# move to device\u001b[39;00m\n\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 80\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# --- compute simple MSE for logging ---\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/fold-opt-package/fold_opt/fold_opt.py:59\u001b[0m, in \u001b[0;36mFixedPtDiffGMRES.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[1;32m     58\u001b[0m     c, x_star_step, x_star, max_iter \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[0;32m---> 59\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m \u001b[43mJgP_GMRES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_star_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_input\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/2024-fall/research/Fairness-Decision-Focused-Loss/fold-opt-package/fold_opt/fold_opt.py:170\u001b[0m, in \u001b[0;36mJgP_GMRES\u001b[0;34m(c, x_star_step, x_star, g, n_steps, tol)\u001b[0m\n\u001b[1;32m    165\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mto(c\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(c\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#y = v_matrix_mult(A, Q[k])\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m#y = v_matrix_mult(A, Q[:,k,:])\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     y \u001b[38;5;241m=\u001b[39m  Q[:,k,:] \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_star_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    171\u001b[0m     H,Q \u001b[38;5;241m=\u001b[39m GMRES\u001b[38;5;241m.\u001b[39mv_update_H_Q(H, Q, y, k, tol)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;66;03m# Initialize the O and R factors\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fair/lib/python3.10/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_2 = run_multiple_trials_foldopt(\n",
    "    feats, gainF, risk, cost, df,\n",
    "    optmodel=solve_coupled_group_alpha,\n",
    "    n_trials=1,\n",
    "    test_size=0.5,\n",
    "    lambda_fairness=0,\n",
    "    num_epochs=50,\n",
    "    lr_pred=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    pgd_lr=5e-3,\n",
    "    n_fixedpt=400,\n",
    "    backprop_rule=\"GMRES\",\n",
    "    alpha=2,\n",
    "    Q=100,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: np.ndarray, shape=(500, 152), ndim=2\n",
      "gainF: np.ndarray, shape=(500,), ndim=1\n",
      "risk : np.ndarray, shape=(500,), ndim=1\n",
      "cost : np.ndarray, shape=(500,), ndim=1\n",
      "race : np.ndarray, shape=(500,), ndim=1\n"
     ]
    }
   ],
   "source": [
    "def _inspect_for_split(feats, gainF, risk, cost, race):\n",
    "    for name, arr in zip(\n",
    "            [\"feats\", \"gainF\", \"risk\", \"cost\", \"race\"],\n",
    "            [feats,  gainF,   risk,   cost,   race]):\n",
    "        if isinstance(arr, torch.Tensor):\n",
    "            print(f\"{name:5s}: torch.Tensor, shape={arr.shape}, ndim={arr.ndim}\")\n",
    "        else:\n",
    "            arr = np.asarray(arr)\n",
    "            print(f\"{name:5s}: np.ndarray, shape={arr.shape}, ndim={arr.ndim}\")\n",
    "_inspect_for_split(feats, gainF, risk, cost, df['race'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
